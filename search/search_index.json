{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Event-driven enterprise","text":"IBM Automation Event-Driven Architecture Introduction <p>Getting started with Event-Driven Architectures.</p>  &gt;&gt; Concepts of EDA <p>Delve into the foundational concepts of Event-Driven Architectures.</p>  &gt;&gt; Advantages of EDA <p>Explore some of the many benefits of Event-Driven Architectures.</p>  &gt;&gt; Patterns in EDA <p>Complex design patterns solved with Event-Driven Architectures.</p>  &gt;&gt; Methodology <p>Master Event-Driven Architectures methodologies from start to finish.</p>  &gt;&gt; Technology <p>Leverage the technologies behind Event-Driven Architectures.</p>  &gt;&gt; Scenarios &amp; Use Cases <p>Walk through real-world use cases of event-driven applications.</p>  &gt;&gt;"},{"location":"tracking/","title":"Track refactoring","text":"<p>From one page fix all links, and consider migration done when links point to page ported to new docs-mk folder. The target page may not be done.</p>"},{"location":"tracking/#to-move","title":"to move","text":""},{"location":"tracking/#done","title":"Done","text":"<pre><code>- introduction\n  - Overview: introduction/overview/index.md\n  - Reference Architecture: introduction/reference-architecture/index.md\n  - Business Use Cases: introduction/usecases/index.md\n  - Target Audiences: introduction/target-audiences/index.md\n- Learning Journey:\n    - Get started (101 content): journey/101/index.md\n    - Get started (201 content): journey/201/index.md\n- Concepts:\n  - Terms &amp; Definitions: concepts/terms-and-definitions/index.md\n  - Event streaming versus Queuing: concepts/events-versus-messages/index.md\n  - Fit for purpose: concepts/fit-to-purpose/index.md\n  - Service mesh: concepts/service-mesh/index.md\n  - Agile Integration: concepts/integration/index.md\n  - Devising the data models: concepts/model/index.md\n  - Flow Architecture: concepts/flow-architectures.md\n- Advantages of EDA:\n  - Microservice decoupling: advantages/microservice/index.md\n  - Reactive systems: advantages/reactive/index.md\n  - Resiliency: advantages/resiliency/index.md\n  - Scalability: advantages/scalability/index.md\n- Patterns in EDA:\n  - Introduction: patterns/intro/index.md\n  - Event Sourcing: patterns/event-sourcing/index.md\n  - CQRS: patterns/cqrs/index.md\n  - Saga: patterns/saga/index.md\n  - Dead Letter Queue: patterns/dlq/index.md\n  - Data Intensive App: patterns/data-pipeline/index.md\n  - Near real-time analytics: patterns/realtime-analytics/index.md\n  - API management: patterns/api-mgt/index.md\n  - Situational decision: patterns/cep/index.md\n- Technology:\n  - Kafka Overview: technology/kafka-overview/index.md\n  -  Event Streams: technology/event-streams/index.md\n  - Kafka FAQ: technology/faq/index.md\n  - Avro Schema: technology/avro-schemas/index.md\n  - MQ in EDA context: technology/mq/index.md\n  - Kafka Consumers: technology/kafka-consumers/index.md\n  - Kafka Producers: technology/kafka-producers/index.md\n  - Mirror Maker 2:  technology/kafka-mirrormaker/index.md\n  - Security: technology/security/index.md\n  - Apache Flink: technology/flink/index.md\n  - Spring cloud: technology/spring/index.md\n\n- Methodology:\n  - Event Storming: methodology/event-storming/index.md\n  - Domain-Driven Design: methodology/domain-driven-design/index.md\n  - Data lineage: methodology/data-lineage/index.md\n  - Data Intensive App Development: methodology/data-intensive/index.md\n  - governance: methodolgy/governance/index.md\n\n- use cases:\n  - Event-driven solution GitOps: use-cases/gitops/index.md\n  - Deploy Event-Streams: technology/event-streams/es-cp4i/index.md\n  - Kafka Connect - S3: use-cases/connect-s3/index.md\n  - Kafka Connect - COS: use-cases/connect-cos/index.md \n  - Kafka Connect - jdbc: use-cases/connect-jdbc/index.md\n  - Kafka Connect - MQ: use-cases/connect-mq/index.md\n  - Kafka Connect - Rabbitmq: use-cases/connect-rabbitmq /index.md  \n  - DB2 - CDC Debezium - Outbox: use-cases/db2-debezium/index.md\n  - Mirror maker 2 labs: use-cases/kafka-mm2/index.md\n</code></pre> <ul> <li>Scenarios:<ul> <li>Overview: scenarios/overview/</li> <li>Reefer Shipment Solution: https://ibm-cloud-architecture.github.io/refarch-kc/</li> <li>Vaccine at Scale: https://ibm-cloud-architecture.github.io/vaccine-solution-main/</li> <li>Near real-time Inventory: https://ibm-cloud-architecture.github.io/eda-rt-inventory-gitops</li> </ul> </li> </ul>"},{"location":"tracking/#moved-content-to-verify-links","title":"moved content to verify links .","text":"<ul> <li>Patterns in EDA:  <ul> <li>Topic Replication: patterns/topic-replication/index.md</li> </ul> </li> </ul> <p>use cases  - api management</p> <ul> <li>Technology</li> <li> <p>streams</p> <ul> <li>Kafka Streams: technology/kafka-streams/index.md</li> </ul> </li> </ul>"},{"location":"tracking/#to-revisit-the-structure-and-content","title":"To revisit the structure and content","text":"<ul> <li>Pattern &gt; Topic replication with DR and MM2</li> <li>Scenario: kafka-mm2, reefer,</li> <li>SAGA with MQ Orchestration: scenarios/saga-orchestration/</li> </ul>"},{"location":"additional-reading/","title":"Additional reading","text":""},{"location":"additional-reading/#event-driven-architecture","title":"Event-Driven Architecture","text":"<ul> <li>Event-Driven Architecture Learning Journey via IBM Skills Gateway, provides a navigable guide through this GitBook and associated assets.</li> </ul>"},{"location":"additional-reading/#ibm-event-streams","title":"IBM Event Streams","text":"<ul> <li>IBM Event Streams presentation</li> <li>Validating Event Streams deployment with sample app.</li> <li>Install IBM Event Streams on Red Hat OpenShift</li> </ul>"},{"location":"additional-reading/#kafka","title":"Kafka","text":"<ul> <li>Start by reading Kafka introduction - a must read!</li> <li>Another introduction from Confluent, one of the main contributors of the open source.</li> <li>Planning event streams installation</li> <li>Develop Stream Application using Kafka</li> <li>Tutorial on access control, user authentication and authorization from IBM.</li> <li>IBM Developer article - learn kafka</li> <li>Using Kafka Connect to connect to enterprise MQ systems - Andrew Schofield</li> <li>Does Apache Kafka do ACID transactions? - Andrew Schofield</li> <li>Spark and Kafka with direct stream, and persistence considerations and best practices</li> <li>Example in scala for processing Tweets with Kafka Streams</li> </ul>"},{"location":"additional-reading/#microservices-and-event-driven-patterns","title":"Microservices and event-driven patterns","text":"<ul> <li>API for declaring messaging handlers using Reactive Streams</li> <li>Microservice patterns - Chris Richardson</li> </ul>"},{"location":"additional-reading/#stream-analytics","title":"Stream Analytics","text":"<ul> <li>IBM Streams Samples</li> <li>Getting started with IBM Streaming Analytics on IBM Cloud</li> </ul>"},{"location":"additional-reading/#integration","title":"Integration","text":"<ul> <li>Interesting article on the evolving hybrid integration reference architecture: How to ensure your integration landscape keeps pace with digital transformation.</li> <li>Companion web site for hybrid integration reference architecture</li> </ul>"},{"location":"additional-reading/#conferences-talks-and-sessions","title":"Conferences, Talks, and Sessions","text":"<ul> <li>IBM THINK 2020 - From Monolithic Application to API Centric and Event-Driven Microservices \u2013 the Cloud Journey of a Leading Health Care Organization</li> <li>IBM THINK 2020 - Change the Way You Integrate Applications with IBM Cloud Pak for Integration</li> <li>IBM THINK 2020 - Modernize Integration to Unlock Data and Applications Securely While Lowering Costs</li> <li>Kafka Summit 2016 - San Francisco</li> <li>Kafka Summit 2017 - New York</li> <li>Kafka Summit 2017 - San Francisco</li> <li>Kafka Summit 2018 - San Francisco</li> <li>Kafka Summit 2019 - San Francisco</li> <li>Kafka Summit 2019 - London</li> <li>Kafka Summit 2020 - Virtual</li> <li>Kafka Summit 2022 - Recap</li> </ul>"},{"location":"advantages/microservice/","title":"Advantages of Event-Driven Reference Architectures - Microservice","text":"<p> Updated 10/07/2021 </p> <p>As we have seen in the introduction, modern business application needs to responds to events in real time,  as the event happen, so it can deliver better user experiences and apply business rule on those events.  The key is to be able to act quickly on those facts. Acting may involve computing analytics or machine  trained models. </p> <p>On top of that a modern cloud native application needs to be reactive, responsive by adopting the reactive manifesto.  We can also claim they are becoming intelligence by integrating rule engine and predictive scoring / AI capabilities. </p> <p>When adopting microservice implementation approach, the bounded context is defined with events and aggregates or main business entity.  So each microservice is responsible to manage the operations of creating, updating and reading the data from a  main business entity. This clear separation leads to exchange data between services, they may used to be integrated in the same monolytic application before.</p> <p>A web application, single page app (SPA), accesses the different microservices using RESTful API,  to get the different data views it needs, or to post new data elements to one of the service. The following diagram illustrates a simple view of the microservice challenges:</p> <p></p> <p>When the user interface exposes entry form to get data for one of the business entity,  it calls a REST end point with a HTTP POST operation, then data are saved to data store:  document oriented database or SQL based RDBMS.</p> <p>When a microservice (1) needs to access data from another service, then it calls  another end point via an HTTP GET. A coupling is still existing, at the data schema definition level: a change to the data model, from the source microservice, impacts the service contract and so all the callers.  This may be acceptable when there is few microservices, but could become a real pain when the number increase.</p> <p>When the microservice dependencies grows in size and complexity,  as illustrated by the following figure from Jack Kleeman's Monzo study,  we can see the coupling impact, which lead to impacting time to deliver new function and  cost to maintain such complexity.</p> <p></p> <p>Finally, imagine we need to join data coming from two different services to address an urgent business request?  Who will implement the join, service A or B? May be the simplest is to add a service C and  implement the join: it will call two API end points, and try to reconcile data using primary keys  on both business entities.</p> <p></p> <p>With event-driven microservices, the communication point becomes the Pub/Sub layer of  the event backbone. By adopting an event-based approach for intercommunication between  microservices, the microservices applications are naturally responsive (event-driven).  This approach enhances the loose coupling nature of microservices because it decouples  producers and consumers. The figure below illustrates, that microservices A and B produces  facts about their business entities, and their life cycle to topic in the pub/sub event backbone:</p> <p></p> <p>The microservice C consumes those facts to build it own projection or view for supporting the join query.</p> <p>When adopting technology like Kafka as messaging backbone, the data sharing is  done via an event log, which can be kept for a very long time period, and is replayable  to improve resilience. These event style characteristics are increasingly important  considerations when you develop microservices style applications. In practical  terms microservices applications are a combination of synchronous API-driven,  and asynchronous event-driven communication styles. </p> <p>There is something important to add, is that coupling by the data, still existing but in a less impactful manner. Messaging structures are defined with JSON schema or Avro schema and managed inside a Schema registry, so Kafka-based applications can get their data contract.  </p> <p>The following figure presents a potential structure for event-driven microservice:  APIs are defined using microprofile OpenAPI annotations in one or more JAXRS resource  classes. Those APIs can then be managed within an API management product as IBM API  Connect.</p> <p></p> <p>The rest of the application structure reflects the DDD approach of onion architecture.  The business logic is in its own layer with DDD aggregate, ubiquitous language, services, business rules, etc\u2026 The repository layer supports persisting those aggregates to an external document-oriented  or SQL-based database. As most of the new microservices are message-driven, we are adding a messaging layer  that may use queues or topics. Use queue for request/response exactly once delivery  and topic for sharing facts in append log. In Java, the Microprofile Reactive Messaging is used to define the different publishing  channels, being queue, topic, or both. From the JSON or Avro schema defining the messages or events structure,  developers can build an AsyncAPI specification which may also be managed by an API product.</p>"},{"location":"advantages/reactive/","title":"Reactive Systems and Reactive Programming","text":"<p>This chapter describes how event-driven architecture addresses the implementation of reactive systems and  presents the most recent technologies to implement such event-driven responsive solutions.</p>"},{"location":"advantages/reactive/#overview","title":"Overview","text":""},{"location":"advantages/reactive/#cloud-native","title":"Cloud native","text":"<p>A buzzy set of words which has some concrete requirements. The 12 factors app defines  the needs for modern business service apps or webapps to be a good citizen in cloud deployment:</p> <ul> <li>Codebase: One codebase tracked in version control, many deploys.</li> <li>Dependencies: Explicitly declare and isolate dependencies.</li> <li>Config: store config in the environment.</li> <li>Backing services: Treat backing services as attached resources.</li> <li>Build, release, run: Strictly separate build and run stages.</li> <li>Processes: Execute the app as one or more stateless (which may be challenged in case of event streaming and real-time analytics) processes</li> <li>Port binding: Export services via port binding.</li> <li>Concurrency: Scale out via the process model.</li> <li>Disposability: Maximize the robustness with fast startup and graceful shutdown.</li> <li>Dev/prod parity: Keep development, staging, and production as similar as possible.</li> <li>Logs: Treat your logs as event streams.</li> <li>Admin processes: Run admin/management tasks as one-off processes.</li> </ul> <p>Achieving cloud native is not an easy task. Avoid vendor locking by using specific vendor services to keep data, messaging... Hybrid cloud applications run on multiple clouds,  mixing private and public clouds, to reduce response time and prevent global unavailability  as recently demonstrated by AWS outtages.  Kubernetes helps delivering an abstraction on top of cloud providers, and uses them as infrastructure as a services, and bring your own platform. Kubernetes facilitates reactive systems through responsiveness and resilience. </p>"},{"location":"advantages/reactive/#distributed-systems","title":"Distributed systems","text":"<p>From Leslie Lamport's definition: </p> <p>A distributed system is one in which the failure of a computer you didn\u2019t even know existed can render your own computer unusable.</p> <p>we need to design application for transient, intermittent, or complete failure and resilience. Complex exchanges involving multiple services cannot expect all the participants and the network  to be operational for the complete duration of that exchange. </p> <p>Always ask ourselves: How would you detect failures? How would you handle them gracefully?</p> <p>When designing a distributed solution, we need to keep in mind that the CAP theorem prevents  a distributed data store from simultaneously providing more than two  out of the three guarantees of Consistency, Availability and Partition tolerance. </p> <p>With simple synchronous calls between services leads to time coupling and enforces programming in sequence. The code must gracefully handle faulty responses and the absence of response.  Quarkus/Vert.x has a router that can be intercepted to simulate communication loss, wrong response or delay, this is vertically helpful to test for failure. See Clement Escoffier's code sample.</p> <p>Combining time-out and retries, is a common development practice, but we can\u2019t assume the service was not invoked and retrying may reprocess the same request multiple times. Service needs to support idempotence: multiple requests with same content, results in the same state and same output. Idempotence can be implemented using unique identifiers added to the payload so consumer can identify same request. But server applications need to keep state of those ids, which means using Storage, which is under the CAP theorem. We may include an in-memory data grid such as Infinispan or Hazelcast, an inventory service such as Apache ZooKeeper, a distributed cache as Redis. </p>"},{"location":"advantages/reactive/#reactive-systems","title":"Reactive systems","text":"<p>Modern business applications embrace the strong need to be responsive, bringing immediate  response and feedbacks to the end user or system acting on it at the moment it needed.  Modern solution based on microservices needs to support load increase and failure and developers are adopting the reactive manifesto and use modern programming libraries and software to support  the manifesto characteristics. </p> <p>The reactive manifesto defines four characteristics modern cloud native application needs to support:</p> <p></p> <ul> <li>Responsive: deliver a consistent quality of service to end users or systems, react quickly and consistently to events happening in the system.</li> <li>Elastic:\u00a0The system stays responsive under varying workload, it can scale up and down the resource utilization depending of the load to the system.</li> <li>Resilient: stay responsive in the face of failure, this is a key characteristics. It implies distributed systems.</li> <li>Message driven: the underlying behavior is to have an asynchronous message driven backbone, to enable loose coupling of the application components by exchanging asynchronous messages to minimize or isolate the negative effects of resource contention, coherency delays and inter-service communication network latency. It is the base to support the other reactive characteristics. It also helps for isolation and support location transparency.</li> </ul> <p>Reactive architecture is an architecture approach aims to use asynchronous messaging or event driven architecture to build Responsive, Resilient and Elastic systems.  Relying on message passing enables the responsive characteristics and more, like flow control  by monitoring the messages in the system and applying backpressure when necessary.</p> <p>Under the \"reactive\" terms we can see two important caveats:</p> <ul> <li>Reactive systems is a group of application components which can heal and scale automatically. It address data consistency, cross domain communication, orchestration, failure, recovery... </li> <li>Reactive programming is a subset of asynchronous programming and a paradigm where the availability of new information drives the logic forward rather than having control flow driven by a thread-of-execution. This is the adoption of non-blocking IO and event-based model.</li> </ul> <p>The following figure illustrates well how those two paradigms work together to deliver business value:</p> <p></p> <p>We recommend to go over this excellent IBM article on defining reactive to go deeper into those concepts.</p>"},{"location":"advantages/reactive/#commands-and-events","title":"Commands and Events","text":"<p>Those two concepts are very fundamental to implement well distributed applications. </p> <ul> <li>Commands: represent action a user or system wants to perform. HTTP APIs pass commands. Commands are sent to a specific service and result is sent back to the caller.</li> <li>Events: are actions that have successfully completed. An event represents a fact. They are immutable. </li> </ul> <p>By looking at your solution in terms of commands and events, you focus on the behavior, workflow, instead of the structure.</p> <p>Events are wrapped into Messages. But Commands can also being passed via messaging and asynchronous communication. Most likely strong consistency is needed and queuing systems are used as message brokers.</p>"},{"location":"advantages/reactive/#is-it-for-me","title":"Is it for me?","text":"<p>We have learnt from years of point to point microservice implementations, that embrassing  asynchronous communication helps a lot to support scaling, integration, coupling and failover.  So adopting reactive design and implementation may look complex at first but is becoming a  necessity in the long run. In e-commerce, a lot of monolithic applications were redesigned  to adopt reactive manifesto characteristics to support scaling the business needs and respond  to sporadic demand. In the world of big data, collecting, aggregating, applying real time  analytics, decisions and AI need to scale and respond to events at the time of occurence. </p>"},{"location":"advantages/reactive/#eda-and-reactive-systems","title":"EDA and reactive systems","text":"<p>The adoption of event driven microservice implementation fits well into the reactive manifesto, where most of the work presented in this git repository started by adopting Kafka as event  backbone, it is too reductor to think EDA is just Kafka. EDA supports reactive systems at large, and developing event-driven microservice should use reactive libraries to support  non-blocking IO and event bus for inter process communication. Also microservices is part of the game, functions / serverless are also in scope and with serverless 2.0,  Knative eventing is one of the new kid in the play.</p> <p>The manifesto stipulates \"message driven\" while EDA is about events and commands.  Events represent unmmutable data and facts about what happened, and components subscribe to those event streams. Command demands the consumer to process the content data sent and gives an answer. Both are sent via messages, and transported and managed by brokers.  For sure we define event-driven implementations to cover both. And we should not be purist and opinionated  about messaging versus eventing: it will not make any sense to say: you are using queue to exchange message  while we produce events to topic. </p> <p>The following figure illustrates the combination of synchronous communication, sending commands to reactive system, supported by reactive applications link together via messaging.</p> <p></p> <p>With messaging applications can scale horizontally giving the elastic need of the reactive manifesto. They  are also more resilient as messages are kept until consumed or for a long period of time and consumer can restart from where they were in the ordered log. </p> <p>Reactive systems are not only exchanging messages. Sending and receiving messages must be done efficiently and Reactive promotes the use of nonblocking I/Os. Which leads to reactive programming and supporting libraries, like Vert.x, Mutiny, reactive messaging...</p>"},{"location":"advantages/reactive/#technology-review","title":"Technology review","text":""},{"location":"advantages/reactive/#concurrency","title":"Concurrency","text":"<p>The following figure illustrates the traditional Java multi-threading approach to handle request and  access I/Os on a two CPUs computer. When the second thread starts working on the IO the CPU is locked  and the CPU yellow is supporting 2 threads (1 and 3)</p> <p></p> <p>On public clouds, the blocking I/O approach inflates your monthly bill; on private clouds,  it reduces the deployment density.</p> <p>Non blocking IO framework or library adopts the <code>reactor pattern</code> where requests are internally asynchronous events processed, in order, by an event loop running in one thread, and handlers  (or callbacks) are used to process the response.</p> <p></p> <p>The above figure and next one are coming from Clement Escoffier's book  Building reactive microservice in Java.</p> <p>In multi CPUs, cores and threading computer, the reactors can run in parallel, with one event  loop per core:</p> <p></p> <p>With non-blocking I/O the I/O operation can be allocated to another pool and the allocation of CPU  to thread is well balanced: </p> <p></p> <p>Vert.X is the open source library to build such non-blocking I/O app, and using <code>vertices</code> to support  scalable concurrent processor, which executes one event loop thread. <code>vertices</code> communicate asynchronously  via an event bus.  </p>"},{"location":"advantages/reactive/#reactive-programming","title":"Reactive programming","text":"<p>Reactive programming is about observing asynchronous streams. Streams can be seen as a pipe in which events flow.  We observe the events flowing\u2014such as items, failures, completion, cancellations\u2014and implement side effects.</p> <p>You structure your code around streams and build chains of transformation, also called pipeline. Reactive programming libraries offer  countless operators that let you create, combine, filter, and transform the object emitted by streams.</p> <p>One important consideration is the speed of items processing by the consumer. If it is too slow compare to the producer, there will be a big proble, that  can be solved efficiency by using backprassure protocol. Reactive Streams is such backpressure protocol. It defines the concept of Subscriber who requests the Publisher a certain amount of items. The consumer controls the flow, as when items are received and processed, consumers can ask for more. Producer is not strongly coupled to the consumer as they participate together to the stream processing. Producer uses a Subscription object to act as a contract between the participants.</p> <p>different libraries support the Reactive Streams protocol, like Mutiny, RxJava, Project Reactor...</p>"},{"location":"advantages/reactive/#vertx","title":"Vert.x","text":"<p>We do not need to reintroduce Vert.X, but with the large adoption of Quarkus  to develop new JVM based microservice, Vert.x is an important library to understand. The main concepts  used are:</p> <ul> <li>An application would typically be composed of multiple vertices running in the same Vert.x instance and communicate with each other using events via the <code>event bus</code>.</li> <li>vertices remain dormant until they receive a message or event.</li> <li>Message handling is ideally asynchronous, messages are queued to the event bus, and control is returned to the sender</li> <li>Regular vertices are executed in the event loop</li> <li>Worker vertices are not executed on the event loop, which means they can execute blocking code</li> <li>Two event loops per CPU core thread</li> <li>No thread coordination mechanisms to manipulate a verticle state</li> <li>A verticle can be deployed several times as part of a container for example</li> <li><code>Event bus</code> is used by different vertices to communicate through asynchronous message passing  (JSON) (point to point, pub / sub, req / resp)</li> <li>We can have in memory event bus or clustered cross nodes, managed by Vert.x with a TCP protocol</li> </ul> <p>Quarkus HTTP support is based on a non-blocking and reactive engine (Vert.x and Netty). All the HTTP requests your application receive are handled by\u00a0event loops\u00a0(IO Thread) and then are routed towards the code that manages the request. Depending on the destination, it can invoke the code managing the request on a worker thread (Servlet, Jax-RS) or use the IO Thread (reactive route).</p> <p></p> <p>(Images src: quarkus.io)</p> <p>The application code should be written in a non-blocking manner using SmallRye Mutiny or RsJava  libraries.</p> <p>So when interactive with different services using kafka as an inter service communication layer  the producer and consumer are handlers and the internal processing can be schematized as:</p> <p></p> <p>Vert.x and reactive messaging applications may be combined to byild reactive systems to support the reactive manifesto:</p> <p></p> <ul> <li>To achieve resilience and responsiveness, microservice can scale vertically using vertices inside the JVM and horizontally via pod  scaling capability within a Kubernetes cluster. The inter-communication between vertices is done via <code>event bus</code> and managed by the Vert.x library  using virtual addresses, service discovery and event bus. </li> <li>Internal service communication (even cross pods) and external cross service boundary are message driven. Using Kafka, they are also  durable improving resilience and recovery.</li> <li>Kubernetes enforces part of the reactive manifesto at the container level: elasticity and resilience with automatic pod recovery and scheduling.</li> </ul>"},{"location":"advantages/reactive/#microprofile-reactive-messaging","title":"MicroProfile reactive messaging","text":"<p>The MicroProfile Reactive messaging specification  aims  to  deliver  applications  embracing  the characteristics of reactive systems as stated by reactive manifesto. It enables non-blocking, asynchronous message passing between services, giving them the ability to scale, fail, and evolve independently.</p> <p>To summarize the main concepts, developer declares channels as way to get incoming or outgoing messages between CDI Java beans and connector to external message brokers:</p> <p></p> <p>The potential matching declarations for the connector, for the above figure, may look like below:</p> <pre><code># Kafka connector to items topic mapped to the item-channel\nmp.messaging.incoming.item-channel.connector=smallrye-kafka\nmp.messaging.incoming.item-channel.topic=items\n</code></pre> <p>For code which defines the 'outhoing' and 'incoming' message processing see this quarkus guide, the EDA quickstart code templates for producer and consumer</p> <pre><code>public class OrderService {\n\n    @Channel(\"orders\")\n    public Emitter&lt;OrderEvent&gt; eventProducer;\n\n        public OrderEntity createOrder(OrderEntity order) {\n            try {\n            // build orderPayload based on cloudevent from order entity\n            Message&lt;OrderEvent&gt; record = KafkaRecord.of(order.getOrderID(),orderPayload);\n            eventProducer.send(record);\n            logger.info(\"order created event sent for \" + order.getOrderID());\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n}\n</code></pre> <p>JMS and message driven bean, were the messaging APIs to asynchronously communicate with other applications.  They support transaction and so it is an heavier protocol to use. They do not support asynchronous IO. </p> <p>When building microservices, the CQRS and event-sourcing patterns provide an answer to the data sharing between microservices.  Reactive Messaging can also be used as the foundation to CQRS and Event-Sourcing  mechanisms.</p> <p>A lot of EDA repositories demonstrating EDA concepts are using microprofile 3.0 Reactive Messaging, as it simplifies the implementation  to produce or consume messages to messaging middleware like Kafka.</p> <p>When you use <code>@Incoming</code> and <code>@Outgoing</code> annotations, the runtime framework (Open Liberty or Quarkus) creates a Reactive Streams component for  each method and joins them up by matching the channel names.</p> <p>A simple guide from Quarkus web site with integration with Kafka. </p> <p>Open Liberty supports this specification implementation.</p>"},{"location":"advantages/reactive/#mutiny","title":"Mutiny","text":"<p>Mutiny is a modern reactive programming library to provide more natural, readable reactive code. It supports asynchrony, non-blocking programming and streams, events, back-pressure and data flows.</p> <p>With Mutiny both Uni and Multi expose event-driven APIs: you express what you want to do upon a given event (success, failure, etc.). These APIs are divided into groups (types of operations) to make it more expressive and avoid having 100s of methods attached to a single class.</p> <p>This section of the product documentation goes over some examples on how to use Uni/ Multi.</p>"},{"location":"advantages/reactive/#amqp","title":"AMQP","text":"<p>Advanced Message Queueing Protocol is an international standard for interoperability between messaging middlewares. IBM MQ supports AMQP client via specific AMQP channel. Clients can connect to the queue manager and send / receive messages to / from queue. </p>"},{"location":"advantages/reactive/#knative-eventing","title":"Knative eventing","text":"<p>Knative is Kubernetes based platform to develop serverless. Major value proposition is a simplified deployment syntax with automated scale-to-zero and scale-out based on HTTP load. </p> <p>Knative consists of the following components:</p> <ul> <li>Eventing - Management and delivery of events</li> <li>Serving - Request-driven compute that can scale to zero</li> </ul> <p>See the RedHat Knative cookbook for a simple tutorial.</p>"},{"location":"advantages/reactive/#code-samples","title":"Code samples","text":"<ul> <li>Vert.x kafka client</li> <li>Experiences writing a reactive Kafka application</li> </ul>"},{"location":"advantages/reactive/#more","title":"More...","text":"<ul> <li>Reactive Systems Explained - Book from Grace Jansen - Peter Gollmar</li> <li>Reactive Java Modules show how to build an event-driven, streams-optimized Kafka-based Java application. You will use the Eclipse MicroProfile Reactive Messaging API and Open Liberty to build it and then you'll learn how to test it in true-to-production environments by using containers with MicroShed testing.</li> <li>Resiliency discussion in IBM architecture center</li> <li>Clement Escoffier's book Building reactive microservice in Java</li> </ul>"},{"location":"advantages/resiliency/","title":"Resiliency","text":"<p>The reduction in inter-dependency between applications that is enabled in an Event-Driven architecture enables increased resiliency. If services fail, they can restart autonomously and, subsequently, recover events and replay them if needed. Their ability to self-heal means that the functionality of the whole system is less reliant on certain services being immediately available. We are detailing how consumer offset management works and how to rebuild data projection after recovery in the Kafka Consumer article.</p> <p>Reduced coupling between services means they do not need to have any knowledge of the services to which they produce or from whom they consume. There are a number of advantages to this. For example, even if a service goes down, events will still be produced or consumed once it has recovered, known as 'Guaranteed Delivery'. </p> <p>For instance, let's say we run a shipping company that operates a fleet of container ships. The containers themselves could be smart IoT devices, in that they collect data about the health of the container (temperature, position etc). At the vessel level, we can use edge computing with local event backbone to do some simple aggregations and correlations before sending those data back at regular intervals to a central onshore monitoring platform. If the ship network goes offline and the refrigerator containers can not send back the data, it can still be collected and will be sent once the service is available again. We have resilience between data centers. Here is diagram illustrating those concepts with some underlying technologies.</p> <p></p> <p>Applications on the right, run in a data center or cloud provider, and receive aggregate data coming from the Kafka cluster running on the vessel. The topic data replication is done via Mirror Maker 2. A second level of real time analytics could compute aggregates between all the vessels sailing over seas. If the connection is lost the mirroring will get the records when reconnecting. On the vessel level, multiple brokers ensure high availability, and replication cross broker ensures data resilience. Real time analytic components can scale horizontally, even when computing global aggregate by using kafka streams capability of Ktable and store. </p>"},{"location":"advantages/scalability/","title":"Scalability","text":"<p>Event-Driven architectures are highly scalable. The use of an event-driven backbone allows for the addition (and removal) of consumers based on the number of messages waiting to be consumed from a topic. This is good for architectures where in-stream transformation is needed, like data science workflows. Messages can be consumed and transformed extremely fast, which is advantageous for processes where millisecond decision making is necessary. </p> <p>When using a system like Kafka, the data (in a given topic) is partitioned by the broker, which allows for parallel processing of the data for consumpton. Consumers are usually assigned to one partition.  </p> <p>As well as scaling up, there is also the ability to scale down (even to zero). When scaling up and down happens autonomously, promoting energy and cost efficiency, it is referred to as 'elasticity'. </p>"},{"location":"concepts/flow-architectures/","title":"Flow architecture","text":"<p>** Added 1/21/2022</p> <p>From the James Urquhart's book: Flow architecture and personal studies.</p> <p>As more and more of our businesses \u201cgo digital\u201d, the groundwork is in place to fundamentally change how real-time data is exchanged across organization boundaries. Data from different sources can be combined to create a holistic view of a business situation.</p> <p>Flow is networked software integration that is event-driven, loosely coupled, and highly adaptable and extensible.</p> <p>Value is created by interacting with the flow, and not just the data movement.</p> <p>Since the beginning of IT as an industry, we are digitizing and automating the exchanges of value, and we spend a lot of time and money to execute key transactions with less human intervention. However, most of the integrations we execute across organizational boundaries today are not in real time. Today, most, perhaps all\u2014digital financial transactions in the world economy still rely on batch processing at some point in the course of settlement.</p> <p>There is no consistent and agreed-upon mechanism for exchanging signals for immediate action across companies or industries.</p> <p>It is still extremely rare for a company to make real-time data available for  unknown consumers to process at will.</p> <p>This is why modern event-driven architecture (EDA) will enable profound changes in  the way companies integrate. EDAs are highly decoupled architectures, meaning there  are very few mutual dependencies between the parties at both ends of the exchange.</p>"},{"location":"concepts/flow-architectures/#1-flow-characteristics","title":"1- Flow characteristics","text":"<ul> <li>Consumer applications requests data streams through self-service interfaces, and get the data continuously.</li> <li>Producers maintain control of relevant information to transmit and when to transmit.</li> <li>Event packages information of data state changes, with timestamp and unique ID.  The context included with the transmitted data allows the consumer to better understand the nature of that data. CloudEvent helps defining such context.</li> <li>The transmission of a series of events between two parties is called an event stream.</li> </ul> <p>The more streams there are from more sources, the more flow consumers will be drawn to  those streams and the more experimentation may be done. Over time, organizations will find  new ways to tie activities together to generate new value.</p> <p>Composable architectures allow the developer to assemble fine grained parts using  consistent mechanisms for both inputting data and consuming the output. In contextual architectures, the environment provides specific contexts in  which integration can happen. Developer must know a lot about the data that is available,  the mechanism by which the data will be passed, the rules for coding and deploying   the software.</p> <p>EDA provides a much more composable and evolutionary approach for building event and data streams.</p>"},{"location":"concepts/flow-architectures/#2-business-motivations","title":"2- Business motivations","text":"<ul> <li>Do digital transformation to improve customer experiences. Customers expect their data to  be used in a way that is valuable to them, not just to the vendors. Sharing data between organizations  can lead to new business opportunities. This is one of the pilard of Society 5.0. </li> </ul> <p>The Japan government defined Society 5.0 as \"A human-centered society that balances economic  advancement with the resolution of social problems by a system that highly integrates cyberspace and physical space\".</p> <ul> <li>Improve process automation, to drive efficiencies and profitability. The most limiting constraint in the  process hides any improvements made to other steps. Finding constraints is where value stream mapping shines: it uses lead time (queue time) and actual time to do the work. EDA will help to get time stamp and data  for steps in the process that are not completely in scope of a business process: may be cross business boundaries.</li> <li>Extract innovative value from data streams. Innovation as better solution for existing problem, or as new solution to emerging problems.</li> </ul> <p>To improve process time, software needs accurate data at the time to process the work. As business evolve, having a rigid protocol to get the data, impacts process time. A business will need to experiment with new data sources  when they are available and potentially relevant to their business.</p> <p>Stream processing improves interoperability (exchange data)</p> <p>Innovation is not adaptation. Companies must adapt constantly just to survive, like adding features on a product to pace with competition. Digital transformation aimed at avoiding competitive disruption is not innovation.</p> <p>As the number of stream options grows, more and more business capabilities will be  defined in terms of stream processing. This will drive developers to find easier ways  to discover, connect to, and process streams.</p>"},{"location":"concepts/flow-architectures/#enabler-for-flow-adoption","title":"Enabler for flow adoption","text":"<ul> <li>Lowering the cost of stream processing: Integration costs dominate modern IT budgets. For many integrations, the cost of creating interaction between systems is simply too high for what little value is gained. With common interfaces and protocols that enable flows, the integration cost will be lower and people will find new uses for streaming that will boost the overall demand for streaming technologies. The Jevons paradox at work</li> <li>Increasing the flexibility in composing data flows: \"pipe\" data streams from one processing  system to another through common interfaces and protocols.</li> <li>Creating and utilizing a rich market ecosystem around key streams. The equities markets have all moved entirely to electronic forms of executing their marketplaces. Health-care data streams for building services around patient data. Refrigerators streaming data to grocery delivery services. </li> </ul> <p>Flow must be secure (producers maintain control over who can access their events),  agile (change schema definitions),  timely (Data must arrive in a time frame that is appropriate for the context to which it is being applied),  manageable and retain a memory of its past. </p> <p>Serverless, stream processing, machine learning, will create alternative to batch processing.</p>"},{"location":"concepts/flow-architectures/#3-market","title":"3- Market","text":"<p>SOA has brought challenges for adoption and scaling. Many applications have their own interfaces and even protocols to expose their functionality, so most integrations need protocol and  data model translations. </p> <p>The adoption of queues and adaptors to do data and protocol translation was a scalable solution.  Extending this central layer of adaptation was the Enterprise Service Bus, with intelligent pipes / flows. </p> <p>Message queues and ESBs are important to the development of streaming architectures but to support scalability and address complexity more decoupling is needed between  producers and consumers.</p> <p>For IoT MQTT is the standard for messaging protocols in a lightweight pub/sub  transport protocol. MQTT supports 3 service levels: 0 - at most once, 1- at least once, 2 - exactly once. It allows for messaging between device to cloud and cloud to device. It supports for persistent sessions  reduces the time to reconnect the client with the broker. The MQTT broker manages a list of topics, which enable it to identify groups of subscribers interested in a collection of messages.</p> <p>For event processing three type of engines:</p> <ul> <li>Functions (including low-code or no-code processors): AWS lambda, Knative eventing, Flink, Storm. Mendix and Vantiq have event-driven low code platform.</li> <li>log-based event streaming platforms: Apache Kafka, Apache Pulsar, AWS Kinesis, and Microsoft Azure Event Hubs. Topic becomes a system of record, as an event-sourcing pattern implementation.</li> <li>real-time stateful systems: Digital twins are software agents supporting the problem domain in a stateful manner. Behavior is supported by code or rules, and relationship between agents.  Agents can monitor the overall system state. Swim.ai builds its model dynamically from the event stream and provides built-in machine learning capabilities that enable both continuous learning and high performance model execution.</li> </ul> <p>Mainstream adoption of flow itself will be five to ten years from now (2020). Flow will have to prove that  it meets security criteria for everything from electronic payments, to health-care data, to classified  information. The CNCF\u2019s CloudEvents specification, for instance, strongly suggests payloads be encrypted. There is no single approach to defining an event with encryption explicitly supported  that can be read by any event-consuming application (MQTT, AMQP, have different encryption and TLS add more for  TCP connection).</p> <p>Consumers need assurances that the data they receive in an event is valid and accurate, a practice  known as data provenance.  </p> <p>Data provenance is defined as \u201ca record of the inputs, entities,  systems, and processes that influence data of interest, providing a historical record of the  data and its origins\"</p> <p>Provenance has to be maintained by the producer as a checksum number created by parsing the event data, and encrypted by the producer's key. CloudEvent has metadata about the message. When sent to Kafka they are  immutable record. Now the traceability of the consumers in kafka world is a major challenge. Blockchain may also be used to track immutable record with network parties attest its accuracy.</p> <p>Applying the concept of data loose value over time, it is important to act on data as early as possible, close to creation time. After a period of time data becomes less valuable.</p> <p>Two time factors are important in this data processing: latency (time to deliver data to consumers) and retention (time to keep data). For latency try to reduce the number of network segment between producer and consumers. Considering edge computing as a way to bring event processing close to the source. The event processing add time to the end to end latency. Considering constraining the processing time frame.</p> <p>Retention is a problem linked to the business requirements, and we need to assess for each topic how long an event is still valuable for the consumers. Not keeping enough events will impact correctness of consumer state,  projection views... keeping for too long, increase the cost of storage, but also the time to rebuild data  projection. </p> <p>Finally, producers will want to trust that only authorized consumers are using the events they produce. Also it may be possible to imagine a way to control the intellectual property of the data so producer can keep  its ownership. Data consumption should be done via payment like we do with music subscription.</p>"},{"location":"concepts/flow-architectures/#flow-patterns","title":"Flow patterns:","text":""},{"location":"concepts/flow-architectures/#collector-pattern","title":"Collector pattern","text":"<p>The Collector pattern is a pattern in which a single consumer subscribes to topics from multiple producers. </p>"},{"location":"concepts/flow-architectures/#distributor-pattern","title":"Distributor pattern","text":"<p>Each event in a stream is distributed to multiple consumers. It could be a hard problem to solve when doing it across geographically distributed systems. Edge computing can be used to distribute  streaming endpoints closer to the consumers that need those streams. Alternate  is to move the event processing close to the source. For many Distributor use cases,   partitioning the data by region is probably smart, and flow interfaces will need to take   this into account.</p>"},{"location":"concepts/flow-architectures/#signal-pattern","title":"Signal pattern","text":"<p>The Signal pattern is a general pattern that represents functions that exchange data between  actors based on a distinct function or process, in can be seen as a traffic cop.   It supports multiple producers and multiple consumers. The signal pattern is supported  by multiple event processing each handling one aspect of the event processing.</p> <p>Stream processing may route event streams between several distributed edge computing services as  well as core shared services, but then we need management layer to get global view of the systems. They need to be integrated into observability tool. But the \"single pane of glass\" is often a lure as distributed systems require distributed decision-making. More local solutions are more agile, flexible and better address local problems for improved resilience.  </p> <p>One of the challenge of complex adaptive systems is that any agent participating in  the system has difficulty seeing how the system as a whole operates, because of its limited connections to other neighbor agents.</p>"},{"location":"concepts/flow-architectures/#facilitator-pattern","title":"Facilitator pattern","text":"<p>A specialized form of Signal pattern, facilitator is a \"broker\" to match producers' events to consumers' demands. It is like matching sellers with buyers.</p>"},{"location":"concepts/flow-architectures/#4-identifying-flow-in-your-business","title":"4- Identifying flow in your business","text":"<p>The classical usee case categories:</p> <ul> <li> <p>Addressing and discovery: In modern complex systems environments, multiple systems need to be informed of the new entity, be able to utilize to assign work to it. Addressing and discovery happens across organizational boundaries (for example in real-time inventory SKU is used to identify item for both supplier and retailers). To seek such use cases, look at tracking problems like who or what is involved in a problem domain that is difficult to scale. With event stream centric approach, A&amp;D is done via a registry service used by new agents to indicate their existence, and the service publishes an event to a topic to broadcast the information about the new agent. A second option is to use a discovery service to watch specific event stream for certain transmissions that indicate the presence of an agent. Swim.ai continuously process and analyze  streaming data in concert with contextual data to inform business-critical, operational decisions. See also SwimOS or Apache Flink.</p> </li> <li> <p>Command and control: sources are connected to key decision-making and action-taking services to complete a business task. So they are everywhere in any business. A typical example of such use case, is the Supervisory Control And Data Acquisition, used in manufacturing, or energy production. Try to ask: where does the organization depend on timely responses to changes in state? C&amp;C can be supported by centralized control with events come from multiple sources to stateless or stateful services, to apply real-time analysis and decision-making algorithms to those streams. Output events are published to sinks for future processing. Scaling with a centralized control approach is not straightforward, as getting the right events to the right processing instances can be a challenge. Also when we need the compute global aggregates by looking at the state of various agents in the systems is more complex, as it needs to integrate with stateful stream processing. Actions can be triggered by state changes, triggers that fire at specific times, or even API requests from other applications or services.</p> </li> </ul> <p>An alternate is to use distributed control, like applying the decision-making logic at the edge. </p> <ul> <li>Query and observability: querying or monitoring individual agents or specific groups of agents. The problem is to locate the right agent target of the query, and get current state or history from that agent. </li> <li>Telemetry and analytics: focuses on understanding systems behavior, and get real-time big data insights (e.g. Click streams).  Need to assess which insights require understanding the emerging behavior of a system of agents emitting vast amounts of data.</li> </ul> <p>Interesting presentations:</p> <ul> <li>Voxxed Athens 2018 - Eventing, Serverless, and the Extensible Enterprise by Clemens Vasters</li> </ul>"},{"location":"concepts/flow-architectures/#5-model-flow","title":"5- Model Flow","text":"<p>Use Event storming to build a timeline of events that are required to complete a complex task, and to get and understanding of the people, systems, commands and policies that affect the event flow.  The Event Storming process is a highly interactive endeavor that :brings subject matter experts  together to identify the flow of events in a process or system</p> <ol> <li>Identify the business activities that you would like to model in terms of event flow</li> <li>Begin by asking the group to identify the events that are interesting and/or required for that business activity</li> <li>Place events along a timeline from earliest action to latest action</li> <li> <p>Capture:</p> <ul> <li>The real-world influences on the flow, such as the human users or external systems that produce or consume events</li> <li>What commands may initiate an event</li> <li>What policies are activated when an event takes place. A policy usually initiates a new command. Events always result in a policy action unless the event is generating output.</li> <li>What are the outputs from the event flow.</li> </ul> </li> </ol> <p>When designing the solution assess:</p> <ul> <li>When the event is simply to broadcast facts for consumption by interested parties. The producer contract is  simply to promise to send events as soon as they are available.</li> <li>If consumers can come and go, and experiment with the consumption of a stream with little risk of consequences if they choose not to continue</li> <li>When event is part of an interaction around an intent, requiring a conversation with the consumer</li> <li>Is the event a stand-alone communication, discrete, or is it only useful in a context that includes a series of events.  Series applications are where log-based queueing shines</li> <li>Is the processing involve one simple action per event, or is there a group of related actions, a workflow, required to complete processing</li> </ul> <p>When building a flow-ready application for messaging, the \u201ctrunk-limb-branch-leaf\u201d pattern is a  critical tool to consider: use edge computing to distribute decision-making close to the related  groups of agents, computing local aggregates, and propagate to larger more central flows. Using messaging middleware to manage interaction between agents, to isolate message distribution to just the needed servers and agents, and propagate aggregates to the trunk, greatly reducing traffic between the original agents and the core.</p> <p>Another consideration is to assess if the consumers need to filter events from a unique topic before doing its own processing, in this case the event payload may include metadata and URL to get the payload. If the metadata indicates an action is required, the consumer can then call the data retrieval URL.</p> <p>Whether or not you include payload data depends a bit on the volume of events being published and the security and latency requirements of your application.</p> <p>Log-based queues can play the role of \u201csystem of record\u201d for both event values and sequence, especially for systems that need both the most recent event and an understanding of the recent history of events received</p> <p>For single action processing, serverless, knative eventing are technologies to consider. Solution needs to route events to the appropriate processor. But if your event processing needs require  maintaining accurate state for the elements sending events then stateful streaming platform are better fit.</p> <p>For workflow, modern solutions, simplify creating and managing process definitions independent of the actions taken in that process. It supports for stepping an event through multiple interdependent actions. Workflow may require to wait for another related event occurs or a human completes his action.</p>"},{"location":"concepts/flow-architectures/#6-today-landscape","title":"6- Today landscape","text":"<ul> <li>Standards are important for flow:  TLS, WebSockets, and HTTP from IETF, MQTT and AMQP from OASIS,  CloudEvents and the Serverless Working Group from CNCF</li> <li> <p>Open sources projects: </p> <ul> <li>Apache Kafka and Apache Pulse for log-based queueing</li> <li>Apache Beam, Flink, Heron, Nifi, Samza, and Storm for stream processing</li> <li>Apache Druid as a \u201cstream-native\u201d database</li> <li>gRPC may play a key role in any future flow interface</li> <li>NATS.io, a cloud-native messaging platform</li> <li>Argo, a Kubernetes-based workflow manager that theoretically could act as the core of an event-driven process automation bus</li> </ul> </li> <li> <p>Opportunities:</p> <ul> <li>Data provenance and security for payloads passed between disparate parties</li> <li>Tracking event data distribution across the world wide flow. Where does the data generated by an event end up being consumed or processed?</li> <li>Platforms for coordinating event processing, aggregation, and synchronization between core data center event processors, edge computing environments, and end-user or IoT devices</li> <li>Monetization mechanisms for all types of event and messaging streams</li> </ul> </li> </ul> <p>The adoption of a technology is not the delivery that makes it valuable, but the ecosystem that consumes it.</p> <p>Look at existing streams and determine how to add value for the consumers of that stream.  Can you automate valuable insights and analytics in real time for customers with shared needs?  Would it be possible to recast the stream in another format for an industry that is currently  using a different standard to consume that form of data? </p>"},{"location":"concepts/events-versus-messages/","title":"Event Streaming versus Queuing","text":"<p>Info</p> <p>Updated 6/18/2022</p> <p>Consider queue system. like IBM MQ, for:</p> <ul> <li>Exactly once delivery, and to participate into two phase commit transaction</li> <li>Asynchronous request / reply communication: the semantic of the communication is for one component to ask a second command to do something on its data. This is a command pattern with delay on the response.</li> <li>Recall messages in queue are kept until consumer(s) got them.</li> </ul> <p>Consider streaming system, like Kafka, as pub/sub and persistence system for:</p> <ul> <li>Publish events as immutable facts of what happened in an application</li> <li>Get continuous visibility of the data Streams</li> <li>Keep data once consumed, for future consumers, for replay-ability</li> <li>Scale horizontally the message consumption</li> </ul>"},{"location":"concepts/events-versus-messages/#events-and-messages","title":"Events and Messages","text":"<p>There is a long history of messaging in IT systems.  You can easily see an event-driven solution and events in the context of messaging systems and messages. However, there are different characteristics that are worth considering:</p> <ul> <li>Messaging: Messages transport a payload and messages are persisted until consumed. Message consumers are typically directly targeted and related to the producer who cares that the message has been delivered and processed.</li> <li>Events: Events are persisted as a replayable stream history. Event consumers are not tied to the producer. An event is a record of something that has happened and so can't be changed. (You can't change history.)</li> </ul> <p></p>"},{"location":"concepts/events-versus-messages/#messaging-versus-event-streaming","title":"Messaging versus event streaming","text":"<p>We recommend reading this article and this one, to get insight on messaging (focusing on operations / actions to be performed by a system or service) versus events (focusing on the state / facts of a system with no knowledge of the downstream processing).</p> <p>To summarize messaging (like MQ) are to support:</p> <ul> <li>Transient Data: data is only stored until a consumer has processed the message, or it expires.</li> <li>Request / reply most of the time.</li> <li>Targeted reliable delivery: targeted to the entity that will process the request or receive the response. Reliable with transaction support.</li> <li>Time Coupled producers and consumers: consumers can subscribe to queue, but message can be remove after a certain time or when all subscribers got message. The coupling is still loose at the data model level and interface definition level.</li> </ul> <p>For events:</p> <ul> <li>Stream History: consumers are interested in historic events, not just the most recent.</li> <li>Scalable Consumption: A single event is consumed by many consumers with limited impact as the number of consumers grow.</li> <li>Immutable Data</li> <li>Loosely coupled / decoupled producers and consumers: strong time decoupling as consumer may come at anytime. Some coupling at the message definition level, but schema management best practices and schema registry reduce frictions.</li> </ul> <p>See also the MQ in an event-driven solution context article</p> <p>See this code (Store sale simulator) to produce messages to different middleware: RabbitMQ, IBM MQ or Kafka.</p>"},{"location":"concepts/fit-to-purpose/","title":"Fit for purpose","text":"<p>Updated 06/18/2022</p> <p>In this note we want to list some of the main criteria to consider and assess during an event-driven architecture establishment work  or during the continuous application governance. This is not fully exhaustive, but give good foundations for analysis and study. Fit for purpose practices should be done under a bigger program about application development governance and data governance.</p>"},{"location":"concepts/fit-to-purpose/#cloud-native-applications","title":"Cloud native applications","text":"<p>With the adoption of cloud native and microservice applications (the 12 factors app), the followings need to be addressed:</p> <ul> <li>Responsiveness with elastic scaling and resilience to failure. Which leads to adopt the 'reactive manifesto' and consider messaging as a way to communicate between apps. Elastic also may lead to multi-cloud deployments.</li> <li>Address data sharing using a push model to improve decoupling, and performance. Instead of having each service using REST endpoints to pull the data from other services, each service pushes the change to their main business entity state to a event backbone.  Each future service in need for those data, pulls from the messaging system.</li> <li>Adopting common patterns like command query responsibility seggregation to help implementing complex queries, joining different business entities owned by different microservices, event sourcing to build logs of what happend, transactional outbox and SAGA for long running transaction.</li> <li>Addressing data eventual consistency to propagate change to other components versus ACID transaction.</li> <li>Support \"always-on\" approach with the deployment to multiple data centers (at least three) being active/active and being able to propagate data in all data centers.</li> </ul> <p>Supporting all or part of those requirements will lead to the adoption of event-driven microservices and architecture.</p>"},{"location":"concepts/fit-to-purpose/#motivation-for-data-streaming","title":"Motivation for data streaming","text":"<p>The central value propositions of data stream are to: </p> <ul> <li>lower the cost of integrating via event streams, </li> <li>use event streams to signal state changes in near-real time</li> <li>replay the past to build data projection</li> </ul> <p>Applying the concept of data loose value over time, it is important to act on data as early as possible, close to creation time. After a period of time data becomes less valuable.</p> <p>Two time factors are important in this data processing: </p> <ul> <li>latency (time to deliver data to consumers)</li> <li>retention (time to keep data). </li> </ul> <p>For latency try to reduce the number of network segment between producer and consumers. Considering edge computing as a way to bring event processing close to the source. The event processing adds time to the end to end latency. Considering constraining the processing time frame.</p> <p>Retention is a problem linked to the business requirements, and we need to assess for each topic how long an event is still valuable for the consumers. Not keeping enough events will impact correctness of consumer state,  projection views... keeping for too long, increase the cost of storage, but also the time to rebuild data  projection. </p>"},{"location":"concepts/fit-to-purpose/#modern-data-pipeline","title":"Modern data pipeline","text":"<p>As new business applications need to react to events in real time, the adoption of event backbone is really part of the IT toolbox.  Modern IT architecture encompasses the adoption of new data hub, where all the data about a 'customer', for example, is accessible in one event backbone.  Therefore, it is natural to assess the data movement strategy and assess how to offload some of those ETL jobs running at night,  by adopting real time data ingestion. </p> <p>We detailed the new architecture in this modern data lake article, so from a fit for purpose point of view,  we need to assess the scope of existing ETL jobs, and refector to streaming logic that can be incorporated into different logs/ topics. With Event Backbone like Kafka, any consumer can join the data log consumption at any point of time, within the retention period.  By moving the ETL logic to a streaming application, we do not need to wait for the next morning to get important metrics.</p>"},{"location":"concepts/fit-to-purpose/#mq-versus-kafka","title":"MQ Versus Kafka","text":"<p>We already addressed the differences between Queueing and Streaming in this chapter.</p> <p>Now in term of technologies we can quickly highlight the followings:</p>"},{"location":"concepts/fit-to-purpose/#kafka-characteristics","title":"Kafka characteristics","text":"<ul> <li>Keep message for long period of time, messages are not deleted once consumed</li> <li>Suited for high volume and low latency processing</li> <li>Support pub/sub model only</li> <li>Messages are ordered in a topic/partition but not cross partitions</li> <li>Stores and replicates events published to a topic, remove on expired period or on disk space constraint</li> <li>Messages are removed from file system independent of applications </li> <li>Topic can have multiple partitions to make consumer processing parallel.</li> <li>Not supporting two phase commits / XA transaction, but message can be produced with local transaction</li> <li>Multi-region architecture requires data replication across regions with Mirror Maker 2</li> <li>Applications (producers, consumers, or streaming) are going to a central cluster.</li> <li>Cluster can support multi availability zones</li> <li>But also support extended cluster to go over different regions if those regions have low latency network</li> <li>Scales horizontally, by adding more nodes</li> <li>non-standard API but rich library to support the main programming language.</li> <li>But support also HTTP bridge or proxy to get message sent via HTTP</li> <li>when there is a problem on the broker it takes a lot of time to recover and it impacts all consumers</li> <li>Kafka is easy to setup with kubernetes deployment with real operator, is more difficult to manage with bare metal deployment.</li> <li>Cluster and topics definition can be managed with Gitops and automatically instantiated in new k8s cluster</li> <li>Consumer and producers are build and deployed with simple CI/CD pipeline</li> </ul>"},{"location":"concepts/fit-to-purpose/#mq-characteristics","title":"MQ characteristics","text":"<ul> <li>Best suited for point-to-point communication </li> <li>Supports horizontal scaling and high volume processing</li> <li>Supports event mesh, where Brokers can be deployed in different environments and serve the async applications locally,  improving communication, performance, placement and scalability.</li> <li>The local transaction supports to write or read a message from a queue is a strength for scalability as if a consumer is not able to process the messagem another one will do. In Kafka, one  partition is assigned to a consumer, and may be reassigned to  a new consumer after failure, which leads to a very costly work of partition rebalancing.</li> <li>Participates to two-phase commit transaction</li> <li>Exactly one delivery with strong consistency</li> <li>Integrate with Mainframes: transactional applications on mainframe </li> <li>Support JMS for JEE applications</li> <li>Support AMQP for lighter protocol</li> <li>Messages are removed after consumption, they stayed persisted until consumed by all subscribers</li> <li>Strong coupling with subscribers, producer knows its consumers</li> <li>Supports MQ brokers in cluster with leader and followers.</li> <li>Support replication between brokers</li> <li>Support message priority</li> <li>Support dynamic queue creation. Typical case it replay to queue.</li> <li>Support much more queue per broker so it is easier to scale.</li> <li>Easily containerized and managed with Kubernetes operators.</li> </ul>"},{"location":"concepts/fit-to-purpose/#direct-product-feature-comparison","title":"Direct product feature comparison","text":"Kafka IBM MQ Kafka is a pub/sub engine with streams and connectors MQ is a queue,or pub/sub engine All topics are persistent Queues and topics can be persistent or non persistent All subscribers are durable Subscribers can be durable or non durable Adding brokers to requires little work (changing a configuration file) Adding QMGRs requires some work (Add the QMGRs to the cluster, add cluster channels.  Queues and Topics need to be added to the cluster.) Topics can be spread across brokers (partitions) with a command Queues and topics can be spread across a cluster by adding them to clustered QMGRs Producers and Consumers are aware of changes made to the cluster All MQ clients require a CCDT file to know of changes if not using a gateway QMGR Can have <code>n number</code> of replication partitions Can have 2 replicas (RDQM) of a QMGR, Multi Instance QMGRs Simple load balancing Load balancing can be simple or more complex using weights and affinity Can reread messages Cannot reread messages that have been already processed All clients connect using a single connection method MQ has Channels which allow different clients to connect, each having the ability to have different security requirements Data Streams processing built in, using Kafka topic for efficiency Stream processing is not built in, but using third party libraries, like MicroProfile Reactive Messaging, ReactiveX, etc. Has connection security, authentication security, and ACLs (read/write to Topic) Has connection security, channel security, authentication security, message security/encryption, ACLs for each Object, third party plugins (Channel Exits) Built on Java, so can run on any platform that support Java 8+ Latest native on AIX, IBM i, Linux systems, Solaris, Windows, z/OS, run as Container Monitoring by using statistics provided by Kafka CLI, open source tools, Prometheus Monitoring using PCF API, MQ Explorer, MQ CLI (runmqsc), Third Party Tools (Tivoli, CA APM, Help Systems, Open Source, etc)"},{"location":"concepts/fit-to-purpose/#migrating-from-mq-to-kafka","title":"Migrating from MQ to Kafka","text":"<p>When the real conditions as listed above are met, architects may assess if it makes sense to migrate MQ application to Kafka.  Most of the time the investment is not justified. Modern MQ supports the same DevOps and deployment pattern as other cloud native applications.</p> <p>JEE or mainframe applications use MQ in transaction to avoid duplicate messages or loss of messages. Supporting exactly once delivery in Kafka needs some configuration and participation of producer and consumers: far more complex to implement.</p> <p>We recommend adopting the two messaging capabilities for any business applications.</p>"},{"location":"concepts/fit-to-purpose/#kafka-streams-vs-apache-flink","title":"Kafka Streams vs Apache Flink","text":"<p>Once we have setup data streams, we need technology to support near real-time analytics and complex event processing. Historically, analytics performed on static data was done using batch reporting techniques. However, if insights have to be derived in near real-time, event-driven architectures help to analyse and look for patterns within events.</p> <p>Apache Flink (2016) is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. It is considered to be superior to Apache Spark and Hadoop. It supports batch and graph processing and complex event processing.  The major stream processing features offered by Flink are:</p> <ul> <li>Support for event time and out of order streams: use event time for consistent results</li> <li>Consistency, fault tolerance, and high availability: guarantees consistent state updates in the presence of failures and consistent data movement between selected sources and sinks</li> <li>Low latency and high throughput: tune the latency-throughput trade off, making the system suitable for both high-throughput data ingestion and transformations, as well as ultra low latency (millisecond range) applications.</li> <li>Expressive and easy-to-use APIs in Scala and Java: map, reduce, join, with window, split,... Easy to implement the business logic using Function.</li> <li>Support for sessions and unaligned windows: Flink completely decouples windowing from fault tolerance, allowing for richer forms of windows, such as sessions.</li> <li>Connectors and integration points: Kafka, Kinesis, Queue, Database, Devices...</li> <li>Developer productivity and operational simplicity: Start in IDE to develop and deploy and deploy to Kubernetes, Yarn, Mesos or containerized</li> <li>Support batch processing</li> <li>Includes Complex Event Processing capabilities</li> </ul> <p>Here is simple diagram of Flink architecture from the Flink web site:</p> <p></p> <p>See this technology summary.</p> <p>See also this article from Confluent about comparing Flink with Kafka Streams.</p>"},{"location":"concepts/integration/","title":"Integration reference architecture","text":"<p>Updated 06/18/2022</p> <p>Event driven architecture is a complement of the overall integration reference architecture as presented in IBM Cloud architecture center.</p> <p>In this note, we want to summarize some of the important aspects of agile integration and how some of the technologies delivered as part of IBM Cloud Pak for Integration are used when doing application modernization with event-driven microservice. Some of our labs and reference implementation code, use API management, MQ and APP Connect.</p> <p>First let do a quick review of the major concepts for agile integration.</p>"},{"location":"concepts/integration/#agile-integration-concepts","title":"Agile integration concepts","text":"<p>The main agile integration concepts as presented in detail in the IBM cloud on agile integration article can be summarized as:</p> <ul> <li>Empower extended teams to create integrations, leveraging a complete set of integration styles and capabilities to increase overall team productivity.</li> <li>Agile integration includes container-based, decentralized, microservices-aligned approach for integrating solutions</li> <li>Existing centralized integration architectures, based on ESB pattern, cannot support the demand, in term of team reactivity and scalability at the internet level.</li> <li>ESB pattern provides standardized synchronous connectivity to back-end systems typically over web services (SOAP based). ESB formed a single infrastructure for the whole enterprise, with tens or hundreds of integrations installed on a production server cluster.</li> <li>A single, centralized ESB certainly simplifies consistency and governance of implementation.</li> <li>Interface maintenance is expensive.</li> <li>Any deployment to the shared servers runs the risk of destabilizing existing critical interfaces.</li> <li>SOA encounters the issue of getting the funding at the enterprise wide program to maintain reusable interface.</li> <li>Integration teams are becoming the bottleneck instead of being enabler.</li> <li>SOA is about real-time integration between applications, whereas a microservices architecture is about how the applications are built internally.</li> <li> <p>Microservice enables greater agility by being:</p> <ul> <li>small enough to be understood completely by their owning team and changed independently</li> <li>elastic to scale horizontally</li> <li>resilient with changes to one microservice will not affect others at runtime</li> </ul> </li> </ul> <p>The following diagram illustrates the agile integration modernization transition from a centralized ESB type of architecture, and breaking integration into smaller  pieces to make them more agile and more scalable.</p> <p></p> <p>In this modernization process, development team can introduce API Management to improve decoupling between consumers and providers ultimately moving to a decentralized approach where each team can manage their own integration.</p> <ul> <li> <p>Three aspects to agile integration:</p> <ol> <li>Decentralized integration ownership: give application teams more control over the creation and exposure of their own integration exposed as APIs, or messages</li> <li>Fine-grained integration deployment to separate integration, scalable independently. Changes to individual integration flows can be automatically rebuilt and deployed independently of other flows to enable safer application of changes and maximize speed to production.</li> <li>Cloud-native integration infrastructure to improve productivity, operational consistency and portability for both applications and integration</li> </ol> </li> </ul>"},{"location":"concepts/integration/#cloud-pak-for-integration-capabilities-relevant-to-this-site","title":"Cloud Pak for Integration capabilities relevant to this site","text":""},{"location":"concepts/integration/#api-management","title":"API management","text":"<ul> <li>Easier to discover new business assets (APIs and events) in current enterprise systems</li> <li>Existing enterprise assets are made available to new channels and new audiences, with enriched customer experience in integrated omnichannel interactions</li> <li>Support asynchAPI.</li> </ul>"},{"location":"concepts/integration/#app-connect","title":"App connect","text":"<p>Connect applications and data sources on premises or in the cloud to coordinate the exchange of business information so that data is available when and where it\u2019s needed.</p> <p>App Connect capabilities:</p> <ul> <li>Low-code/no-code integration tooling leverages natural language processing (NLP) and AI to offer custom mapping suggestions when building integration flows.</li> <li>Pre-built smart connectors and a common, shareable asset repository increases speed of delivery and eliminates concerns about mismatched sources, formats, or standards</li> </ul>"},{"location":"concepts/integration/#labs","title":"Labs","text":"<ul> <li>Secure API, App connect and MQ: older version of the UI in screen shots but still relevant use case.</li> </ul>"},{"location":"concepts/integration/#messaging","title":"Messaging","text":"<p>Messaging is to support asynchronous communication between applications. Two technologies are part of the reference architecture, queuing and pub/sub.</p>"},{"location":"concepts/integration/#ibm-mq","title":"IBM MQ","text":"<p>See a technology summary in this note and this MQ to Kafka lab. </p>"},{"location":"concepts/integration/#kafka-based-product","title":"Kafka based product","text":"<p>This site includes a lot of content around Kafka (see this technology summary), but the major capabilities of interest in term of agile integration is the pub/sub model, long term persistence via append log and replication to support high availability and resiliency, with data streaming logic, and a lot of connectors to source data or sink data to external systems. </p> <p>Kafka scale and is a reliable messaging system for modern event-driven microservice solution.</p>"},{"location":"concepts/integration/#bridge-your-digital-ecosystem-and-core-enterprise","title":"Bridge your digital ecosystem and core enterprise","text":"<p>At a high level, modern integration involves bridging the capabilities between your digital ecosystem and your traditional core enterprise. The bridging takes place in a seamless, frictionless way so that you can uniformly operate your entire business anytime, anywhere, regardless of technological fluctuations.</p> <p>In your digital transformation journey, your digital ecosystem and your core enterprise constantly change. You need a set of integration capabilities to support rapid change to interface, develop new business oriented integration flow to consume cloud services and software as a service, get visibility to the data in motion, integrate with existing transactional systems and system of record. </p> <p></p> <p>You modern cloud native applications use microservice design, function as a service, and may use agile, no code, integration logic to integrate with existing systems, SOA services, or cloud based services. Modern applications are reusing public cloud services, like CRM application, Database as a service, Chat bot as a services... Those services can be offered by different cloud providers, and architects will select the best services according to their requirements and cost / benefits analysis. A unique cloud provider will not have all the best useful services, and hybrid cloud is a standard approach in the 2020s. What is important is to get a set of tool that makes the integration easy to do with simple configuration to integrate with the needed data and inject those data in the operational messaging system for other to consumer. This agile integration follows the same DevOps pattern as other microservices. </p> <p>The messaging layer can support point to point, request/reply type of communication, or a pub/sub model with long retention time, and data streams processing. This data injection layer can be a buffer to modern data lake.</p> <p>Finally existing applications, system of records, transactional systems have to be integrated, consumed and accessed from modern applications, with new digital channel like mobile and single page web application.  </p> <p>API management is an important elements of the integration, to manage and provide API economy but also secure access to internal systems, with controlled traffic. This is the role of the API gateway.</p>"},{"location":"concepts/integration/#optimize-your-integration-platform","title":"Optimize your integration platform","text":"<p>A vendor neutral hybrid cloud, that uses open standards and container orchestration technology, presents the optimal platform for modern integration. It addresses multicloud operations, different deployment options, and integration patterns within a modular and scalable environment.</p> <p>As shown in the diagram below, the integration platform must accommodate many integration patterns and have that support ready on demand and as a self-service model.</p> <p></p> <ul> <li> <p>API integration enables synchronous access to fine-grained services, such as create, retrieve, update, and delete operations to business objects across various backends. Thus, the composition of the integration logic on the consumer side. Modern API management also includes the management of AsynchAPI for messaging systems and asynchronous communication between event-driven microservices.</p> </li> <li> <p>Application data integration enables synchronous access to coarse-grained services such as transaction processing across various backends in accordance with enterprise compliance requirements. Thus, the governance of integration logic on the provider side.</p> </li> <li> <p>Enterprise messaging enables asynchronous point-to-point access to services such as those that involve closed heritage systems, transactional integrity systems or heterogeneous partner backends.</p> </li> <li> <p>Event publish/subscribe integration enables asynchronous many-to-many coordination of services across both cloud and on-premises components in an event-driven architecture context.</p> </li> <li> <p>File transfer enables batch integration between SORs that involves the movement of large data files for content across vast physical distances within short time windows.</p> </li> </ul> <p>The integration platform must provide a unified framework for security, management operations, and resiliency. The container orchestration platform provides resiliency through the elasticity of container clusters and platform-level security. The unified management component provides ease of operations with a single view across all integration components within the platform. The gateway services provide runtime-level security and enforce access control policies to integration services.</p>"},{"location":"concepts/integration/#api-lifecycle-management-architecture","title":"API lifecycle management architecture","text":"<p>The API lifecycle management reference architecture bridges the gap between cloud and on-premises applications quickly and easily. It allows customers to securely unlock IT assets and to deliver innovative applications with modern architectures.</p> <p></p> <p>An international bank develops a new business model in a new ecosystem. This model exposes their rewards enterprise application program with a retail partner's online and mobile applications via self-service access to the program APIs. The bank's objective is to leverage the hybrid cloud capability to manage a comprehensive end-to-end integrated experience across the API lifecycle: create, run, manage, secure, socialize, and analyze APIs.</p> <ol> <li>An API developer signs on to the API management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the synch API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to API management. He can also create Asynch APIs from a messaging system by binding channels to topic or queue and define message payload definition.</li> <li>API owner signs on to the API management cloud services account and accesses the API management component. She includes the synch API endpoint to existing API products, and plans and specifies access control. She publishes the API to the developer portal for external discovery by application developers.</li> <li>An application developer accesses the developer portal, uses search, and discovers the API.</li> <li>The application developer uses the API in an application and deploys that application to the device.</li> <li>The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway validates access policies with API management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the back end. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API management.</li> <li>API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics.</li> </ol>"},{"location":"concepts/model/","title":"Different Data Models","text":"<p>Updated 06/18/2022</p> <p>The development of modern business application using cloud native microservices, long running business processes, decision services, different data stores, events, existing software as service like CRM or ERP applications, AI,... are adding complexity on the data view, data transformation and the data modeling. Some older approaches of adopting a unique canonical model, has some advantages and some counter productive ones. </p> <p>With microservice adoption, and applying domain driven design, it is easier to define a bounded context for microservice, but as soon as you start coding you immediately encounter questions about your information model and what shapes it should have. Canonical model is kept at the messaging or integration layer to do a one to one mapping between models. </p> <p>This note presents the reality view of the different data model to consider. The goal is still to apply clear separation of concern, focus on data needed for implementing business logic, and adapt interface for easy consumption and reuse.</p> <p>The following figure illustrates a potential business automation solution using different components, supported on top of different software products.</p> <p></p> <p>From Top-Left bottom right:</p> <ul> <li>Single Page applications are running in Web Browser and use different javascript library to support HTML rendering, business logic implementation and data model as JSON document. The model view is focusing to expose data to user and to support form entries. The app interacts with one Backend For Frontend app's REST API or other services with CORS support. The model and API are designed for this user centric application. </li> <li>Microservices are exposing model via their APIs, in Java EE, it used to be named Data Transfer Object, and it is still used, named this way in microservice. The focus on this model is to enforce reusability, and expose APIs about the main business entity or aggregate (Domain Driven Design - Aggregate). This entity is persisted in different formats, and when using traditional SQL based technology, Object Relational Mapping is needed. In Java, hibernate ORM is used with JPA to support transaction and entity annotation. Quarkus Panache is a nice way to map business entity transparently to DB. But still we need to design those models. Within the microservice, we will also find specific implementation model used to support the business logic the service implements. </li> <li>The adoption of JSON document to get a more flexible, schemaless model, helps at the technology selection to be more agile as the model can evolve easily, but still we need to think and design such models. </li> <li>Modern business applications are using Artificial Intelligence model. To make it more generic, we defined a predictive scoring service which computes a score between 0 to 1, given a feature set represented as tuples. It does not have to be a predictive scoring and can be anything a machine learned model can support, but it is exposed with an API and a flat data model.</li> </ul> <p>In the middle we have an integration layer:</p> <ul> <li>The modern event backbone to support event streaming. The event model is also defined with schemas, and any application producing such events, needs to have a specific model to represent fact, of what happens in their boundary.</li> <li>Enterprise service buses are still in scope, as service gateway, interface mapping, and heteregeonous integration are needed. The model definition will better fit to the adoption of canonical model to facilitate the one to one mapping between source and destination models.  </li> <li>Those integration can do data mapping with application like CRM, ERP running as system. The name ASBO, for application specific business object, was used in the past, and it still applies in modern solutions. We need to consider them as-is or by using mapping to internal models. </li> <li>Some business applications need to get human involved, in the form of workflow, where process application implements the workflow and define process variables to keep data between process tasks. Those data are persisted with process instances within the Business Process Management datasource. The business process can be triggered by user interface (claiming to start a process), by exposing a service, in this case we are back to the DTO definition, or triggered asynchronously via messages (back to a message model). </li> <li>Externalizing business rules, is still a relevant pattern, and in the case of decision modeling, a specific data model to support the rules inference and simplify the rule processing is needed. This is what we called RBO: Rule Business Object in the Agile Business Rule Development methodology.</li> </ul> <p>The persisted information model (in system of records or DB) is different than the process variables in BPM, which is different than the rule business object model used to make decisions. </p> <p>A good architecture leverages different information model views to carry the data to the different layers of the business application: the user interface, the database, the service contract, the business process data, the messaging structure, the events, the business rules, the raw document, the AI scoring input data.... There is nothing new in this, but better to keep that in mind anyway when developing new solutions.</p> <p>The information model for such service has to be designed so there is minimum dependency between consumers and providers. The approach of trying to develop a unique rich information model to support all the different consumers, may lead having the consumers dealing with complex data structure sparsely populated, and will increase coupling between components.</p> <p>When defining service specifications it is important to assess if each business entities supported by the service operations is going to have different definitions or if the same definition applies to the whole service portfolio?</p> <p>The scope of reuse of data types is an important architecture decision; there is no one-size-fits-all solution. However, a few different patterns are emerging:</p> <ul> <li>One object model per interface: Using an independent data model for every service interface assures the highest level of decoupling between services. As negative, the consumers have to understand different representations of the same business entities across multiple services and have to cope with all the relative data transformations. This strategy can be very effective for coarse-grained service operations that do not exchange large amount of data with consumers.</li> <li>One object model per business domain: the service information models are organized in domains, every domain sharing the same data model. The downside of this approach, is once domains are defined, changing their boundaries can be costly.</li> <li>A single object model for the whole enterprise: the approach is to define a single common set of data structures shared across all the enterprise service interfaces. The amount of model customization is kept to a minimum and its management is centralized. The negatives are having overly complicated model the consumers need to process. As of now it becomes a bad practices to adopt such canonical model at the API, persistence and eventing level. </li> </ul>"},{"location":"concepts/service-mesh/","title":"Microservice mesh","text":"<p>Updated 06/18/2022</p> <p>In this note we are grouping the studies around microservice to microservice communication with Kubernetes deployment. We are addressing:</p> <ul> <li>how ingress controller helps inside Kubernetes</li> <li>how API gateway helps for API management and service integration</li> <li>how to expose service in hybrid cloud</li> <li>how to discover service</li> </ul> <p>All come back to the requirements, skill set and fit to purpose.</p>"},{"location":"concepts/service-mesh/#definitions","title":"Definitions","text":"<p>Service meshes provide visibility, resiliency, traffic, and security control of distributed application services. They deliver policy-based networking for microservices in the contraints of virtual network and continuous topology updates. Externalizing, via declarations, the logic to support network potential issues, like resiliency, simplifies dramatically developers work.   </p> <p>Some misconception to clarify around microservice and APIs:</p> <ul> <li>microservices are not fine grained web services</li> <li>APIs are not equivalent to microservices</li> <li>microservices implement APIs in the scope of their domain. Separation between queries and commands may lead to two different services so the APIs will be splitted.</li> <li>microservice implements business logic</li> </ul> <p>API is an interface, a way to make a request to get or change data in an application. In modern use API refers to REST web APIs using HTTP protocol, with JSON format (sometime XML is still used). Interface decouples the caller from the implementation. The caller has no idea how API is implemented.</p> <p>A microservice is in fact a component. Micro refers to the granularity of the component not of the exposed interface. The following diagram illustrates all those concepts.</p> <p></p> <p>We encourage you to go to read integration design and architecture series.</p> <p>Container orchestration like Kubernetes are mainly doing application scheduling, cluster management, resource provisioning, platform and workload monitoring and service discovery.</p> <p>When application solutions are growing in size and complexity, you need to addres the following subjects:</p> <ul> <li>visibility on how traffic is flowing between microservices, how routing is done between microservices based on requests content or the origination point or the end point</li> <li>how to support resiliency by handling failure in a graceful manner</li> <li>how to ensure security with identity assertion</li> <li>how to enforce security policy   </li> </ul> <p>Addressing those subjects help defining the requirements for service mesh.</p> <p>Service mesh architecture defines a data and control planes:</p> <ul> <li>Control plane: supports policy and configuration for services in the mesh, and provides aggregation for telemetry. It has API and CLI to centralize control to the services deployed. In Kubernetes control planes are deployed in a system namespace.</li> <li>Data plane: handles the actual inspection, transit, and routing of network traffic. It is responsible for health checking, load balancing, authentication, authorization, inbound (ingress) and outbound (egress) cluster network traffic.</li> </ul> <p>Applications / microservices are unaware of data plane.</p>"},{"location":"concepts/service-mesh/#context","title":"Context","text":"<p>Traditional modern architecture involves having different components exposing reusable APIs, addressing different channels (mobile, single page application, traditional server pages or B2B apps), consuming APIs (mobile APIs, back end for front end, shared common apis like authentication, authorization,...) and backend services addressing reusable business services:</p> <p></p> <p>API management can be added via API gateway. This is a distributed application with cross related communication channels, where any changes to the service interface characteristics impact any of the components.</p> <p>Moving to microservices architecture style adds more communication challenges and DevOps complexity but provides a lot of business values such as:</p> <ul> <li>rapid deployment of new business capabilities, co-evolving in parallel of other services.</li> <li>focusing on business domain with clear ownership of the business function and feature roadmap</li> <li>better operation procedure, automated, and with easy rollout and continuous delivery.</li> <li>A/B testing to assess how new feature deployed improve business operations</li> <li>improve resiliency by deploying on multi language cluster</li> </ul> <p>As an example we can use the a classical web application with the following capabilities:</p> <ul> <li>user authentication</li> <li>user management: add / delete new user</li> <li>user self registration, reset password</li> <li>user permission control</li> <li>user profile</li> <li>asset management</li> <li>risk assessment service</li> </ul> <p>Each capability could be grouped by business domain like the user management, asset management, and application access control. So domain separation can be a good microservice boundary. This is the Domain Driven Design Bounded context construct. But if the number of user reach millions then we may need to optimize the runtime processing of reading user credential, and scale the service differently, leading to a service map like the diagram below, where runtime and management are separated services.</p> <p> </p> <p>All of these still does not address the fact that data are distributed and even more with microservices owning their data persistence. As developers and architects, we still have to address the following data integrity problems:</p> <ul> <li>two phases commit</li> <li>compensating operation</li> <li>eventual data consistency: some microservice updating data may share those updates with other microservices.</li> <li>data aggregation: adding new views on data, owned by a microservice, to support new aggregates. Examples are preparing data view for machine learning modeling, analytics, or business intelligence...</li> </ul> <p>From the previous microservice allocation we can see the needs to propagate data update between services. Adding or unsubscribing a user involves updating the asset the user owns and the authentication runtime service:</p> <p> </p> <p>Adding a new application changes the authorization runtime service.</p> <p>We are now looking at the following questions:</p> <ul> <li>how does webapp access APIs for their main service, of back end for front end service.</li> <li>how does deployed microservice access other service: discover and access?</li> <li>How data consistency can be ensured?</li> <li>is there a simpler way to manage cross microservice dependency?</li> </ul> <p>The answers depend on the existing infrastructure and environment, and deployment needs.</p>"},{"location":"concepts/service-mesh/#service-routing","title":"Service routing","text":"<p>We have to dissociate intra-cluster communication versus inter clusters or cluster to external services. Without getting into too much detail of IP routing within Kubernetes some important elements of the cluster are important to remember:</p> <ul> <li>microservices are packaged as docker image and expose port. When deployed they run in a pod within a node (physical or virtual machine)</li> <li>containers can talk to other containers only if they are on the same machine, or when they have exposed port.</li> <li>Kubernetes is configured with a large flat subnet (e.g. 172.30.0.0/16) which is used for internal application traffic inside of the cluster. Each worker node in the Kubernetes cluster is assigned one or more non-overlapping slices of this network, coordinated by the Kubernetes master node. When a container is created in the cluster, it gets assigned to a worker node and is given an IP address from the slice of the subnet for the worker node.</li> </ul> <p> </p> <ul> <li>Kube-proxy intercepts and controls where to forward the traffic, either to another worker node running your destination pod, or outside of the cluster</li> <li>Kube-proxy watches the API Server on the Master Node for the addition and removal of Services endpoints. It configures the IPtable rules to capture the traffic for its ClusterIP and forwards it to one of the endpoints.</li> <li>Worker nodes have internal DNS service and load balancer</li> </ul> <p>Within Kubernetes, Ingress is a service that balances network traffic workloads in your cluster by forwarding public or private requests to your apps. You use <code>ingress</code> when you need to support HTTP, HTTPS, TLS, load balancing, expose app outside of the cluster, and custom routing rules...</p> <p>One <code>ingress resource</code> is required by namespace. So if microservices are in the same namespace you can define a domain name for those services (e.g. assetmanagement.greencompute.ibmcase.com) and defined path for each service:</p> <p><pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: assetmanagement\nspec:\n  rules:\n    - host: assetmanagement.greencompute.ibmcase.com\n      http:\n        paths:\n          - path: /assetconsumer\n            backend:\n              serviceName: asset-consumer-svc\n              servicePort: 8080\n          - path: /assetdashboard\n            backend:\n              serviceName: asset-dashboard-bff-svc\n              servicePort: 8080\n          - path: /assetmgrms\n            backend:\n              serviceName: asset-mgr-ms-svc\n              servicePort: 8080\n</code></pre> The backend for front end component, the asset manager microservice and the asset consumer components are exposed in the same domain. The <code>serviceName</code> matches the service exposed for each components. The following diagram presents how an external application accesses deployed microservice within Kubernetes pod.</p> <p>The following diagram shows how Ingress directs communication from the internet to a deployed microservice:</p> <p></p> <ol> <li>A user sends a request to your app by accessing your app's URL. DNS name abstracts the application from the underlying infrastructure. Inter clusters microservice to microservice should use the same approach</li> <li>A DNS system service resolves the hostname in the URL to the portable public IP address of the load balancer</li> <li>Based on the resolved IP address, the client sends the request to the load balancer service that exposes the Application Load Balancer (ALB)</li> <li>The ALB checks if a routing rule for the app path in the cluster exists. If a matching rule is found, the request is forwarded according to the rules that you defined in the Ingress resource to the pod where the app is deployed. If multiple app instances are deployed in the cluster, the ALB load balances the requests between the app pods. To also load balance incoming HTTPS connections, you can configure the ALB to you can use your own TLS certificate to decrypt the network traffic.</li> <li>Microservice to microservice can use this DNS name to communicate between them.</li> </ol> <p>Using Ingress, the global load balancer can support parallel, cross region clusters.</p>"},{"location":"concepts/service-mesh/#service-exposition","title":"Service exposition","text":"<p>There is an architecture style focusing on APIs which proposes to have different SLA and semantic for external, internet facing API versus internal back end APIs only exposed within intranet.</p> <p>Backend data services are not exposed directly to internet. API Gateway provides a secure end point for external web app to access those business functions.</p> <p>So the decisions on how to expose service are linked to:</p> <ul> <li>do you need to do API management</li> <li>do you need to secure APIs</li> <li>do you need to expose to internet</li> <li>do you need to support other protocol then HTTP</li> <li>do you need to have multiple instances of the application</li> </ul> <p>When deploying a microservice to Kubernetes it is recommended to use Ingress rule as presented above.. The following yaml file exposes the BFF service using ClusterIP:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: asset-consumer-svc\n  labels:\n    chart: asset-consumer\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8080\n    targetPort: 8080\n    protocol: TCP\n    name: asset-consumer-svc\n  selector:\n    app: asset-consumer\n</code></pre>"},{"location":"concepts/service-mesh/#service-discovery","title":"Service discovery","text":"<p>When deploying on Kubernetes cluster, microservices use the DNS lookup to discover deployed microservice.</p>"},{"location":"concepts/service-mesh/#istio","title":"ISTIO","text":"<p>ISTIO provides an easy way to create a network of deployed services with load balancing, service-to-service authentication, traffic flow management, monitoring, etc... By deploying a special sidecar proxy (called Envoy) throughout your environment, all network communication between microservices is intercepted and controlled by ISTIO control panel.</p> <p>The control plane manages the overall network infrastructure and enforces the policy and traffic rules.</p> More reading <ul> <li>Istio and Kubernetes 101</li> <li>Advanced traffic management with ISTIO</li> <li>Istio workshop for IBM Cloud Container service for inspiration but this repository was not touched since 4 years.</li> </ul>"},{"location":"concepts/service-mesh/#asynchronous-loosely-coupled-solution-using-events","title":"Asynchronous loosely coupled solution using events","text":"<p>If we change of paradigm and use a messaging approach or better an event approach of the data update requirements, we will implement a loosly coupled solution with a pub/sub communication protocol. We need to think about the activities that apply within each service and how they can be of interest to other components. Internal microservice tasks are becoming facts about something happened and those facts may be published as events for others to consume. The first level of refactoring may become:  </p> <p></p> <p>An event is a fact that happens in the past and carry all the data needed, and it becomes a source of record. It becoming consistent if it is played in a messaging backbone via topics.</p> <p> </p> <p>But the persistence of data can be externalized to consumer and then simplify the architecture:</p> <p></p> <p>Then we can use the history of the persisted events to add features not address before, and outside of the direct scope of a microservice. For example to compute the number of users added last month, just a query on the users topic will get the answer: no or very limited coding needed.</p> <p>We recommend to go deeper in event driven architecture with this site</p>"},{"location":"concepts/terms-and-definitions/","title":"Terms & Definitions","text":""},{"location":"concepts/terms-and-definitions/#events","title":"Events","text":"<p>Events are notifications of change of state.  Notifications are issued, or published and interested parties can subscribe and take action on the events. Typically, the issuer of the notification has no knowledge of what action is taken and receives no corresponding feedback that the notification has been processed.</p> <ul> <li>Events are notifications of change of state.</li> <li>Typically, events represent the change of state of something of interest to the business.</li> <li>Events are records of something that has happened.</li> <li>Events can't be changed, that is, they are immutable. (We can't change something that has happened in the past).</li> </ul>"},{"location":"concepts/terms-and-definitions/#event-streams","title":"Event streams","text":"<p>An event stream is a continuous unbounded series of events.</p> <ul> <li>The start of the stream may have occurred before we started to process the stream.</li> <li>The end of the stream is at some unknown point in the future.</li> <li>Events are ordered by the point in time at which each event occurred.</li> </ul> <p>When developing event driven solutions, you will typically see two types of event streams:</p> <ul> <li>Event streams whose events are defined and published into a stream as part of a solution.</li> <li>Event streams that connect to a near real-time event stream, for example from an IOT device, a voice stream from a telephone system, a video stream, or ship or plane locations from global positioning systems.</li> </ul>"},{"location":"concepts/terms-and-definitions/#event-backbone","title":"Event backbone","text":"<p>The event backbone is the communication layer in the event driven architecture.  It provides the connection between event driven capabilities and in the Cloud Native, it becomes the Pub/Sub communication layer for event driven microservices.</p> <p>At this high level we would consider two types of relevant technologies for the event backbone, Message Brokers and Event Logs.  Both technology types could be used to achieve the event communication style, with the \"Publish and subscribe\" model however, it is also important to consider other capabilities which are frequently used within event driven solutions:</p> <ul> <li>Keeping an Event Log as a time sequenced as it happened recording of events (Source of the truth).</li> <li>Enabling direct replay of events.</li> <li>Enabling Event Sourcing as a way of recording state changes in distributed systems.</li> <li>Enabling programmatic access to the continuous event stream.</li> </ul> <p>When viewed across these wider event driven capabilities, an event log style technology can provide a central component which can support all of these capabilities, whereas a message broker would have to be extended with other components.</p>"},{"location":"concepts/terms-and-definitions/#selecting-the-event-backbone-for-the-reference-architecture","title":"Selecting the Event Backbone for the reference architecture","text":"<p>For the event-driven architecture, we defined the following characteristics to be essential for the event backbone:</p> <ul> <li>Publish-subscribe event communication between event producers and consumers</li> <li>Facilitate many consumers with shared central \u201csource of truth\u201d.</li> <li>Capability to store events for a given period of time (event log). This is the shared source of the truth for events.</li> <li>Ability for consumers to subscribe to events.</li> <li>Provide replay of events from history for evolving application instances.</li> <li>Provide programmatic access to continuous stream of events, with minimum time lag.</li> <li>Must be highly scalable and resilient to cloud deployment levels.</li> </ul> Event backbone considerations <p>While choosing an event backbone for your event-driven application development, the following functional and non-functional requirements should be taken into consideration:</p> <p>Looking across these capabilities, the potential technologies, the amount of adoption and community activity around the technologies, and the considerations listed above, we selected Apache Kafka as our event backbone for the event-driven reference architecture.</p>"},{"location":"concepts/terms-and-definitions/#persistence","title":"Persistence","text":"<p>When source systems generate events, the consumers of those are interested in those events may not be online or available at the same time. So you need a way to store these messages for a configurable period of time until they are consumed and acted upon. Event backbone should be able to provide such event persistence.</p>"},{"location":"concepts/terms-and-definitions/#observability","title":"Observability","text":"<p>At times, you need an overall view of how events are ingested by source systems and getting processed by consumers. It could be a management console where events can be observed. Event backbone should provide such observability.</p>"},{"location":"concepts/terms-and-definitions/#fault-tolerance","title":"Fault tolerance","text":"<p>Event backbone could be made of several components. If one of them becomes unavailable, there should not be any impact on the event processors dependent on the backbone. Event backbone needs to provide this resiliency.</p>"},{"location":"concepts/terms-and-definitions/#high-availability","title":"High availability","text":"<p>Event backbone provides persistence of messages/events. If one of the components of the backbone becomes unavailable, there should not be any impact on the availability of these messages/events. Event backbone should be highly available.</p>"},{"location":"concepts/terms-and-definitions/#performance","title":"Performance","text":"<p>Event backbone should provide means of accelerating the event processing operations (e.g. parallelising event processing) thereby providing enhanced performance.</p>"},{"location":"concepts/terms-and-definitions/#delivery-guarantees","title":"Delivery guarantees","text":"<p>Event backbone should support guaranteed delivery both for producer and consumer. It should support the delivery guarantee options of <code>at least once</code>, <code>at most once</code>, and <code>exactly once</code>.</p>"},{"location":"concepts/terms-and-definitions/#security","title":"Security","text":"<p>The data residing in the event backbone should be secured, at rest as well as in transit. Only authenticated and authorized users should be able to publish and consume messages from the backbone. Topic specific authorizations will also help blocking access by unauthorized consumers. Event backbone should provide these security measures.</p>"},{"location":"concepts/terms-and-definitions/#stateful-operations-for-events-streams","title":"Stateful operations for events streams","text":"<p>Sometimes, source systems generate a continuous flow of 'inter-related' events (e.g. IoT sensors sending data every second). In order to process such messages correctly, the event backbone needs to support for stateful operations like windowing, joins, aggregations. and any type of real time analytics.</p>"},{"location":"concepts/terms-and-definitions/#event-routing-options","title":"Event routing options","text":"<p>In EDA, event consumers may not be online at all times. So, it should be easier for consumers to subscribe to a topic when it comes online.</p>"},{"location":"concepts/terms-and-definitions/#on-failure-hooks","title":"On-failure hooks","text":"<p>Event backbone can support pre-configured actions/behaviors for certain messages. E.g. if a consumer fails to process a message more than a certain number of times, that message can be sent to another topic for re-trying the processing action.</p>"},{"location":"concepts/terms-and-definitions/#event-sources","title":"Event sources","text":"<p>When you consider an event-driven architecture, think about event producers and event consumers as the interaction points with events. As you develop event-driven applications following a microservices architecture, the microservices you develop play the role of both event producers and event consumers, with the events being passed as the communication payload between them. However, as you look at the wider opportunities that being event driven offers, you need to widen your view and consider event sources that come from beyond the application code you are writing.  These are events that may be produced from outside our immediate system but have business relevance or enable us to gain valuable insights into things that are affecting your business.</p>"},{"location":"concepts/terms-and-definitions/#here-is-a-list-of-common-event-sources","title":"Here is a list of common event sources:","text":"<ul> <li>IoT devices or sensors showing device status changes</li> <li>Click Stream data from web or mobile applications</li> <li>Mobile applications (HTTP to Back-end for Front-end service and then to topic)</li> <li>Geospatial data</li> <li>Weather alerts</li> <li>Social media feeds</li> <li>Real-time voice feeds</li> <li>Other messaging backbone</li> <li>Data change event streams from databases (change data capture)</li> </ul>"},{"location":"concepts/terms-and-definitions/#iot-devices-and-sensors","title":"IoT devices and sensors","text":"<p>With IoT devices and sensors you typically have a gateway providing the connectivity for the device, and a level of event enrichment and filtering. In terms of domain driven design you would see the device and gateway as being the technical domain and the event-driven reference architecture as providing the infrastructure for the applications in a business domain. In practice, the IoT gateway or platform provides the connectivity and is the point of filtering and consolidation of events so that only business-relevant events are passed up to the business domain. The gateway can also be the point where the technical event is enhanced to relate to something recognizable at the business level.  One example of this is to relate a device number or identifier in the event to something that the business recognizes.</p> <p></p>"},{"location":"concepts/terms-and-definitions/#clickstream-data","title":"Clickstream data","text":"<p>Clickstream data is often used to understand the behavior of users as they navigate their way through web or mobile apps. It provides a recording of the actions they take, such as the clicks, the mouse-movements, and the gestures.  Analysis of the clickstream data can lead to a deep understanding of how users actually interact with the application. It enables you to detect where users struggle and to look for ways to improve the experience.</p> <p>Processing the clickstream in real time in an event-driven architecture can also give rise to the opportunity to take direct action in response to what a user is currently doing, or more accurately has just done.</p> <p>There are various \"collectors\" that enable collection of standard clickstream events and allow custom actions to be collected as events typically through tags in Javascript.</p> <p>Within the Apache Open Source communities the Divolte collector is an example of one of these collectors that directly publishes events to Kafka topics.</p> <p></p>"},{"location":"concepts/terms-and-definitions/#event-standards-and-schemas","title":"Event standards and schemas","text":"<p>Where you have control as the producer of an event we should consider having an event schema and following a standard to provide the best opportunity for portability of the solutions across cloud environments. With a lack of formal standards, a working group under the Cloud Native Computing Foundation (CNCF) has recently been formed to define and propose Cloud Events as the standard.</p>"},{"location":"concepts/terms-and-definitions/#microservices","title":"Microservices","text":"<p>The event-driven reference architecture provides support for event-driven microservices - microservices which are connected through and communicate via the pub/sub communication protocol within an event backbone.</p> <p>With Kafka as the event backbone and pub/sub messaging provider, microservices can use the Kafka API's to publish and listen for events.</p> <p>On cloud-native platforms, microservices are the application architecture of choice. As businesses become event-driven, event driven pattern needs to extend into our microservices application space. This means that your microservices are still doing REST calls to well-known microservice but they must respond to and send out events, or in event-driven terms they need to be both event producers and consumers to enforce strong decoupling.</p> <p>With the adoption of microservices, the focus on synchronous communication between services has increased. Service mesh packages such as Istio help with the management of communication, service discovery, load balancing, and visibility in this synchronous communication environment.</p> <p>With event-driven microservices, the communication point becomes the Pub/Sub layer of the event backbone. By adopting an event-based approach for intercommunication between microservices, the microservices applications are naturally responsive (event-driven). This approach enhances the loose coupling nature of microservices because it decouples producers and consumers.  Further, it enables the sharing of data across microservices through the event log. These event style characteristics are increasingly important considerations when you develop microservices style applications. In practical terms microservices applications are a combination of synchronous API-driven, and asynchronous event-driven communication styles. For the implementation point of view a set of established patterns are used, such as Database per Service, Event Sourcing, Command Query Responsibility Segregation, Saga, ...</p>"},{"location":"concepts/terms-and-definitions/#event-driven-apps-with-containers","title":"Event-driven apps with containers","text":"<p>While the serverless approach with Cloud Functions provides a simplified event-based programming model, the majority of microservices applications today are developed for and deployed to a container-based cloud-native stack.  Within the cloud-native landscape, Kubernetes is the standard platform for container orchestration, and therefore becomes the base for the container platform in the event-driven architecture.</p> <p>As before, the event backbone is the Pub/Sub communication provider and event log for shared data for the microservices. In this context microservices are developed as direct consumers and producers of events on the backbone via topics.  The extra work in this environment is in managing consumer instances to respond to the demand of the event stream. You must determine how many consumer instances need to be running to keep pace with, or always be immediately available to execute, the microservice in response to an arriving event.</p>"},{"location":"concepts/terms-and-definitions/#commands","title":"Commands","text":"<p>A command, is an instruction to do something. Typically, commands are directed to a particular consumer. The consumer runs the required command or process, and passes back a confirmation to the issuer stating that the command has been processed.</p> <p>The Command concept plays a key role in the Command-Query Responsibility Segregation pattern that is more commonly known as CQRS.</p>"},{"location":"concepts/terms-and-definitions/#loose-coupling","title":"Loose coupling","text":"<p>Coupling measures the independance between connected applications and systems. Dependances is not one dimension but includes protocol, technology, communication interaction model (synchronous or asynchronous), data model, location, ...</p> <p>Decoupling is complex, has a cost at design level but also at technology choices so at runtime level. </p> <p>Synchronous request-response model is used a lot in RESTful microservice, and it brings advantages for the developers, at it is simple to implement, it has low latency and helps to address quickly receiver failure. The disadvantages come when the receiver failure blocks the requester to serve its own business logic, and may fails too or when the receiver starts to be overloaded by multiple requests.</p> <p>Adding queues as a way to decouple, with asynchronous point to point communication, is a natural solution for previous disadvantages. It decouples at the time level. With high availability queueing middleware the end to end processing is more resilient to receiver failure, and to traffic burst as the receiver is the one controlling the consumption of messages. Still if the queue starts to fill up, producers need to address back preassure. Also lost receiver can  be handled by dead letter queues. With point-to-point, only one consumer can get a message.</p> <p>Asynchronous implementations are more complex to do as developers need to address response correlation with some sort of transaction ID, timestamp, consistency, horizontal scalling, and even data model contract evolution.</p> <p>Recalls the CAP theorem which stipulates that distributed applications could not ensure consistency, availability and partition tolerance all at the same time. They can, at most, support only two out of the three. In the internet of scale and for a lot of business application, it is better tohave availability along with eventual consistency, rather than compromising on availability on the whole. </p> <p>Loose coupling is one of the main benefits of event-driven processing. It allows event producers to emit events without any knowledge  about who is going to consume those events (temporal decoupling). Likewise, event consumers don't need to be aware of the event emitters. Because of this,  event consuming modules and event producer modules can be implemented in different languages or use technologies that are differents and  appropriate for specific jobs. Loosely coupled modules are better suited to evolve independently and, when implemented correctly,  result in a significant decrease in system complexity.</p> <p>Loose coupling, however, does not mean \u201cno coupling\u201d. An event consumer consumes events that are useful in achieving its goals and in  doing so establishes what data it needs and the type and format of that data. The event producer emits events that it hopes are understood and  useful to consumers thus establishing an implicit contract with potential consumers. For example, an event notification in XML format must  conform to a certain schema that must be known by both the consumer and the producer.  One of the most important things that you can do to reduce  coupling in an event-driven system is to reduce the number of distinct event types that flow between modules. To do this you must pay attention  to the cohesiveness of those modules.</p>"},{"location":"concepts/terms-and-definitions/#cohesion","title":"Cohesion","text":"<p>Cohesion is the degree to which related things are encapsulated together in the same software module. For the purposes of this EDA discussion,  a module is defined as an independently deployable software unit that has high cohesion. Cohesion is strongly related to coupling in the sense   that a highly cohesive module communicates less with other modules, thus reducing the number of events most importantly, the number of event   types in the system. The less frequently modules interact with each other, the less coupled they are. Achieving cohesion in software while   optimizing module size for flexibility and adaptability is difficult, but something to strive for. Designing for cohesion starts with a   holistic understanding of the problem domain and good analysis work. Sometimes it must also take into account the constraints of the supporting   software environment. Monolithic implementations and implementations that are excessively fine-grained must be avoided.</p>"},{"location":"contribute/","title":"Contributing to this Site","text":"<p> Anyone can contribute to IBM Cloud Architecture reference applications and their associated projects, whether you are an IBMer or not. We welcome your collaboration &amp; contributions happily, as our reference applications are meant to reflect your real world scenarios. There are multiple ways to contribute: report bugs and improvement suggestions, improve documentation, and contribute code. </p>"},{"location":"contribute/#bug-reports-documentation-changes-and-feature-requests","title":"Bug reports, documentation changes, and feature requests","text":"<p>If you would like to contribute your experience with an IBM Cloud Architecture project back to the project in the form of encountered bug reports, necessary documentation changes, or new feature requests, this can be done through the use of the repository's Issues list.</p> <p>Before opening a new issue, please reference the existing list to make sure a similar or duplicate item does not already exist.  Otherwise, please be as explicit as possible when creating the new item and be sure to include the following:</p> <ul> <li>Bug reports</li> <li>Specific Project Version</li> <li>Deployment environment</li> <li>A minimal, but complete, setup of steps to recreate the problem</li> <li>Documentation changes</li> <li>URL to existing incorrect or incomplete documentation (either in the project's GitHub repo or external product documentation)</li> <li>Updates required to correct current inconsistency</li> <li>If possible, a link to a project fork, sample, or workflow to expose the gap in documentation.</li> <li>Feature requests</li> <li>Complete description of project feature request, including but not limited to, components of the existing project that are impacted, as well as additional components that may need to be created.</li> <li>A minimal, but complete, setup of steps to recreate environment necessary to identify the new feature's current gap.</li> </ul> <p>The more explicit and thorough you are in opening GitHub Issues, the more efficient your interaction with the maintainers will be.  When creating the GitHub Issue for your bug report, documentation change, or feature request, be sure to add as many relevant labels as necessary (that are defined for that specific project).  These will vary by project, but will be helpful to the maintainers in quickly triaging your new GitHub issues.</p>"},{"location":"contribute/#code-contributions","title":"Code contributions","text":"<p>We really value contributions, and to maximize the impact of code contributions, we request that any contributions follow the guidelines below.  If you are new to open source contribution and would like some more pointers or guidance, you may want to check out Your First PR and First Timers Only.  These are a few projects that help on-board new contributors to the overall process.</p>"},{"location":"contribute/#coding-and-pull-requests-best-practices","title":"Coding and Pull Requests best practices","text":"<ul> <li>Please ensure you follow the coding standard and code formatting used throughout the existing code base.</li> <li>This may vary project by project, but any specific diversion from normal language standards will be explicitly noted.</li> <li>One feature / bug fix / documentation update per pull request</li> <li>Always pull the latest changes from upstream and rebase before creating any pull request.</li> <li>New pull requests should be created against the <code>integration</code> branch of the repository, if available.</li> <li>This ensures new code is included in full-stack integration tests before being merged into the <code>master</code> branch</li> <li>All new features must be accompanied by associated tests.</li> <li>Make sure all tests pass locally before submitting a pull request.</li> <li>Include tests with every feature enhancement, improve tests with every bug fix</li> </ul>"},{"location":"contribute/#github-and-git-flow","title":"Github and git flow","text":"<p>The internet is littered with guides and information on how to use and understand git. However, here's a compact guide that follows the suggested workflow</p> <p></p> <ol> <li> <p>Fork the desired repo in github.</p> </li> <li> <p>Clone your repo to your local computer.</p> </li> <li> <p>Add the upstream repository</p> <p>Note: Guide for step 1-3 here: forking a repo</p> </li> <li> <p>Create new development branch off the targeted upstream branch.  This will often be <code>master</code>.</p> <pre><code>git checkout -b &lt;my-feature-branch&gt; master\n</code></pre> </li> <li> <p>Do your work:</p> </li> <li>Write your code</li> <li>Write your tests</li> <li>Pass your tests locally</li> <li>Commit your intermediate changes as you go and as appropriate</li> <li> <p>Repeat until satisfied</p> </li> <li> <p>Fetch latest upstream changes (in case other changes had been delivered upstream while you were developing your new feature).</p> <p><pre><code>git fetch upstream\n</code></pre> 7. Rebase to the latest upstream changes, resolving any conflicts. This will 'replay' your local commits, one by one, after the changes delivered upstream while you were locally developing, letting you manually resolve any conflict.</p> <p><pre><code>git branch --set-upstream-to=upstream/master\ngit rebase\n</code></pre> Instructions on how to manually resolve a conflict and commit the new change or skip your local replayed commit will be presented on screen by the git CLI.</p> </li> <li> <p>Push the changes to your repository</p> <pre><code>git push origin &lt;my-feature-branch&gt;\n</code></pre> </li> <li> <p>Create a pull request against the same targeted upstream branch.</p> <p>Creating a pull request</p> </li> </ol> <p>Once the pull request has been reviewed, accepted and merged into the main github repository, you should synchronise your remote and local forked github repository <code>master</code> branch with the upstream master branch. To do so:</p> <ol> <li> <p>Pull to your local forked repository the latest changes upstream (that is, the pull request).</p> <pre><code>git pull upstream master\n</code></pre> </li> <li> <p>Push those latest upstream changes pulled locally to your remote forked repository.</p> <pre><code>git push origin master\n</code></pre> </li> </ol>"},{"location":"contribute/#what-happens-next","title":"What happens next?","text":"<ul> <li> <p>All pull requests will be automatically built and unit tested by travis-ci, when implemented by that specific project.</p> </li> <li> <p>You can determine if a given project is enabled for travis-ci unit tests by the existence of a <code>.travis.yml</code> file in the root of the repository or branch.</p> </li> <li> <p>When in use, all travis-ci unit tests must pass completely before any further review or discussion takes place.</p> </li> <li> <p>The repository maintainer will then inspect the commit and, if accepted, will pull the code into the upstream branch.</p> </li> <li>Should a maintainer or reviewer ask for changes to be made to the pull request, these can be made locally and pushed to your forked repository and branch.</li> <li>Commits passing this stage will make it into the next release cycle for the given project.</li> </ul>"},{"location":"introduction/overview/","title":"Introduction","text":""},{"location":"introduction/overview/#introduction-to-the-ibm-automation-event-driven-reference-architecture","title":"Introduction to the IBM Automation Event-Driven Reference Architecture","text":"<p>The modern digital business works in near real-time; it informs interested parties of things of interest when they happen, makes sense of, and derives insight from an ever-growing number of sources. It learns, predicts, and is intelligent -- it is by nature Event-Driven.</p> <p>Event-driven architecture (EDA) is an architecture pattern that promotes the production, detection, consumption of, and reaction to events. This architectural pattern can be applied to the systems that transmit events among loosely coupled software components and services.</p> <p>Events are a way of capturing a statement of fact.  Events occur in a continuous stream as things happen in the real and digital worlds.  By taking advantage of this continuous stream, applications can not only react in near real-time, but also reason about the future based upon what has happened in the past.</p> <p>The business value of adopting this architecture is that you can easily extend EDA with new components that are ready to produce or consume events that are already present in the overall system. While events are more visible, new business capabilities are addressed, like near real-time insights. EDA helps also to improve the continuous availability of a microservice architecture.</p> <p>For enterprise IT teams, embracing event-driven development is foundational to the next generation of digital business applications. IT teams will need to be able to design, develop, deploy and operate event-driven solutions, in cloud-native styles.</p> <p></p> <p>While event-driven architectures and reactive programming models are not new concepts, the move to cloud-native architectures with microservices, container based workloads and \"server-less\" computing allow us to revisit event-driven approaches in this cloud-native context.  Indeed, we could think of event-driven as extending the resilience, agility and scale characteristics of \"cloud-native\" to also be reactive and responsive. Two aspects of a cloud-native architecture are essential to developing an event-driven architecture:</p> <ul> <li>Microservices - These provide the loosely coupled application architecture which enables deployment in highly distributed patterns for eesilience, agility and scale.</li> <li>Cloud-native platforms with Containers and \"Serverless deployments\" - These provide the application platform and tools which realize the resilience, agility and scale promise of the microservices architectures.</li> </ul> <p>An event-driven architecture should provide the following essential event capabilities to the cloud-native platform.</p> <ul> <li>Being able to communicate and persist events.</li> <li>Being able to take direct action on events.</li> <li>Processing event streams to derive near real-time insight/intelligence.</li> <li>Providing communication for event driven microservices.</li> </ul>"},{"location":"introduction/reference-architecture/","title":"Reference Architectures","text":"<p>Updated 05/27/2022</p> <p>We defined the starting point for a cloud-native event-driven architecture to be that it supports at least the following important capabilities:</p> <ul> <li>Being able to communicate asynchronously between components to improve elasticity, resilience and responsiveness. </li> <li>Support exactly one delivery of messages in a asynchronous request/response interactions</li> <li>Publish messages as facts or events as immutable records to an append log so subscribers can consume them at any point of time.</li> <li>Processing streams of events to derive real time insight/intelligence.</li> <li>Providing communication mechanism between event-driven microservices and functions.</li> </ul>"},{"location":"introduction/reference-architecture/#from-soa-to-eda-and-meet-in-the-middle","title":"From SOA to EDA and meet in the middle","text":"<p>We do not need to present Service Oriented Architecture as a way to organize modern application... Well modern was started in 2003.  In 2006, Enterprise Service Bus was at the center of thee SOA adoption with API exposure, mediation flows, and service gateway. ESB is a pattern but also a product.</p> <p></p> <p>Business services exposes APIs, defined with WSDL and the protocol is XML heavy with SOAP. The rich set of specification in the ws-*  helps to address complex business applications, where application can dynamically searching for API producer, bind to the end point, and then call the service.</p> <p>SOA is still a core structure of IT architecture today.</p> <p>Two major characteristics of IT architecture are the scalability and the coupling with others. With the way most of SOA services are done is by pulling data from other services or call operation on API to make something done (delegation, command patterns). The dependencies between service via the service contract, and the heavy use of XML have put some limit to the scalability and coupling dimensions.</p> <p>EDA was already proposed, in 2004, as a way to address scalability, as illustrated by this figure below:</p> <p></p> <p>EDA helps to decouple communication and contract between services, move to push data as immutable facts amd async communication.</p> <p>New applications started to be implemented, early 2010s, to address scalability need. RESTful and JSON are becoming ubiquituous technology, protocols to use. This is a neat evolution from SOAP and XML, but the applications exposing API with OpenAPI specification are still point to point, and the data pull is still the  pattern to get access to data. The reactive manifesto is also  promoting sound principles to make modern applications more responsive, resilient and elastics therefore adopting messages. </p> <p>EDA and SOA do not have to compete, but are complementary addressing different kind of problem.  Combined together they provide innovative business solutions to problems we never thought could be solved:</p> <ul> <li>Event driven business process</li> <li>near real-time analytics</li> <li>real-time flow analysis</li> <li>complex event processing with time window based business rules</li> <li>services act as event source or sink</li> <li>event trigger services</li> <li>service to process events</li> </ul>"},{"location":"introduction/reference-architecture/#event-driven-architecture","title":"Event Driven Architecture","text":"<p>Event-driven architecture is not something new, and it is not just about Kafka. As listed in previous section EDA at the core has to support  asynchronous communication between application in two major semantics:</p> <ul> <li>request/response with exactly one delivery with no data lost, when a component is asking another component to do something for it. This approach is to address long running transaction or business process and enforce using queuing technology.</li> <li>deliver facts about its own data to an immutable log so other components can derive something about it. This is a pub/sub model with strong time decoupling.</li> </ul> <p>To support these EDA has a message as a service capability as cluster of message brokers. The brokers provides the connectivity between the other components: </p> <p></p> <p>Where:</p> <ul> <li>Event sources: generates events from sources such as IoT devices, web apps, mobile apps, mainframe applications, change data capture agents...</li> <li>Mainframe queuing apps are source for messages using the IBM MQ replication feature to expose mainframe messages to the cloud native MQ broker and so to the cloud native microservices. The strong consistency is kept but facts about the business transactions are propagated to the eventual consistency world.</li> <li> <p>Messaging as a service is the core backbone of the architecture to support  any type of asynchronous communication:</p> <ul> <li>IBM MQ: delivers the exatly one delivery, strong consistency and supports queuing and pub/sub protocol. MQ brokers in cluster support high availability cross data centers to build event-mesh. IBM MQ will be the unique technology to guarantee the end to end message availability.</li> <li>IBM Event Streams: provides an event backbone supporting Pub/Sub protocol only, with immutable append log. Event Streams is based on Apache Kafka and can run on-premise or as managed services</li> </ul> </li> <li> <p>Reactive cloud native applications: The top row represents modern applications which adopt the reactive manifesto as a way to be resilient, elastic and message driven. To reduce microservice to microservice dependency and reduce coupling, modern microservices are  event-driven and  implemented with reactive framework (e.g. vert.x )and reactive messaging (e.g. microprofile 3.0 reactive messaging). Some of those applications can be function and serverless apps, meaning scaling to zero and being able to wake-up on event arrival. Finally it is important to note that business process applications running in BPM can be triggered by event arrival, can generate intermediate events, and can generate events at the process completion. Those business events are important to shared with other applications to being expose to event backbone.</p> </li> <li>The adoption of Kafka as a way to support event backbone capability, also means that records can be saved for a long period of time, but it is relevant to be able to persist those records or an aggregate view of those records to a data lake or a set of s3 buckets. So most of EDA has sink connectors to data lake. </li> <li>The governance of these asynchronous applications is becoming a major requirement when the adoption of such architecture grows. AsyncAPI, combined with schema registry helps defining the intercommunication contracts. While most of the event-driven microservices are exposing Open APIs, enforced by API gateway, it is now  possible to do the same enforcement and monitoring with the event endpoint gateway.</li> <li>The bottom row supports new types of application for data streaming: the first set of applications  are for getting business insight of the event sequencing by looking at event patterns as supported by the complex event processing engine (Apache Flink), and the second type are to process near-real time analytics to  compute analytical processing across multiple event streams. The technologies of choice are Apache Flink and IBM SQL query. Those applications are also cloud native, and run in container deployable inside Kubernetes clusters.</li> <li>Apache Pinot bring Realtime distributed OLAP datastore to support fast indexing, scale horizontally, OLAP queries for user-facing analytics, and application queries. Support low latency &lt;  1s with millions events per s. Pinot can also be used for anomaly detection.</li> </ul> <p>In term of physical deployment on OpenShift the following figures illustrates a multi zone deployment,  with Event Streams and MQ Operators deployed and managing five Event Streams brokers Cluster, and three MQ brokers</p> <p></p> <p>This reference architecture is illustrated in the implementation of different solutions: </p> <ul> <li>The shipping goods oversea solution </li> <li>The real-time inventory</li> <li>The vaccine at scale solution.</li> </ul>"},{"location":"introduction/reference-architecture/#kappa-architecture","title":"Kappa architecture","text":"<p>The\u00a0Kappa Architecture\u00a0is a software architecture used for processing streaming data.  The main premise behind the Kappa Architecture is that we can perform both real-time and batch  processing, especially for analytics, with a single technology stack.</p> <p></p> <ul> <li>Data in motion includes append-log based topic, and Apache Kafka acts as the store for the streaming data.</li> <li>Streaming processing is the practice of taking action on a series of data at the time the data is created. It can be done with different technologies like Kafka Streams, Apache Sparks streaming, Apache Flink, Redis streaming, or Hazelcast.</li> <li>The serving layer is where OLAP queries and searches are done, most of the time with indexing and other advanced capabilities are needed to offer excellent response time, high throughput and low latency. </li> </ul> <p>It is a simpler alternative to the\u00a0Lambda Architecture \u2013 as all data is treated as if it were a  stream. Both architectures entail the storage of historical data to enable large-scale analytics.</p>"},{"location":"introduction/reference-architecture/#agile-integration-and-eda","title":"Agile integration and EDA","text":"<p>Event driven architecture is a complement of the overall integration reference architecture as presented in IBM Cloud architecture center.</p> <p>In this section, we want to summarize some of the important aspects of agile integration and  how some of the technologies delivered as part of IBM Cloud Pak for Integration are used  when doing application modernization with event-driven microservice. </p> <p>The main agile integration concepts as presented in detail in the IBM cloud on agile integration article can be summarized as:</p> <ul> <li>Empower extended teams to create integrations, leveraging a complete set of integration styles and capabilities to increase overall team productivity.</li> <li>Agile integration includes container-based, decentralized, microservices-aligned approach for integrating solutions</li> <li>Existing centralized integration architectures, based on ESB pattern, cannot support the demand, in term of team reactivity and scalability at the internet level.</li> <li>ESB pattern provides standardized synchronous connectivity to back-end systems typically over web services (SOAP based). ESB formed a single infrastructure for the whole enterprise, with tens or hundreds of integrations installed on a production server cluster.</li> <li>A single, centralized ESB certainly simplifies consistency and governance of implementation.</li> <li>Interface maintenance is expensive.</li> <li>Any deployment to the shared servers runs the risk of destabilizing existing critical interfaces.</li> <li>SOA encounters the issue of getting the funding at the enterprise wide program to maintain reusable interface.</li> <li>Integration teams are becoming the bottleneck instead of being enabler.</li> <li>SOA is about real-time integration between applications, whereas a microservices architecture is about how the applications are built internally.</li> <li> <p>Microservice enables greater agility by being:</p> </li> <li> <p>small enough to be understood completely by their owning team and changed independently</p> </li> <li>elastic to scale horizontally</li> <li>resilient with changes to one microservice will not affect others at runtime</li> </ul> <p>The following diagram illustrates the agile integration modernization transition from a centralized ESB type of architecture, and breaking integration into smaller  pieces to make them more agile and more scalable.</p> <p></p> <p>In this modernization process, development team can introduce API Management to improve decoupling between consumers and providers ultimately moving to a decentralized approach where each team can manage their own integration.</p> <ul> <li> <p>Three aspects to agile integration:</p> <ol> <li>Decentralized integration ownership: give application teams more control over the creation and exposure of their own integration exposed as APIs, or messages</li> <li>Fine-grained integration deployment to separate integration, scalable independently. Changes to individual integration flows can be automatically rebuilt and deployed independently of other flows to enable safer application of changes and maximize speed to production.</li> <li>Cloud-native integration infrastructure to improve productivity, operational consistency and portability for both applications and integration</li> </ol> </li> </ul> <p>Read more on those concepts in this note</p>"},{"location":"introduction/reference-architecture/#integrating-with-ibm-automation-products","title":"Integrating with IBM automation products","text":"<p>EDA is becoming the new foundation to expose business services, and business applications. Combined with  SOA and microservices architecture, it exposes messaging as a services to be used by automation products, like Robot Process Automation bots, Process Mining, Chat Bots, and the more traditional workflow engines and decision engines.</p> <p>Those products are leveraging data in the event backbone or in the queues to do their own processing. But they are also event producer. We can build a new digital automation reference architecture,  based on event-driven and messaging communication like in the figure below:</p> <p></p> <p>On the top row, the capabilities include:</p> <ul> <li>Existing business applications, and SOA services: most of them are J2EE applications running on application servers, such  as WebSphere Application Server, some are mainframe applications using transactions and strong consistency. Those applications will integrate new microservices or webapps using JSON / RESTful resources exposed via an API Gateway, or will use MQ queue when asynchronous protocol is used.</li> <li>New single page applications are also accessing business services via the API gateway, and use HTTP (RESTful) protocol. Those single page applications  can be developed using IBM Business Automation Application.</li> <li>Existing business process applications, running within BPM servers, may have been designed to be triggered by message arrival, and may be able to emit intermediate events within a task execution. IBM BPM, for example, has JMS queue integration to consume and publish messages to JMS queues, which should be defined in IBM MQ Brokers.</li> </ul> <p>A business process is the set of combined activities defined as an orchestrated workflow  within the business to achieve a business goal which uses system services or refers work to process workers</p> <ul> <li>New workflow can be entirely event-driven, and designed to act on event arrival to trigger process instances or task instances.  They are also supported by Case management products which are well suited to address unstructured execution path, where next steps are not pre-determined.</li> <li>As stated above, new microservices are becoming message-driven and are using queues for async request / response communication,  they ask someone to participate into their business process. And they publish state changes via events to pub/sub topics.</li> <li>Consumers can use Event End point management to access to topics and process the data, do data mapping and data transformation for long persistence into data lake.</li> <li>The key integration between EDA and automation is by adding stateful agents that are aggregating, correlating,  and computing the next best actions. Those actions can be to trigger a human workflow, initiate a RPA bot, call one or more decision services. Those stateful agents can use complex event processing engines, rule engines, and  integrate with predictive scoring. A lot of new use-cases are covered by these capabilities like fraud detection, know your customer, product recommendations...</li> <li>The predictive scoring models are defined in AI workbench tool, like Watson Studio or Cloud Pak for Data.  The data used to develop the model come from data lake, existing datawarehouse but also the messaging service, like Kafka topics.</li> <li>The AI workbench includes tools to do data analysis and visualization, to build training and test sets from any datasources and in particular topics, and tp develop models. Models are integrated to streaming analytics component.</li> <li>With this extended architecture, most of business applications are generating events,  directly by code, or by adopting less intruisive techniques like change data capture agents. Those events are representing facts about the busienss entities and the processes implemented cross microservices and applications.  This is were Process Mining product are very important to plug into this data stream.   </li> </ul>"},{"location":"introduction/reference-architecture/#process-mining-integration","title":"Process Mining integration","text":"<p>Process mining is a tool to analyze process execution logs or data. The consumption vehicle are cvs files. When adopting EDA, data in the business process may be initiated by a single page application, sent to Kafka as events, cosumed by a human centric, long running, process application, processed by a data streaming application in Flink... Each element of this 'Saga' transaction will event time based fact about the data and so about the process. Then it will make sense to analyze those data.  The integration will look like in the figure below, where data processing prepares the expected CVS format, and persists file in scalable, secured long storage like S3 buckets (IBM Cloud Object Storage), and then business analysts loads those files into Process Mining to conduct their analysis: </p> <p></p>"},{"location":"introduction/reference-architecture/#integration-with-decision-service","title":"Integration with decision service","text":"<p>The figure below illustrates an integration between data produces on Event Streams, consumed by Apache Flink data streaming jobs which detect fraud or business situation which needs to be processed by business rule logic in IBM Operational Decision Management to compute the next best action:</p> <p></p>"},{"location":"introduction/reference-architecture/#business-process-integration","title":"Business process integration","text":"<p>BPMN has multiple event constructs to support receiving events, generating  intermediate events,  and sending closing events.</p>"},{"location":"introduction/reference-architecture/#integration-with-analytics-and-machine-learning","title":"Integration with analytics and machine learning","text":"<p>The extended architecture extends the basic EDA reference architecture with concepts showing how data science, artificial intelligence and machine learning can be incorporated into an event-driven solution. The following diagram illustrats the event sources on the left injecting events to topics where green components are consuming from. Those components apply filtering, compute aggregates and stateful operation with time window based rules. Some of those components can include training scoring model, to do for example anomaly detection. The model is built with data scientist workbench tool, like Watson Studio.</p> <p></p> <p>The starting point for data scientists to be able to derive machine learning models or analyze data for trends and behaviors is the existence of the data in a form that they can be consumed. For real-time intelligent solutions, data scientists typically inspect event histories and decision or action records from a system. Then, they reduce this data to some simplified model that scores new event data as it arrives.</p>"},{"location":"introduction/reference-architecture/#getting-the-data-for-the-data-scientist","title":"Getting the data for the data scientist:","text":"<p>With near real-time event streams, the challenge is in handling unbounded data or a continuous flow of events. To make this consumable for the data scientist you need to capture the relevant data and store it so that it can be pulled into the analysis and model-building process as required.</p> <p>Following our event-driven reference architecture the event stream would be a Kafka topic on the event backbone.  From here there are two possibilities for making that event data available and consumable to the data scientist:</p> <ul> <li>The event stream or event log can be accessed directly through Kafka and pulled into the analysis process</li> <li>The event stream can be pre-processed by the streaming analytics system and stored for future use in the analysis process. You have a choice of store type to use. Within public IBM cloud object storage Cloud Object Store can be used as a cost-effective historical store.</li> </ul> <p>Both approaches are valid, pre-processing through streaming analytics provides opportunity for greater manipulation of the data, or storing data over time windows for complex event processing. However, the more interesting distinction is where you use a predictive (ML model) to score arriving events or stream data in real time. In this case you may use streaming analytics to extract and save the event data for analysis, model building, and model training and also for scoring (executing) a derived model in line in the real time against arriving event data.</p> <ul> <li>The event and decision or action data is made available in cloud object storage for model building through streaming analytics.</li> <li>Models may be developed by tuning and parameter fitting, standard form fitting, classification techniques, and text analytics methods.</li> <li>Increasingly artificial intelligence (AI) and machine learning (ML) frameworks are used to discover and train useful predictive models as an alternative to parameterizing existing model types manually.</li> <li>These techniques lead to process and data flows where the predictive model is trained offline using event histories from the event and the decision or action store possibly augmented with some supervisory outcome labelling, as illustrated by the paths from the <code>Event Backbone</code> and <code>Stream Processing</code> store into <code>Learn/Analyze</code>.</li> <li>A model trained in this way includes some \u201cscoring\u201d API that can be invoked with fresh event data to generate a model-based prediction for future behavior and event properties of that specific context.</li> <li>The scoring function is then easily reincorporated into the streaming analytics processing to generate predictions and insights.</li> </ul> <p>These combined techniques can lead to the creation of near real-time intelligent applications:</p> <ol> <li>Event-driven architecture</li> <li>Identification of predictive insights using event storming methodology</li> <li>Developing models for these insights using machine learning</li> <li>Near real-time scoring of the insight models using a streaming analytics processing framework</li> </ol> <p>These are scalable easily extensible, and adaptable applications responding in near real time to new situations. There are easily extended to build out and evolve from an initial minimal viable product (MVP) because of the loose coupling in the event-driven architecture, , and streams process domains.</p>"},{"location":"introduction/reference-architecture/#data-scientist-workbench","title":"Data scientist workbench","text":"<p>To complete the extended architecture for integration with analytics and machine learning, consider the toolset and frameworks that the data scientist can use to derive the models.  Watson Studio  provides tools for data scientists, application developers, and subject matter experts to collaboratively and easily work with data to build and train models at scale.</p> <p>For more information see Getting started with Watson Studio.</p>"},{"location":"introduction/reference-architecture/#modern-data-lake","title":"Modern Data Lake","text":"<p>One of the standard architecture to build data lake is the lambda architecture with data injection, stream processing, batch processing to data store and then queries as part of the service layer. It is designed to handle massive quantities of data by taking advantage of both batch and stream processing methods. Lambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record. The batch layer precomputes results using a distributed processing system that can handle very large quantities of data. Output from the batch and speed layers are stored in the serving layer, which responds to ad-hoc queries by returning precomputed views or building views from the processed data.</p> <p>The following figure is an enhancement of the lambda architecture with the adoption of Kafka as event backbone for data pipeline and source of truth and streaming processing to support real time analytics and streaming queries.</p> <p></p> <p>On the left the different data sources, injected using different protocols like MQTT, HTTP, or Kafka Connect... The business applications are supported by different microservices that are exposed by APIs and event-driven. The APIs is managed by API management product. Business events are produced as facts about the business entities, and persisted in the append log of kafka topic. Transactional data can be injected from MQ queues to Kafka topic, via MQ connectors. </p> <p>The data platform offers a set of capabilities to expose data for consumers like Data Science workbench (Watson Studio) via virtualization and data connections. The data are cataloged and governed to ensure integrity and visibility. The storage can be block based, document oriented or table oriented.</p> <p>Batch queries and map reduce can address huge data raw, while streaming queries can support real time aggregates and analytics.</p>"},{"location":"introduction/reference-architecture/#legacy-integration","title":"Legacy Integration","text":"<p>While you create new digital business applications as self-contained systems, you likely need to integrate legacy apps and databases into the event-driven system. Two ways of coming directly into the event-driven architecture are as follows:</p> <ol> <li> <p>Where legacy applications are connected with MQ. You can connect directly from MQ to the Kafka in the event backbone.  See IBM Event Streams getting started with MQ article. The major idea here is to leverage the transactionality support of MQ, so writing to the databased and to the queue happen in the same transaction:</p> <p></p> </li> <li> <p>Where databases support the capture of changes to data, you can publish changes as events to Kafka and hence into the event infrastructure. This could leverage the outbox pattern where events are prepared by the application and written, in the same transaction as the other tables, and read by the CDC capture agent.</p> <p></p> </li> </ol> <p>Or use an efficient CDC product to get the change data capture at the transaction level. IBM offers the best CDC product on the market, (InfoSphere Data Replication 11.4.0), with subsecond average latency and support full transactional semantics with exactly once consumption. It includes an efficient Kafka integration. </p> <p>One of the challenges of basic CDC products, is the replication per table pattern, leads to retry to rebuild the transaction integrity using kafka stream to join data from multiple topics. The TCC (Transactionally consistent consumer) technology allows Kafka replication to have semantics similar to a relational database. This dramatically increases the types of business logic that can be implemented. Developer can recreate the order of operations in source transactions across multiple Kafka topics and partitions and consume Kafka records that are free of duplicates by including the Kafka transactionally consistent consumer library in your Java applications.  TCC allows:</p> <ul> <li>Elimination of any duplicates, even in abnormal termination scenarios</li> <li>Reconstruction of exact transaction order, despite those transactions having been performance optimized and applied out of order to Kafka</li> <li>Reconstruction of exact operation order within a transaction, despite said operations having been applied to different topics and/or partitions.  This is not offered by default Kafka's \"exactly once\" functionality</li> <li>Ability for hundreds+ producers to participate in a single transaction.  Kafka's implementation has one producer create all messages for a transaction, despite those messages going to different topics.</li> <li>Provides a unique bookmark, so that downstream applications can check-point and resume exactly where they last left off if they fail.</li> </ul> <p>We recommend listeing to this presentation from Shawn Roberston - IBM, on A Solution for Leveraging Kafka to Provide End-to-End ACID Transactions</p> <p>The second, very important, feature is on the producer side, with the Kafka custom operation processor (or KCOP) infrastructure. KCOP helps you to control over the Kafka producer records that are written to Kafka topics in response to insert, update, and delete operations that occur on source database tables. It allows a user to programmatically dictate the exact key an byte values of the message written to Kafka. Therefore any individual row transformation message encoding format is achievable. Out of the box it includes Avro, CSV, JSON message encoding formats. It is possible to perform column level RSA encryption on certain values before writing. It also permits enriching of the message with additional annotation if needed. Developers have the complete choice over how data is represented. Eg. Can write data in Kafka Compaction compliant format with deletes being represented as Kafka tombstones or can write the content of the message being deleted.</p> <p>It also supports Kafka Header fields for efficient downstream routing without the need for immediate de-serialization. The KCOP allows a user to determine how many messages are written to Kafka in response to a source operation, the content of the messages, and their destination.</p> <ul> <li>Allows for filtering based on column values.</li> <li>Allows for writing the entire row with sensitive data to highly restricted topics and a subset of the columns to wider shared topics.</li> <li>Allows for writing the same message in two different formats to two different topics.  Useful in environments where some consuming applications want JSON, others prefer Avro, both can be produced in parallel if desired.</li> <li>Allows for sending special flags to a monitoring topic.  Eg. when a transaction exceeds $500, in addition to the regular message, a special topic is written to notifying of the large value transaction</li> </ul> <p>The two diagrams above, illustrate a common architecture for data pipeline, using event backbone, where the data is transformed into different data model, that can be consumed by components that act on those data, and move the data document into data lake for big data processing.</p> <p>Finally it is important to note that the deployment of the event streams, CDC can be colocated in the mainframe to reduce operation and runtime cost. It also reduces complexity. In the following diagram, event stream brokers are deployed on OpenShift on Linux on Z and the CDC servers on Linux too.</p> <p></p> <p>This architecture pattern try to reduce the MIPs utilization on the mainframe to the minimum by still ensuring data pipeline, with transactional integrity. </p> <ul> <li>Quality of Service \u2013 autoscaling / balancing between Linux nodes, Resilience.</li> <li>Latency  - memory speed (Network -&gt;  HiperSocket, with memory speed and bandwidth)</li> <li>Reduce MIPS  (avoid Authentication-TLS overhead on z/OS as no network traffic is encrypted)</li> <li>Avoid network spend / management / maintenance between servers</li> <li>Improved QoS for the Kafka service \u2013 inherits Z platform  (Event Streams is the only Kafka variant currently supported on Linix on Z) </li> <li>Reduced complexity / management cost</li> <li>Reduced latency / network infrastructure (apply \u2013 Kafka hop is now  in memory) \u2013 avoids need for encryption </li> </ul> <p>The CDC server uses Transaction Capture Consumer to keep transaction integrity while publishing to kafka topic. CICS Business events are mechanism for declaratively emitting event from CICS routines.</p>"},{"location":"introduction/target-audiences/","title":"Target Audiences","text":"<p>While the content of this repository is mostly technical in nature and is intended for a technical audience, it also introduces methodology practices, such as Event Storming, which would be used with business leaders to identify key business domain events and actions. You may find it useful to share this information with your business leaders before engaging them in such activities.</p> <p>At a high level, this is what you should expect to learn by working through this repository and the related examples:</p> <ul> <li>As an architect, you will understand how the event-driven architecture provides capabilities which support development of event-driven solutions.</li> <li>As a developer, you will understand how to develop event-driven applications and develop analytics based on event streams.</li> <li>As a project manager, you may understand all the artifacts which may be required for an event-driven solution.</li> </ul> <p>The related repositories provide sample code and best practices  which you may want to reuse during your future implementations. The reference architecture has been designed to be portable and applicable to Public Cloud, Hybrid Cloud and across multiple clouds. Examples given are directly deployable in IBM Public Cloud and Red Hat OpenShift Container Platform.</p>"},{"location":"introduction/usecases/","title":"Generic Business Use Cases","text":"<p>Updated 04/19/2022</p>"},{"location":"introduction/usecases/#business-use-cases","title":"Business use cases","text":"<p>In recent years, due to the business demands for greater responsiveness and awareness of context in  business decisions, organizations have taken a more strategic approach to supporting EDA.</p> <p>We have observed the following the main business justifications for adopting real-time processing, event-driven architecture.</p> <p></p> <ul> <li>Response to events in real time: Get visibility of events cross applications, even if they were not designed as event-producer and then act on those events, by deriving synthetic events or trigger business processes. Business are looking at better way to understand user's behavior on their on-line presence, and being able to act quickly for product recommendations, propose ad-hoc support, gather data for fraud detection, cause analysis of potential customer churn, fraud detection...</li> <li>Deliver responsive customer experiences: This is also about scaling web applications, locate processing closer to the end users, be resilient to underlying business services failure, separate read from write models, adoption reactive - manifesto while programming new business services</li> <li>Brings real-time intelligence: is about integrating analytics to real-time data streaming. Moving out of batch processing when it is relevant to compute aggregates on data in motion. This also includes embedding AI model, into the streaming application. Intelligence also means rule based reasoning, and complex event processing helps to recognize event patterns and act on them. The use </li> </ul> <p>Event in 2022, a lot of companies are not aware of what's is going on inside the company and with their customer's interactions.</p>"},{"location":"introduction/usecases/#technical-use-cases","title":"Technical use cases","text":"<p>Recurring technical needs and use cases are specifics for adopting event-driven architecture.  We can list the following important concerns:</p> <p></p> <ul> <li> <p>Communication layer: </p> </li> <li> <p>Adopt messaging and asynchronous communication between applications and  microservices for loosely coupled services. Messaging helps exchanging data between services and message brokers are needed to support the persistence and high availability to produce and consume those data. (See this note where we present a way to support a service mesh solution using asynchronous event). The data are pushed as immutables record, or commands are sent with exactly once delivery. Reducing pull data approach. </p> </li> <li> <p>Pub/sub messaging for cloud native applications to improve communication inter microservices.</p> </li> <li>Support Always-on services with asynchronous data replication.</li> <li>Expose data to any application to consume.</li> <li> <p>Time decoupling is important and consumers fetch data when they want.</p> </li> <li> <p>Reactive systems: to support reactive, responsive applications for addressing resiliency, scaling so adopting message buses.</p> </li> <li> <p>Adopt real-time processing moving out of batch processing when possible.</p> </li> <li> <p>Data Agility:</p> </li> <li> <p>The distributed nature of cloud native applications, means we need to address subjects like availability, consistency and resilience to network partitioning. Data consitency could not be guarantee, without strong transaction support, but multi-cloud, multi data centers,  application allocations in different container orchestrator, means dealing with eventual consistency.</p> </li> <li>Centralize online data pipeline to decouple applications and microservices.</li> <li>Monitor distributed applications to produce centralized feed of operational data.</li> <li>Logs collector from multiple services.</li> <li>Implement event sourcing pattern out of the box, using configuration to keep message for a long time period. Data are replicated between brokers within the cluster and cross availability zones if needed.</li> <li> <p>As data is visible in highly available manner, persisted in distributed broker cluster, those brokers are becoming data hub, data buffering for larger data pipeline processing.</p> </li> <li> <p>Near real-time analytics insights:</p> </li> <li> <p>Compute real-time aggregates, time window based reasoning, and even complex event-processing which looks after event sequencing patterns.</p> </li> <li>Aggregation of event coming from multiple producers.</li> <li>Look at event sequencing patterns</li> <li>Compute next best action from event streams</li> </ul>"},{"location":"journey/101/","title":"Learning Journey - getting started (101 content)","text":"<p> Updated 05/28/2022 - Ready for consumption - Open Issue for request for improvement </p> <p>This chapter is to get you understanding what is Event-Driven architecture, what is Kafka, how to consider Messaging as a service as a foundation for event-driven solution, and getting started on IBM Event Streams and IBM MQ.</p>"},{"location":"journey/101/#important-concepts-around-event-driven-solution-and-event-driven-architecture","title":"Important concepts around Event-driven solution and Event-driven architecture","text":"<p>First to get an agreement on the terms and definitions used in all  this body of knowledge content, with a clear definitions for <code>events</code>, <code>event streams</code>, <code>event backbone</code>, <code>event sources</code>....</p> <p>The main up-to-date reference architecture presentation is in this section with all the component descriptions.</p> <p></p> <p>This architecture is built after a lot of customer engagements, projects, and review, and  it is driven to support the main technical most common use cases  we observed for EDA adoption so far.</p>"},{"location":"journey/101/#understand-kafka-technology","title":"Understand Kafka technology","text":"<p>You may want to read from the [Kafka] documentation](https://kafka.apache.org/documentation/#intro_nutshell) to understand the main concepts, but we have also summarized those into this chapter and if you like quick  video, here is a seven minutes review of what Kafka is:</p> <p>The major building blocks are summarized in this diagram:</p> <p></p> <p>To learn more about:</p> <ul> <li>Kafka Cluster see this 101 content</li> <li>For Producer introduction see this section</li> <li>For Consumer introduction see this section</li> <li>For Kafka Connector framework introduction</li> </ul>"},{"location":"journey/101/#so-what-is-ibm-event-streams","title":"So what is IBM Event Streams?","text":"<p>Event Streams is the IBM packaging of Kafka for the Enterprise. It uses an award winner integrated user interface, kubernetes operator based on open source Strimzi,  schema registry based on OpenSource and connectors.  The phylosophie is to bring together Open Source leading products to support event streaming solution developments on kubernetes platform.  Combined with event-end point management, no-code integration and IBM MQ, Event Streams brings Kafka to the enterprise.</p> <p>It is available as a Managed Service or as part of Cloud Pak for integration. See this developer.ibm.com article about Event Streams on Cloud  for a quick introduction.</p> <p>The Event Streams product documentation as part of IBM Cloud Pak for Integration is here.</p>"},{"location":"journey/101/#runnable-demonstrations","title":"Runnable demonstrations","text":"<ul> <li>You can install the product using the Operator Hub, IBM Catalog and the OpenShift console.  This lab can help you get an environment up and running quickly.</li> <li>You can then run the Starter Application as explained in this tutorial. </li> <li>You may need to demonstrate how to deploy Event Streams with CLI, and this lab will teach you how to leverage our gitops repository</li> <li>To go deeper in GitOps adoption and discussion, this tutorial is using two public repositories to demonstrate how to do the automatic deployment of event streams and an event-driven solution.</li> <li>Run a simple Kafka Streams and Event Stream demo for real-time inventory using this demonstration scripts and GitOps repository.</li> </ul>"},{"location":"journey/101/#event-streams-resource-requirements","title":"Event streams resource requirements","text":"<p>See the detailed tables in the product documentation.</p>"},{"location":"journey/101/#event-streams-environment-sizing","title":"Event Streams environment sizing","text":"<p>A lot of information is available for sizing:</p> <ul> <li>Cloud Pak for integration system requirements</li> <li>Foundational service specifics</li> <li>Event Streams resource requirements</li> <li>Kafka has a sizing tool (eventsizer) that may be questionable but some people are using it.</li> </ul> <p>But in general starts small and then increase the number of nodes over time.</p> <p>Minimum for production is 5 brokers and 3 zookeepers.</p>"},{"location":"journey/101/#use-cases","title":"Use Cases","text":"<p>We have different level of use cases in this repository: </p> <p>The generic use cases which present the main drivers for EDA adoption are summarized here. </p> <p>But architects need to search and identify a set of key requirements to be supported by thier own EDA:</p> <ul> <li>get visibility of the new data created by existing applications in near real-time to get better business insights and propose new business opportunities to the end-users / customers.  This means adopting data streaming, and intelligent query solutions.</li> <li>integrate with existing transactional systems, and continue to do transaction processing while adopting eventual data consistency, characteristics of distributed systems. (Cloud-native serverless or microservices are distributed systems)</li> <li>move to asynchronous communication between serverless and microservices, so they become more responsive, resilient, and scalable (characteristics of \u2018Reactive applications\u2019).</li> <li>get loosely coupled but still understand the API contracts. API being synchronous (RES, SOAP) or async (queue and topic).</li> <li>get clear visibility of data governance, with cataloging applications to be able to continuously assess which apps securely access what, with what intent.</li> </ul> <p>We have done some reference implementations to illustrate the major EDA design patterns:</p> Scenario Description Link Shipping fresh food over sea (external) The EDA solution implementation using event driven microservices in different language, and demonstrating different design patterns. EDA reference implementation solution Vaccine delivery at scale (external) An EDA and cross cloud pak solution Vaccine delivery at scale Real time anomaly detection (external) Develop and apply machine learning predictive model on event streams Refrigerator container anomaly detection solution"},{"location":"journey/101/#why-microservices-are-becoming-event-driven-and-why-we-should-care","title":"Why microservices are becoming event-driven and why we should care?","text":"<p>This article explains why microservices are becoming event-driven  and relates to some design patterns and potential microservice code structure.</p>"},{"location":"journey/101/#messaging-as-a-service","title":"Messaging as a service?","text":"<p>Yes EDA is not just Kafka, IBM MQ and Kafka should be part of any serious EDA deployment. The main reasons are explained in this fit for purpose section and in the messaging versus eventing presentation.</p> <p>An introduction to MQ is summarized in this note in the 301 learning journey, developers need to get the MQ developer badge as it cover the basics for developing solution on top of MQ Broker.</p>"},{"location":"journey/101/#fit-for-purpose","title":"Fit for purpose","text":"<p>We have a general fit for purpose document that can help you reviewing messaging versus eventing, MQ versus Kafka, but also Kafka Streams versus Apache Flink.</p>"},{"location":"journey/101/#getting-started-with-event-streams","title":"Getting Started With Event Streams","text":""},{"location":"journey/101/#with-ibm-cloud","title":"With IBM Cloud","text":"<p>The most easy way to start is to create one <code>Event Streams on IBM Cloud</code> service instance, and then connect a starter application. See this service at this URL.</p>"},{"location":"journey/101/#running-on-you-laptop","title":"Running on you laptop","text":"<p>As a developer, you may want to start quickly playing with a local Kafka Broker and MQ Broker  on you own laptop. We have developed such compose files in this gitops project under the <code>RunLocally</code> folder.</p> <p><pre><code># Kafka based on strimzi image and MQ container\ndocker-compose -f maas-compose.yaml up &amp;\n# Kafka only\ndocker-compose -f kafka-compose.yaml up &amp;\n</code></pre> The kafka images are based on Strimzi images, but act as Event Streams one. So any code developed and configured on this image will run the same when deployed on OpenShift using Event Streams.</p>"},{"location":"journey/101/#running-on-openshift","title":"Running on OpenShift","text":"<p>Finally if you have access to an OpenShift Cluster, version 4.7 +, you can deploy  Event Streams as part of IBM Cloud Pak for Integration very easily using the OpenShift Admin console.</p> <p>See this simple step by step tutorial here which covers how to deployment and configuration an Event Streams cluster using the OpenShift Admin  Console or the <code>oc CLI</code> and our manifest from our EDA gitops catalog repository.</p>"},{"location":"journey/101/#show-and-tell","title":"Show and tell","text":"<p>Once you have deployed a cluster, access the administration console and use the very good getting started application.</p>"},{"location":"journey/101/#automate-everything-with-gitops","title":"Automate everything with GitOps","text":"<p>If you want to adopt a pure GitOps approach we have demonstrated how to use ArgoCD (OpenShift GitOps) to deploy an Event Streams cluster and maintain it states in a simple real time inventory demonstration in this repository.   </p>"},{"location":"journey/101/#methodology-getting-started-on-a-good-path","title":"Methodology: getting started on a good path","text":"<p>We have adopted the Event Storming workshop to discover the business process from an event point of view, and then apply domain-driven design practices to discover aggregate, business entities, commands, actors and bounded contexts. Bounded context helps to define microservice and business services scope. Below is a figure to illustrate the DDD elements used during  those workshops:</p> <p></p> <p>Bounded context map define the solution. Using a lean startup approach, we focus on defining a minimum viable product, and preselect a set of services to implement.</p> <p>We have detailed how we apply this methodology for a Vaccine delivery solution  in this article which can help you understand how to use the method for your own project.</p>"},{"location":"journey/101/#frequently-asked-questions","title":"Frequently asked questions","text":"<p>A separate FAQ document groups the most common questions around Kafka.</p>"},{"location":"journey/201/","title":"Learning Journey - deeper dive (201 content)","text":"<p>In this <code>201</code> content, you should be able to learn more about Kafka, Event Streams, Messaging, and Event-driven solution.</p>"},{"location":"journey/201/#more-kafka","title":"More Kafka","text":"<p>We have already covered the Kafka architecture in this section. When we deploy Event Streams on Kubernetes, it uses Operator, and it is in fact a wrapper on top of Strimzi, the open source kafka operator.</p> <p>Developer.ibm learning path: Develop production-ready, Apache Kafka apps</p>"},{"location":"journey/201/#strimzi","title":"Strimzi","text":"<p>Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters.  When the Strimzi Cluster Operator is up and runnning, it starts to watch for certain OpenShift or Kubernetes resources containing the  desired Kafka and/or Kafka Connect cluster configuration. </p> <p></p> <p>It supports the following capabilities:</p> <ul> <li>Deploy Kafka OOS on any OpenShift or k8s platform</li> <li>Support TLS and SCRAM-SHA authentication, and automated certificate management</li> <li>Define operators for cluster, users and topics</li> <li>All resources are defined in yaml file so easily integrated into GitOps</li> </ul> <p>The Cluster Operator is a pod used to deploys and manages Apache Kafka clusters, Kafka  Connect, Kafka MirrorMaker (1 and 2), Kafka Bridge, Kafka Exporter, and the Entity Operator. When deployed the following commands goes to the Cluster operator:</p> <pre><code># Get the current cluster list\noc get kafka\n# get the list of topic\noc get kafkatopics\n</code></pre>"},{"location":"journey/201/#installation-on-openshift","title":"Installation on OpenShift","text":"<p>The Strimzi operators deployment is done in two phases:</p> <ul> <li>Deploy the main operator via Subscription</li> <li>Deploy one to many instances of the Strimzi CRDs: cluster, users, topics...</li> </ul> <p>For that we have define subscription and configuration in this eda-gitops-catalog repo.  So below are the operations to perform:</p> <pre><code> # clone \n git clone https://github.com/ibm-cloud-architecture/eda-gitops-catalog.git\n # Define subscription\n oc apply -k kafka-strimzi/operator/overlays/stable/\n # The subscription creates an operator pod under the openshift-operators project\n oc get pods -n openshift-operators\n # Create a project e.g. strimzi\n oc new-project strimzi\n # deploy a simple kafka cluster with 3 brokers\n oc apply -k  kafka-strimzi/instance/\n # Verify installation\n oc get pods\n # should get kafka, zookeeper and the entity operator running.\n</code></pre> <p>The Strimzi documentation is very good to present a lot of configuration and tuning practices.</p>"},{"location":"journey/201/#application","title":"Application","text":"<p>All applications written with Kafka API will work the same way with Strimzi and Event Streams. So developer can use Strimzi images for their local development.</p>"},{"location":"journey/201/#production-deployment-high-availability","title":"Production deployment - High Availability","text":"<p>Kafka clustering brings availability for message replication and failover, see details in this high availability section. This chapter presents replicas, in-synch replicas concepts and addresses some broker failure scenarios that are important to understand.</p> <p>When looking how Kafka is deployed on Kubernetes / Openshift it is important to isolate each broker to different worker node as illustrated in this section.</p> <p>In end-to-end deployment, the high availability will become more of a challenge for the producer and consumer. Consumers and producers should better run on separate servers than the brokers nodes. Producer may need to address back preasure on their own. Consumers need to have configuration that permit to do not enforce partition to consumer reassignment too quickly. Consumer process can fail and restart quickly and get the same partition allocated.</p>"},{"location":"journey/201/#event-driven-solution-gitops-deployment","title":"Event-driven solution GitOps deployment","text":"<p>The \"event-driven solution GitOps approach\" article goes over how to use OpenShift GitOps to deploy Event Streams, MQ, and  a business solution. You will learn how to bootstrap the GitOps environment and deploy the needed IBM operators then use custom resource to define Event Streams cluster, topics, users...</p> <p>The following solution GitOps repositories are illustrating the proposed approach:</p> <ul> <li>refarch-kc-gitops: For the shipping fresh food overseas solution we have defined. It includes the SAGA choreography pattern implemented with Kafka</li> <li>eda-kc-gitops: For the shipping fresh food overseas solution we have defined. It includes the SAGA orchestration pattern implemented with MQ</li> <li>eda-rt-inventory-gitops to deploy the demo of real-time inventory</li> </ul>"},{"location":"journey/201/#performance-considerations","title":"Performance considerations","text":"<p>Read this dedicated article on performance, resilience, throughput.</p>"},{"location":"journey/201/#eda-design-patterns","title":"EDA Design patterns","text":"<p>Event-driven solutions are based on a set of design pattern for application design. In  this article, you will find the different pattern which are used a lot in the field like</p> <ul> <li>Event sourcing: persists, to an append log, the states of a business entity, such as an Order, as a sequence of immutable state-changing events.</li> <li>Command Query Responsibility Segregation: helps to separate queries from commands and help to address queries with cross-microservice boundary.</li> <li>Saga pattern: Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice, interested in other business entities, subscribes to those events and it can update its own state and business entities on receipt of these events. Business entity keys need to be unique and immutable.</li> <li>Event reprocessing with dead letter: event driven microservices may have to call external services via a synchronous call. We need to process failure in order to get response from those services using event backbone.</li> <li>Transactional outbox: A service command typically needs to update the database and send messages/events. The approach is to use an outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone. (Source Chris Richardson - Microservices Patterns)</li> </ul>"},{"location":"journey/201/#kafka-connect-framework","title":"Kafka Connect Framework","text":"<p>Kafka connect is an open source component for easily integrate external systems with Kafka.  It works with any Kafka product such as IBM Event Streams, Red Hat AMQ Streams, or Strimzi. You can learn more about it in this article and with those labs:</p> <ul> <li>Connect to S3 source and sink</li> <li>Connect to IBM Cloud Object Storage</li> <li>Connect to a Database with JDBC Sink</li> <li>Connect to MQ queue as source or Sink</li> <li>Connect to RabbitMQ</li> </ul>"},{"location":"journey/201/#integrate-with-mq","title":"Integrate with MQ","text":"<p>Using Kafka Connect framework, IBM has a MQ source connector and MQ Sink connector to integrate easily between Event Streams and IBM MQ The following labs will help you learn more about how to use those connectors and this gitops repository helps you to run a store simulation producing messages to MQ queue, with Kafka Connector injecting those message to Event Streams.</p>"},{"location":"journey/201/#introduction-to-schema-management","title":"Introduction to schema management","text":"<p>Schema management and schema registry are mandatory for doing production deployment, of any Kafka based solution. To understand the following components read this note</p> <p></p>"},{"location":"journey/201/#asyncapi","title":"AsyncAPI","text":"<p>This article on AsyncAPI management presents the value of using AsyncAPI in API Connect. This blog from development What is Event Endpoint Management? presents the methodology for event endpoint management:</p> <ul> <li>event description including the event schema, the topic, and thte communication protocol</li> <li>event discovery, with centralized management of the API</li> <li>self service to easily try the API, but with secure policies enforcement</li> <li>decoupled by API helps to abtract and change implementation if needed.</li> </ul>"},{"location":"journey/201/#debezium-change-data-capture","title":"Debezium change data capture","text":"<p>Change data capture is the best way to inject data from a database to Kafka.  Debezium is the Red Hat led open source project in this area. The IBM InfoSphere Data Replication is a more advanced solution for different sources and to kafka or other middlewares. </p> <p>One lab DB2 debezium not fully operational, looking for contributor to complete it.</p> <p>and an implementation of Postgresql debezium cdc with outpost pattern and quarkus.</p>"},{"location":"journey/201/#mirroring-data","title":"Mirroring Data","text":"<p>To replicate data between Kafka clusters, Mirror Maker 2 is the component to use. It is based on Kafka connector and will support a active-passive type of deployment or active active, which is little bit more complex.</p> <p></p> <p>See the detail Mirror Maker 2 technology summary</p>"},{"location":"methodology/data-intensive/","title":"Develop Data Intensive Application Methodology","text":"<p>In this section we are introducing the different elements of the software life cycles, particular to the development of intelligent applications that leverage data, machine learned models, analytics and cloud native microservices.</p> <p>The method supports lightweight development practices, to start the implementation of a MVP (Minimum Viable Product), to support scaling-up the architecture and the complexity of an end-to-end integrated solution. This method always guarantees to have deployable components at any point in time, a concept known in CI/CD (Continuous Integration, Continuous Delivery) applied to microservices, integration, data and machine learning models.</p>"},{"location":"methodology/data-intensive/#integrating-data-devops-and-ai-analytics-development-practices","title":"Integrating data - devops and AI-Analytics development practices","text":"<p>Most organizations need to manage software lifecycles. The key tenets listed above imply the need for a separate lifecycle for data, because the outcome or deliverable for any of the key tenets should not be considered static and immutable. Like data, you can think of analytics as having its own lifecycle independent from the software lifecycle and the data lifecycle, although they are all complementary.</p> <p>To help achieve digital transformation, your organization should integrate three development lifecycles:</p> <ul> <li>Software/Application</li> <li>AI-Analytics</li> <li>Data</li> </ul> <p></p> <p>Each development lifecycle is representative of an iterative workflow that can be used by agile development teams to craft higher value business outcomes. Lifecycles are often timeboxed iterations and can follow a fail-fast paradigm to realize tangible assets. Speed is important in business; most businesses work to meet new needs quickly. The objective of each lifecycle iteration is to address business speed efficiently and proficiently while maximizing business value. </p>"},{"location":"methodology/data-intensive/#personas","title":"Personas","text":"<p>We recommend reading this article to assemble the team to support a data-driven project where many roles support the project execution. The following table presents the icons we are using in subsquent figures, with a short description for the role.</p> <p></p>"},{"location":"methodology/data-intensive/#differences-between-analysts-and-data-scientists","title":"Differences between analysts and data scientists","text":"<p>The MITSloan did a review of the major differences between data scientists and business analysts by considering a set of common dimensions. The table below summarizes the results:</p> Analysts Data Scientists Types of data Structured mostly numeric data All data types, including unstructured data Preferred tools Statistical and modeling tools using data repository Programming language with strong statistical library, machine learning, deep learning. Use ML cluster servers Nature of work Report, predict, prescribe and optimize Explore, discover, investigate and visualize Typical educationl background Operations research, statistics, applied mathematics, predictive analytics Computer science, data science, cognitive science Mindset Entreprenaurial 69%, explore new ideas 58%, gain insights outside of formal projects  54% Entreprenaurial 96%, explore new ideas 85%, gain insights outside of formal projects  89%"},{"location":"methodology/data-intensive/#devops-lifecycle","title":"DevOps lifecycle","text":"<p>The software/application development lifecycle (SDLC) is a well-known and supports both traditional and agile development. The SDLC iterates on incorporating business requirements, adopt test driven development, continuous deployment and continuous integration. The diagram below demonstrates the iteration over recurring developer tasks to build the business intelligent application (internal loop), and the release loop (external) to continuously deliver application features to production.    </p> <p></p> <p>Before enterring the development iteration cycles, there are tasks to scope the high level business challenges and opportunities, define the business case for the project, define and build the development and operation strategy, define the target infrastructure, security... </p> <p>The smaller loop represents development iteration, while the outer loop represents software release to production with continous feedback to monitor and assess features acceptance. This task list is not exhaustive, but represents a common ground for our discussion.</p> <p>\"Understanding business objectives\" is a common task in each lifecycle, but in the context of microservice solution, adoption event storming practice and domain driven design will help understanding the business process, the data aggregates, and the bounded contexts.</p> <p>The solution will group a lot of out of the shelves components and a set of microservices supporting the implementation of the business logic and the intelligent application. A lot of things need to be considered while implementing each microservice from a data point of view. We recommend reading data pipeline for data intensive application to assess what needs to be done, and some best practices. </p> <p>Among the tasks described in these release and development iteration loops, we do not need to cover each of them, but may be highlight the Integration service task  which has a blue border to demonstrate integration activities between the different lifecycles, like ML model integration which is developed in the MLops iteration.</p>"},{"location":"methodology/data-intensive/#dataops","title":"DataOps","text":"<p>The data development lifecycle (DataOps) places the data management philosophy into an organic and evolving cycle that is better suited to the constantly changing needs of a business. The DataOps is impacted by the DevOps and the MLOps. It iterates on both the incorporation of business requirements and on the manifestation of data. Data has value and the evaluation of data article introduces to the concepts to recognize the value of data.</p> <p></p> <p>The discover business objectives activity task, groups a set of different subjects depending of the context: data, integration, machine learning model development. The goal is to highlight the measurable outcomes expected by business stakeholders. The build a business objective article) presents the concepts and some questions that can be used to assess the general business requirements and current knowledge of the data. And the translating a business problem into an AI and data science solution practice helps the analysis team to assess what data may be needed and what kind of model to develop.</p> <p>As you can see activities are addressing data preparation and understanding, so data architecture need to be in place before doing any data sciences work.</p> <p>As part of the gather data requirements, it is important to review the dimensions of value as introduced in the \"valuation of data\" article, then formalize the value chain of the data  in the scope of the project, and finally address if the data contains the correct information to answer the business challenges and support the business process.</p> <p><code>Transform data for AI</code> and <code>Deploy data integration flow</code> tasks have different border colors to demonstrate integration activities between the different lifecycles.</p>"},{"location":"methodology/data-intensive/#mlops-lifecycle","title":"MLOps lifecycle","text":"<p>The Machine learning development lifecycle supports the full spectrum of analytical work in the artificial intelligence ladder. This lifecycle incorporates model development and remediation to avoid drift. Because one of the purposes of analytics is to enable an action or a decision, MLOps relies on feedback mechanisms to help enhancing machine models and the overall analytical environment. </p> <p></p> <p>An example of a feedback mechanism is capturing data points on the positive or negative effects or outcomes from an action or a decision. This process iterates on data. </p> <p>The developed AI or Analytics model is deployed as one to many services that are integrated in the microservice architecture. So synchronization with devops team is important and part of the method.  </p> <p>Understanding the data and analytics goals task is explained in this note with the business analytic patterns.</p> <p>Defining the analytic approach task groups sub activities that help to understand the past activity and assess what kind of predictions, actions are expected by the business users. The patterns and goals analysis will help to assess for supervised or unsupervised leaning needs.</p>"},{"location":"methodology/data-intensive/#integrating-the-cycles","title":"Integrating the cycles","text":"<p>Although the three lifecycles are independents, you can use them together and establish dependencies to help drive business outcomes. Each lifecycle should be agile and should be incorporated into a DevOps process for development and deployment.</p> <p></p> <p>The intersection of the three lifecycles highlights the need for unified governance. The intersection between software/app and data, highlights integration and access paths to information. The intersection between data and analytics, highlights integration with the underlying data stores. The intersection between analytics and software/app highlights integration and the use of APIs or other data exchange techniques to assist in resolving complex algorithms or access requirements.</p> <p>An other interesting view, is to consider the high level artifacts built in those overlapping areas, as they are very important elements to project managed efficiently to avoid teams waiting for others. </p> <p></p> <p>Interface definitions and data schema are important elements to address as early as possible in the SDLC. Data access integration includes dedicated microservices managing the full lifecycles and business operations for each major business entities of the solution. The integration can be event-based and adopt an event-driven architecture.</p> <p>The <code>data store integration</code> addresses storage of high volume data, but also access control, any transformation logic, and event data schema to be consumable by AI workbench.</p> <p>The AI model as a service can be mocked-up behind <code>Facade</code> interface so the developed microservice in need to get prescriptive scoring can be developed with less dependencies. </p> <p>Finally the integration of those three lifecycle over time can be presented in a Gantt chart to illustrate the iterations and the different focuses over time.</p> <p></p> <p>Each development life cycle includes architecture and development tasks. Architecture activities focus on defining infrastructure for runtimes and machine learning environment as well as data topology etc...</p> <p>The different iterations of the data, IA-Analytics and devops life cycle are synchronized via the integration artifacts to build. When components are ready for production, the go-live occurs and the different <code>operate</code> tasks are executed, combined with the different monitoring. From the production execution, the different teams get continuous feedbacks to improve the application.</p> <p>The AI-Analytics tasks are colored in blue and green, on purpose to show the strong dependencies between data and AI. This means the data activities should start as early as possible before doing too much of modeling. </p>"},{"location":"methodology/data-intensive/#challenges","title":"Challenges","text":"<p>There are a set of standard challenges while developing an IT solution which integrates results  from analytics model. We are listing some that we want to address, document and support as  requirements.</p> <ul> <li>How will the model be made available to developers?</li> <li>Is it a batch process updating/appending static records or real time processing on a data stream or transactional data</li> <li>How to control resource allocation for Machine Learning job.</li> <li>How to manage consistency between model, data and code: version management / data lineage</li> <li>How to assess the features needed for the training sets.</li> </ul>"},{"location":"methodology/data-intensive/#the-ibm-method-for-cloud-with-datafirst","title":"The IBM Method for Cloud with DataFirst","text":"<p>Every department within your organization has different needs for data and analytics. How can you start your data-driven journey? The IBM Method for Cloud with DataFirst provides strategy and expertise to help you gain the most value from your data. This method starts with your business outcomes that leverage data and analytics, not technology. Defining focus in a collaborative manner is key to deriving early results. Your roadmap and action plan are continuously updated based on lessons learned. This is an iterative and agile approach to help you define, design, and prove a solution for your specific business problem.</p>"},{"location":"methodology/data-lineage/","title":"Data lineage","text":"<p>From a stream data platform adoption point of view, we have multiple components working on data, with online and offline processing. The following figure illustrates the concepts and components we may need to consider as part of the data lineage discussion:</p> <p></p> <ul> <li>Mobile, web single page application, IoT, microservice app, change data capture (not presented here) and SaaS offering applications are source of events. Event backbone persists those events in append log with long retention time (days). </li> <li>Stream applications are running statefule operations to compute real time analytics or doing complex event processing. Kafka Streams and Apache Flinks are the technology of choices. </li> <li>On the right side, consumer can trigger what will be the next best action to perform on a specific event. Those action can be a business process, a product recommendations, an alert...</li> <li>Often data can be persisted on a longer retention storage, with full cloud availability like S3 buckets (Cloud Object storage on IBM Cloud or on Premise).</li> <li>Big data processing plaform like Sparks can run batch processing, map reduce type, on data at rest, that comes from the data ingestion layer supported but the event backbone.</li> <li>Finally business dashboard can integrate queries on data at rest and in motion via interactive / streaming queries.</li> </ul> <p>With those simple component view we can already see data lineage will be complex, so we need practices and tools to support it. As more data is injected into the data platform, the more you need to be able to answer a set of questions like:</p> <ul> <li>Where the data is coming from </li> <li>Who create the data</li> <li>Who own it</li> <li>Who can access those data and where</li> <li>How can we ensure data quality</li> <li>Who modify data in motion</li> </ul>"},{"location":"methodology/data-lineage/#data-lineage-requirements","title":"Data lineage requirements","text":"<p>Data lineage describes data origins, movements, characteristics, ownership and quality. As part of larger data governance initiative, it may encompass data security, access control, encryption and confidentiality. </p>"},{"location":"methodology/data-lineage/#contracts","title":"Contracts","text":"<p>In the REST APIs, or even SOA, worlds request/response are defined via standards like OpenAPI or WSDL. In the event and streaming processing the AsynchAPI is the specification to define schema and middleware binding.</p>"},{"location":"methodology/data-lineage/#openapi","title":"OpenAPI","text":"<p>We do not need to present OpenAPI but just the fact that those APIs represent request/response communication and may be managed and integrated with the development life cycle. Modern API management platform should support their life cycle end to end but also support new specifications like GraphQL and AsynchAPI.</p>"},{"location":"methodology/data-lineage/#asynchapi","title":"AsynchAPI","text":"<p>Without duplicating the specification is the specification to define schema and middleware binding. we want to highlight here what are the important parts to consider in the data goverance:</p> <ul> <li>The Asynch API documentation which includes:<ul> <li>The server definition to address the broker binding, with URL and protocol to be used. (http, kafka, mqtt, amqp, ws)</li> <li>The channel definition which represents how to reach the brokers. It looks like a Path definition in OpenAPI</li> <li>The message definition which could be of any value. Apache Avro is one way to present message but it does not have to be.</li> <li>Security to access the server via user / password, TLS certificates, API keys, OAuth2...</li> </ul> </li> <li>The message schema</li> </ul> <p>The format of the file describing the API can be Yaml or Json. </p>"},{"location":"methodology/data-lineage/#schema","title":"Schema","text":"<p>Schema is an important way to ensure data quality by defining a strong, but flexible, contract between producers and consumers and to understand the exposed payload in each topics. Schema definitions improve applications robustness as downstream data consumers are protected from malformed data, as only valid data will be permitted in the topic. Schemas need to be compatible from version to version and Apache Avro supports defining default values for non existing attribute of previous versioned schema. Schema Registry enforces full compatibility when creating a new version of a schema. Full compatibility means that old data can be read with the new data schema, and new data can also be read with a previous data schema.</p> <p>Here is an example of such definition:</p> <p><pre><code>\"fields\": [\n     { \"name\": \"newAttribute\",\n       \"type\": \"string\",\n       \"default\": \"defaultValue\"\n     }\n]\n</code></pre> Remarks: if you want backward compatibility, being able to read old messages, you need to add new fields with default value.To support forward compatibility you need to add default value to deleted field, and if you want full compatibility you need to add default value on all fields.</p> <p>Metadata about the event can be added as part of the <code>field</code> definition, and should include source application reference, may be a unique identifier created at the source application to ensure traceability end to end, and when intermediate streaming application are doing transformation on the same topic, the transformer app reference. Here is an example of such data:</p> <pre><code>\"fields\": [\n     { \"name\": \"metadata\",\n       \"type\": {\n           \"name\": \"Metadata\",\n           \"type\": \"record\",\n           \"fields\": [\n               {\"name\": \"sourceApp\", \"type\": \"string\"},\n               {\"name\": \"eventUUID\", \"type\": \"string\"},\n               {\"name\": \"transformApps\", \"type\": \n                { \"name\" : \"TransformerReference\",\n                  \"type\": \"array\",\n                  \"items\": \"string\"\n                }\n               }\n           ]\n       }\n     }\n]\n</code></pre> <p>The classical integration with schema registry is presented in the figure below:</p> <p></p> <p>Schema registry can be deployed in different data centers and serves multi Kafka clusters. For DR, you need to use a 'primary' server and one secondary in different data center. Both will receive schema update via DevOps pipeline. One of the main open source Schema Registry is Apicurio, which is integrated with Event Streams and in most of our implementation. Apicurio can persist schema definition inside Kafka Topic and so schema replication can also being synchronize via Mirror Maker 2 replication. If Postgresql is used for persistence then postgresql can be used for replicating schemas.</p> <p>We recommend reading our schema registry summary and our OpenShift lab or Event Streams on Cloud schema registry lab.</p> <p>Integrating schema management with code generation and devOps pipeline is addressed in this repository.</p>"},{"location":"methodology/domain-driven-design/","title":"Domain Driven Design","text":"<p>This section describes how to apply domain driven design with event based application and describe the high level steps which uses output from the event storming session and derives a set of micro services design specifications.</p>"},{"location":"methodology/domain-driven-design/#overview","title":"Overview","text":"<p>Event storming is part of the domain-driven design methodology. And domain-driven design was deeply described in Eric Evans's \"Domain Driven Design: Tackling Complexity in the Heart of Software\" book from 2004.</p> <p>From this body of knowledge we can derive the following important elements:</p> <ul> <li>Domain Driven Design is about understanding the business domain in which the solution has to be developed</li> <li>Context is important to define the business term definitions within it</li> <li>For microservice we need to find the boundary of responsability to be able to apply the clear separation of concern pattern. </li> </ul> <p>The goals for the design step are:</p> <ul> <li>Improve communication between subject matter experts and developer, make the code readable by human and long term maintanable </li> <li>To support highly modular cloud native microservices.</li> <li>To adopt event coupled microservices - facilitating independent modification and evolution of each microservice separately.</li> <li>To allow applying event-driven patterns such as event sourcing, CQRS and SAGA to address some of the challenges of distributed system implementation: data consitency, transaction cross domains, and complex queries between aggregates managed by different services.</li> </ul> <p>DDD suits better long term project, where the domain of knowledge is important and complex, and may be less appropriate for small projects.</p>"},{"location":"methodology/domain-driven-design/#starting-materials","title":"Starting materials","text":"<p>At IBM client engineering, we start with a design thinking workshop enhanced with event storming to model the to-be scenario with an end-to-end business process discovery as needed.  Discovering events using event storming simplifies the process discovery and sequencing of tasks. BPMN modeling may also being used to model a process but it has  the tendency to lock the thinking in a flow, while event storming focuses on what happened as facts, and so it easier to get events sequencing.  Also events coming from no-where could still be discovered and have their value. Event storming workshop starts some domain driven design analysis by getting some of the following materials:</p> <ul> <li>Domains - sub domains</li> <li>Event Sequence flow.</li> <li>Events \u2013 business term definition: the start of ubiquitous language.</li> <li>Critical events.</li> <li>Business entities, aggregates, value objects</li> <li>Commands</li> <li>Actors - Users \u2013 based on Empathy maps and stakeholders list.</li> <li>Event linkages.</li> <li>Business Policies.</li> <li>Event prediction and probability flows.</li> </ul> <p>The derivation of those materials was described in the event storming introduction.</p> <p>Here is an example of starting material illustrating the beginning of the process for the Reefer shipping business process, with order placement and shipping contract elaboration:</p> <p></p> <p>From there we complement this analysis by extending it with the domain driven design elements.</p>"},{"location":"methodology/domain-driven-design/#domain-driven-design-steps","title":"Domain driven design steps","text":""},{"location":"methodology/domain-driven-design/#step-1-assess-domains-and-sub-domains","title":"Step 1: Assess domains and sub-domains","text":"<p>Domain is what an organization does, and it includes the \"how to\", it performs its operations. Domain may be composed of sub-domains. A \"Core\" domain is a part of the business domain that is of primary importance to  the success of the organization, the organization needs to excel at it to make business impact and difference. The application to be built is within a domain.</p> <p>Domain model is a conceptual object model representing part of the domain to be used in the solution. It includes behavior and data. </p> <p>During the event storming analysis, you define the domains and groups a set of sub-domains together.  In the following figure, the container shipping domain is what the application we have to develop belongs to,  and is composed of sub-domains like orders, shipping, inventory, .... Other domains like weather, CRM, invoice,  are supporting the shipping domain but are not the focus of the design. Here is an example of such domain and subdomains:</p> <p></p> <p>We have three core sub-domains and the rest are supports. Shipping over seas company needs to excel at managing container inventory, managing the shipping, the itineraries, and vessels.  </p>"},{"location":"methodology/domain-driven-design/#step-2-define-the-application-context","title":"Step 2: Define the application context","text":"<p>At the high level, when doing the analysis, you should have some insight decisions of the top level application to develop,  with some ideas of the other systems to interact with. A classical \"system context diagram\" is a efficient tool  to represent the application high level context. The external systems to integrate with, are strongly correlated  to the domains discovered from previous step. </p> <p>For the EDA reference implementation solution the system context diagram is visible here.</p> <p>Each interface to those system needs to be documented using the interface characteristics approach presented by IBM Kim Clark.</p> <p>The full set of interface characteristics to consider for each system to integrate with is summarized below:</p> <ul> <li> <p>FUNCTIONAL DEFINITION</p> <ul> <li>Principal data objects</li> <li>Operation/function</li> <li>Read or change</li> <li>Request/response objects</li> </ul> </li> <li> <p>TECHNICAL INTERFACE</p> <ul> <li>Transport</li> <li>Protocol</li> <li>Data format</li> </ul> </li> <li> <p>INTERACTION TYPE</p> <ul> <li>Request-response or fire-forget</li> <li>Thread-blocking or asynchronous</li> <li>Batch or individual</li> <li>Message size</li> </ul> </li> <li> <p>PERFORMANCE</p> <ul> <li>Response times</li> <li>Throughput</li> <li>Volumes</li> <li>Concurrency</li> </ul> </li> <li> <p>INTEGRITY</p> <ul> <li>Validation</li> <li>Transactionality</li> <li>Statefulness</li> <li>Event sequence</li> <li>Idempotence</li> </ul> </li> <li> <p>SECURITY</p> <ul> <li>Identity/authentication</li> <li>Authorization</li> <li>Data ownership</li> <li>Privacy</li> </ul> </li> <li> <p>RELIABILITY</p> <ul> <li>Availability</li> <li>Delivery assurance</li> </ul> </li> <li> <p>ERROR HANDLING</p> <ul> <li>Error management capabilities</li> <li>Known exception conditions</li> <li>Unexpected error presentation</li> </ul> </li> </ul> <p>So with a system context diagram and interface characteristics to external system we have a good undestanding of the  application context and interaction with external domains.</p>"},{"location":"methodology/domain-driven-design/#step-3-define-the-ubiquitous-language","title":"Step 3: Define the ubiquitous language","text":"<p>Eric Evans: \"To communicate effectively, the code must be based on the same language used to write the requirements, the same language the developers speak with each other and with domain experts\"</p> <p>This is where the work from the event storming and the relation with the business experts should help. Domain experts use their jargon while technical team members have their own language tuned for discussing the domain in terms of design. The terminology of day-to-day discussions is disconnected from the terminology embedded in the code, so the ubiquitous language helps to allign knowledge with design elements, code and tests. (Think about EJB, JPA entity, all the JEE design jargon versus ShippingOrder, order provisioning, fullfillment... )</p> <p>The vocabulary of that ubiquitous language includes the class names and prominent operation names. The language includes terms to discuss rules that have been made explicit in the model. Be sure to commit the team to exercising that language relentlessly in all communication within the business and in the code. Use the same language in diagrams, writing, and especially speech.</p> <p>Play with the model as you talk about the system. Describe scenarios out loud using the elements and interactions of the model, combining concepts in ways allowed by the model. Find easier ways to say what you need to say, and then take those new ideas back down to the diagrams and code.</p>"},{"location":"methodology/domain-driven-design/#entities-and-value-objects","title":"Entities and Value Objects","text":"<p>Entities are part of the ubiquitous language, and represent business concepts that can be uniquely identified by some attributes.  They have a life cycle that is important to model, specially in the context of an event-driven solution.</p> <p>Value Objects represent things in the domain but without identity, and they are frequently transient, created for an operation and then discarded.</p> <p>Some time, in a certain context, a value object could become an entity. As an example, an Order will be a value object in the  context of a fullfilment domain, while a core entity in order management domain.</p> <p>Below is an example of entities (Customer and Shipping Order) and value objects (delivery history and delivery specification):</p> <p></p>"},{"location":"methodology/domain-driven-design/#aggregate-boundaries","title":"Aggregate boundaries","text":"<p>An aggregate is a cluster of associated objects that we treat as a unit for the purpose of data changes.  It is a collection of values and entities which are bound together by a root entity (the aggregate root). An entity is most likely an aggregate and every things related to it define its boundaries. External client consider the aggregate as a all. It guaranteees the consistency of changes made within the aggregate.</p>"},{"location":"methodology/domain-driven-design/#bounded-contexts","title":"Bounded Contexts","text":"<p>Bounded Context explicitly defines the boundaries of your model. A language in one bounded context can model  the business domain for the solving of a particular problem.  This concept is critical in large software projects.  A Bounded Context sets the limits around what a specific team works on and helps them to define their own vocabulary within  that particular context. When you define a bounded context, you define who uses it, how they use it, where it applies within  a larger application context, and what it consists of, in terms of things like OpenAPI documentation and code repositories.</p> <p>Within a business context every use of a given domain terms, phrases, or sentences, the Ubiquitous Language, inside  the boundary has a specific contextual meaning. So order context is a bounded context and groups order, ordered product type,   pickup and shipping addresses, delivery specifications, delivery history....</p>"},{"location":"methodology/domain-driven-design/#context-maps","title":"Context maps","text":"<p>Bounded contexts are not independent, a solution uses multiple bounded contexts, which are interacting with each others. The touchpoint between business contexts is a contract.</p> <p>To define relationships and integrations between bounded contexts we can consider the nature of the collaboration between teams, which can  be grouped as:</p> <ul> <li>Cooperation: Uses well established communication and control. Change to the APIs are immediately communicated, the integration is both ways and teams solve together integration issues.  If more formal cooperation is needed, the shared kernel pattern can be used and technic like contract testing is used. Sometime the shared kernel can be deployed a mediation flow, canonical model in enterprise service bus. But the scope has to be small anyway.</li> <li>Customer-supplier: the supplier context provides a service to its customers. Upstream or the downstream team can dictate the integration contract. With conformist supplier defines the contract based on its model and domain, and customer conforms to it.  When the customer needs to translate the upstream bounded context model into its own model, it uses anticorruption layer.  Anticorruption layer helps to isolate from messy model, model with high velocity of changes, or when the consumer model is core to its operations. When power is on the consumer side, then the supplier uses open-host service by decoupling its interface from the implementation, and design it for the consumer. This is the published language.</li> <li>Separate ways: no collaboration at all. Which leads to duplicate functionality in multiple bounded contexts.  Avoid this solution when integrating sub-domains.</li> </ul> <p>The context map illustrates the integration between bounded contexts. It is the first high level design of the system components and the models they implements. The diagram below represents a simple view of e-commerce domain with the sub domains and bounded contexts</p> <p></p>"},{"location":"methodology/domain-driven-design/#business-operation-api","title":"Business operation API","text":"<p>As part of the commands discovered during the event storming, some of them are related to business operations that could be managed and have business objective in term of cost control or new revenu stream. </p> <p>If needed, some more specific requirement about the APIs can be formalized using the API adoption canvas, which helps to capture all key aspects of API adoption with a lean approach.</p> <p>The characteristics to consider for a development point of view and DDD are:</p> <ul> <li>API product description</li> <li>SLA and expected performance </li> <li>Exposure to developer communities</li> <li>Expected short, medium and long term API management and versioning strategy</li> </ul> <p>For an implementation point of view, when mapping API to RESTful resource the data and verbs need to be defined, When APIs are asynchronous, the description of the delivery channel becomes important.</p>"},{"location":"methodology/domain-driven-design/#repositories","title":"Repositories","text":"<p>Repository represents the infrastructure service to persist the root aggregate during its full life cycle. Client applications request objects from the repository using query methods that select objects based on criteria specified  by the client, typically the value of certain attributes. Application logic never accesses storage implementation directly, only via the repository.</p>"},{"location":"methodology/domain-driven-design/#event-linked-microservices-design-structure","title":"Event linked microservices design - structure","text":"<p>A complete event driven microservice specification (the target of this design step) includes specifications of the following elements:</p> <ul> <li>Event Topics<ul> <li>Used to configure the Event Backbone</li> <li>Mapped to the life cycle of the root entity</li> <li>Topics can be chained to address different consumer semantic</li> <li>Single partition for keeping order and support exactly once delivery</li> </ul> </li> <li>Event types within each event topic</li> <li>Microservices:<ul> <li>They may be finer grained than aggregates or mapped to aggregate boundaries.</li> <li>They may separate query and command; possibly multiple queries.</li> <li>They could define demonstration control and serve main User Interface.</li> <li>Reference the related Entities and value objects within each microservice.</li> <li>Define APIs  ( Synchronous or asynchronous) using standards like openAPI. Could be done bottom up from the code, as most of TDD implementation will lead to.</li> <li>Topics and events Subscribed to.</li> <li>Events published / emitted.</li> </ul> </li> <li>List of end to end interactions:<ul> <li>List of logic segments per microservice</li> </ul> </li> <li>Recovery processing, scaling:<ul> <li>We expect this to be highly patterned and template driven not requiring example-specific design.</li> </ul> </li> </ul>"},{"location":"methodology/domain-driven-design/#step-4-define-modules","title":"Step 4: Define modules","text":"<ul> <li> <p>Each aggregate will be implemented as some composition of:</p> <ol> <li>a command microservice managing state changes to the entities in this aggregate</li> <li>possibly one or more separate (CQRS) query services providing internal or external API query capabilities</li> <li>additional simulation, predictive analytics or User Interface microservices</li> <li>The command microservice will be built around a collection of active entites for the aggregate, keyed by some primary key.</li> <li>The separation of each aggregate into specific component microservices as outlined above, will be a complete list of microservices for the build / sprint.</li> <li>Identify the data collections, and collection organization (keying structure) in each command and query microservice for this build.</li> </ol> </li> </ul>"},{"location":"methodology/domain-driven-design/#step-5-limit-the-context-and-scope-for-this-particular-build-sprint","title":"Step 5: Limit the context and scope for this particular build / sprint","text":"<p>We assume that we are developing a particular build for a sprint within some agile development approach, deferring additional functions and complexity to later sprints:</p> <ul> <li>Working from the initial list of aggregates, select which aggregates will be included in this build</li> <li> <p>For each aggregate the possible choices are:</p> <ol> <li>to completely skip and workaround the aggregate in this build.</li> <li>to include a full lifecycle implementation of the aggregate</li> <li>to provide a simplified lifecycle implementation - typically a table of entities is initialized at start up, and state changes to existing entities are tracked</li> </ol> </li> <li> <p>Determine whether there are simulation services or predictive analytics service to be included in the build</p> </li> <li>Identify the external query APIs and command APIs which this build should support</li> <li>Create entity lifecycle diagrams for entites having a full lifecycle implementation in this build / sprint.</li> </ul>"},{"location":"methodology/domain-driven-design/#step-6-generate-microservice-interaction-diagrams-for-the-build","title":"Step 6: Generate microservice interaction diagrams for the build","text":"<ul> <li>The diagram will show API calls initiating state change. They should map the commands discovered during the event storming sessions.</li> <li>It shows for each interaction whether this is a synchronous API calls or an asynchronous event interaction via the event backbone.</li> <li>The diagram labels each specific event interaction between microservices trigerring a state change.</li> <li>Typically queries are synchronous API calls since the caller cannot usefully proceeed until a result is returned.</li> <li> <p>From these, we can extract:</p> <ol> <li>a complete list of event types on each event backbone topic, with information passed on each event type.</li> <li>the complete list of \u201clogic segments\u201d for each microservice processing action in response to an API call or initiating event.</li> </ol> </li> <li> <p>When, at the next level of detail, the individual fields in each event are specified and typed, the CloudEvents standard may be used as a starting point.</p> </li> </ul>"},{"location":"methodology/domain-driven-design/#step-7-tdd-meets-ddd","title":"Step 7: TDD meets DDD","text":"<p>Test Driven Development is a well established practice to develop software efficiently. Since 2003 and extreme programming practice, it helps when doing refactoring, improve code quality and focus on simple design. As Dan North stated in 2018, it could have been bettwe name 'Example Guided Design' as starting by the test improve the design, and writing tests that map example and end user stories will make it more clear and mapped to the requirements. With DDD, TDD needs to support the ubiquituous language and have tests intent clearly stated.  We can easily adopt the following practices:</p> <ul> <li>Isolate tests per bounded context</li> <li>Adopt collective ownership of code and tests</li> <li>Pair programming between tester and developer</li> <li>Enforce contunuous feedbacks</li> <li>Keep tests in source control with the code and for integration tests in a dedicated folder or even repository. Have their executions as part of CI/CD pipeline.</li> <li>Write test with te Gherkin syntax:</li> </ul> <pre><code>Feature: title of the scenario or test method\nGiven [initial context]\nWhen [event or trigger]\nthen [expected output]\n</code></pre> <p>Here is an example of such structure in Junit 5 test:</p> <pre><code>public void given_order_created_should_emit_event() throws OrderCreationException {\n    assume_there_is_no_event_emitted();\n    OrderEventPayload new_order = ShippingOrderTestDataFactory.given_a_new_order();\n    // when\n    service.createOrder(new_order);\n    // then\n    assertTrue(order_created_event_generated());\n    OrderCommandEvent createdOrderEvent = (OrderCommandEvent)commandEventProducer.getEventEmitted();\n    assertTrue(new_order.getOrderID().equals(((OrderEventPayload)createdOrderEvent.getPayload()).getOrderID()));\n}\n</code></pre>"},{"location":"methodology/domain-driven-design/#step-8-address-failover-and-other-nfrs","title":"Step 8: Address failover and other NFRs","text":"<p>NFRs impact architecture decision, technology choices and coding. So address those NFRs as early as possible during requirements and analysis and then design phases. </p> <ul> <li>If a microservice fails it will need to recover its internal state by reloading data from one or more topics, from the latest committed read.</li> <li>In general, command and query microservices will have a standard pattern for doing this.</li> <li>Any custom event filtering and service specific logic should be specified.</li> </ul>"},{"location":"methodology/domain-driven-design/#concepts-and-rationale-underlying-the-design-approach","title":"Concepts and rationale underlying the design approach","text":"<p>What is the difference between event information stored in the event backbone and state data stored in the microservices?</p> <p>The event information stored persistently in the event backbone is organized by topic and, within each topic, entirely by event time-of-occurrence. While the state information in a microservice is a list (collection) of all currently active entities of the owning aggregate (e.g. all orders, all voyages etc) and the current state of each such entity. The entity records are keyed by primary key, like an OrderID. While implementing microservice using event sourcing, CQRS, the persisted entity records are complementary to the historically organized information in the event backbone.</p> <p>When is it acceptable to be using synchronous interactions between services instead of asynchronous event interacts through the event backbone?</p> <p>For non-state-changing queries, for which the response is always instantaneously available a synchronous query call may be acceptable and will provide a simpler more understandable interface. Any processing which can be though of as being triggered by some state change in another aggregate should be modelled with an asynchronous event, because as the solution evolves other new microservices may also need to be aware of such event. We do not want to have to go back and change logic existing service where this event originated to have that microservice actively report the event to all potential consumers.</p> <p>How do we save microservices from having to maintain data collections with complex secondary indexing for which eventual consistency will be hard to implement?</p> <ul> <li>Each command  microservice should do all its state changing updates using the primary key lookup only for its entities.</li> <li>Each asynchronous event interaction between microservices should carry primary entityIds ( orderID, VoyageID, shipID) for any entities associated with the interaction.</li> <li>Each query which might require speciaoized secondary indexing to respond to queries can be implemented in a separate CQRS query service which subscribes to events  to do all internal updating and receives events from the event backbone in a ( Consistent) eventually correct order.</li> <li>This allows for recovery of any failed service by rebuilding it in \"eventually correct\" order.</li> </ul>"},{"location":"methodology/domain-driven-design/#applied-ddd-for-the-reference-implementation","title":"Applied DDD for the reference implementation","text":"<p>See this article to see how we applied DDD for the shipping good over seas implementation.</p>"},{"location":"methodology/domain-driven-design/#more-reading-and-sources","title":"More reading and sources","text":"<ul> <li>Eric Evans' book</li> <li>Scott Millet and Nick Tune's book: \"Patterns, Principles, and Practices of Domain-Driven Design\"</li> <li>Kyle Brown's article: Apply Domain-Driven Design to microservices architecture</li> </ul>"},{"location":"methodology/event-storming/","title":"Event Storming","text":"<p>In this article we are presenting an end to end set of activities to run a successful Minimum Viable Product for an event-driven solution using cloud native microservices and event backbone as the core technology approach.</p> <p>The discovery and analysis of the MVP scope starts with an event storming workshop where designer, architect work hand to hand with business users and domain subject matter experts. From the different outcomes of the workshop, the development team starts to outline components, microservices, business entity life cycle, etc... in a short design iteration. The scope is well defined Epics, Hill and user stories defined, at least for the first iterations, and the MVP can start.</p>"},{"location":"methodology/event-storming/#event-storming-workshop-introduction","title":"Event Storming workshop introduction","text":"<p>Event storming is a workshop format for quickly exploring complex business domains by focusing on domain events generated in the context of a business process or a business application. A domain event is something meaningful to the experts that happened in the domain. The workshop focuses on communication between product owner, domain experts and developers.</p> <p>The event storming method was introduced and publicized by Alberto Brandolini in \"Introducing event storming book\". This approach is recognized in the Domain Driven Design (DDD) community as a technique for rapid capture of a solution design and improved team understanding of the domain. Domain represents some area of the business that has the analysis focus. This article outlines the method and describes refinements and extensions that are useful in designs for an event-driven architecture. This extended approach adds an insight storming step to identify and capture value adding predictive insights about possible future events. The predictive insights are generated by using data science analysis, data models, artificial intelligence (AI), or machine learning (ML).</p> <p>This article describes in general terms all the steps to run an event storming workshop. The output of an actual workshop done on a sample problem - a world wide container shipment, is further detailed in the container shipment analysis example.</p>"},{"location":"methodology/event-storming/#conducting-the-event-and-insight-storming-workshop","title":"Conducting the event and insight storming workshop","text":"<p>Before conducting an event storming workshop, complete a Design Thinking Workshop in which Personas and Empathy Maps are developed and business pains and goals are defined. The event storming workshop adds more specific design on the events occuring at each step of the process, natural contexts for microservices and predictive insights to guide operation of the system. With this approach, a team that includes business owners and stakeholders can define a Minimal Viable Prototype (MVP) design for the solution.  The resulting design is organized as a collection of loosely coupled microservices linked through an event-driven architecture and one or more event backbones. This style of design can be deployed into multicloud execution environments and allows for scaling and agile deployment.</p> <p>Preparations for the event storming workshop include the following steps:</p> <ul> <li>Get a room big enough to hold at least 6 to 8 persons and with enough wall space on which to stick big paper sheets: you will need a lot of wall space to define the models.</li> <li>Obtain green, orange, blue, and red square sticky notes, black sharpie pens and blue painter's tape.</li> <li>Discourage the use of open laptops during the meeting.</li> <li>Limit the number of chairs so that the team stays focused and connected and conversation flows easily.</li> </ul>"},{"location":"methodology/event-storming/#concepts","title":"Concepts","text":"<p>Many of the concepts addressed during the event storming workshop are defined in the Domain Driven Design approach. The following diagrams present the elements used during the analysis.  The first diagram shows the initial set of concepts that are used in the process.</p> <p></p> <p>Domain events are also named business events.  An event is some action or happening which occurred in the system at a specific time in the past. The first step in the event storming process consists of these actions:</p> <ul> <li>Identifying all relevant events in the domain and specific process being analyzed,</li> <li>Writing a very short description of each event on a \"sticky\" note</li> <li>and placing all the event \"sticky\" notes in sequence on a timeline.</li> </ul> <p>The act of writing event descriptions often results in questions to be resolved later, or discussions about definitions that need to be recorded to ensure that everyone agrees on basic domain concepts.</p> <p></p> <p>A timeline of domain events is the critical output of the first step in the event storming process.  The timeline gives everyone a common understanding of when events take place in relation to each other.  You still need to be able to take this initial level of understanding and move it towards an implementation.  In making that step, you must expand your thinking to encompass the idea of a command, which is the action that kicks off the processing that triggers an event.  As part of understanding the role of the command, you will also want to know who invokes a command (actors) and what information is needed to allow the command to be executed.  This diagram show how those analysis elements are linked together:</p> <p></p> <p>One-View Figure.</p> <ul> <li>Actors consume data by using a user interface and use the UI to interact with the system via commands. Actors could also be replace by articial intelligent agents.</li> <li>Commands are the result of some user decision or policy, and act on relevant data which are part of a Read model in the CQRS pattern.</li> <li>Policies (represented by lilac stickies) are reactive logics that take place after an event occurs, and trigger other commands. Policies always start with the phrase \"whenever...\". They can be a manual step a human follows, such as a documented procedure or guidance, or they may be automated. When applying the Agile Business Rule Development methodology it will be mapped to a Decision within the Decision Model Notation.</li> <li>External systems produce events.</li> <li>Data can be presented to users in a user interface or modified by the system.</li> </ul> <p>Events can be created by commands or  by external systems including IOT devices.  They can be triggerred by the processing of other events or by some period of elapsed time. When an event is repeated or occurs regularly on a schedule, draw a clock or calendar icon in the corner of the sticky note for that event. As the events are identified and sequenced into a time line, you might find multiple independent subsequences that are not directly coupled to each other and that represent different perspectives of the system, but occur in overlapped periods of time. These parallel event streams can be addressed by putting them into separate swimlanes delineated by using horizontal blue painter's tape. As the events are organized into a timeline, possibly with swim lanes, you can identify pivotal events. Pivotal events indicate major changes in the domain and often form the boundary between one phase of the system and another. Pivotal events will typically separate (a bounded context in DDD terms). Pivotal events are identified with vertical blue painters tape (crossing all the swimlanes).</p> <p>An example of a section of a completed event time line with pivotal events and swimlanes is shown below.</p> <p></p>"},{"location":"methodology/event-storming/#conducting-the-workshop","title":"Conducting the workshop","text":"<p>The goal of the workshop is to better understand the business problem to address with a future application. But the approach can also apply to finding solutions to bottlenecks or other issues in existing applications. The workshop helps the team to understand the big picture of the solution by building a timeline of domain events as they occur during the business process life span. During the workshop, avoid documenting processing steps. The event storming method is not trying to specify a particular implementation. Instead, the focus in initial stages of the workshop is on identifying and sequencing the events that occur in the solution. The event timeline is a useful representation of the overall steps, communicating what must happen while remaining open to many possible implementation approaches.</p>"},{"location":"methodology/event-storming/#step-1-domain-events-discovery","title":"Step 1: Domain events discovery","text":"<p>Begin by writing each domain event on an orange sticky note with a few words and a verb in a past tense. Describe What's happened. At first just \"storm\" the events by having each domain expert generate an individual lists of domain events. You might not need to initially place the events on the ordered timeline as they write them.  The events must be worded in a way that is meaningful to the domain experts and business stakeholder. You are explaining what happens in business terms, not what happens inside the implementation of the system.</p> <p></p> <p>You don't need to describe all the events in your domain, but you must cover the process that you are interested in exploring from end to end. Therefore, make sure that you identify the start and end events and place them on the timeline at the beginning and end of the wall covered with paper. Place the other events that you identified between these two endpoints in the closest approximation that the team can agree to a sequential order. Don\u2019t worry about overlaps at this point; overlaps are addressed later.</p>"},{"location":"methodology/event-storming/#step-2-tell-the-story","title":"Step 2: Tell the story","text":"<p>In this step, you retell the story by talking about how to relate events to particular personas. A member of the team (often the facilitator, but others can do this as well)  acts this out by taking on the perspective of a persona in the domain, such as a \"manufacturer\" who wants to ship a widget to a customer, and asking which events follow which other events. Start at the beginning of that persona's interaction and ask \"what happens next?\". Pick up and rearrange the events that the team storms.  If you discover events that are duplicates, take those off the board.  If events are in the wrong order, move them into the right order.</p> <p>When some parts are unclear, add questions or comments by using the red stickies.. Red stickies indicate that the team needs to follow up and clarify issues later. Likewise you want to use this time to document assumptions on the definition stickies. This is also a good time to rephrase events as you proceed through the story. Sometimes you need to rephrase an event description by putting the verbing in past tense, or adjusting the terms that are used to relate clearly to other identified events. In this step you focus on the mainline \"happy\" end-to-end path to avoid getting bogged down in details of exceptions and error handling. Exceptions can be added later</p>"},{"location":"methodology/event-storming/#step-3-find-the-boundaries","title":"Step 3: Find the Boundaries","text":"<p>The next step of this part of the process is to find the boundaries of your system by looking at the events. Two types of boundaries can emerge; the first type of boundary is a time boundary. Often specific key \"pivotal events\" indicate a change from one aspect of a system to another. This can happen at a hand-off from one persona to another, but it can also happen at a change of geographical, legal, or other type of boundary. If you notice that the terms that are used on the event stickies change at these boundaries, you are seeing a \"bounded context\" in Domain Driven Design terms. Highlight pivotal events by putting up blue painter\u2019s tape vertically behind the event.</p> <p>The second type of boundary is a subject boundary. You can detect a subject boundary by looking for the following conditions:</p> <ul> <li>You have multiple simultaneous series of events that only come together at a later time.</li> <li>You see the same terms being used in the event descriptions for a particular series of events.</li> <li>You can \u201cread\u201d a series of events from the point of view of a different persona when you are replaying them.</li> </ul> <p>You can delineate these different sets of simultaneous event streams by applying blue painter\u2019s tape horizontally, dividing the board into different swim lanes.</p> <p>Below is an example of a set of ordered domain events with pivotal events and subject swim lanes indicated.  This example comes from applying event storming to the domain of container shipping process and is discussed in more detail in the container shipment analysis example. When the reefer container is plugged to the Vessel, it starts to emit telemetries, we change context.</p> <p></p>"},{"location":"methodology/event-storming/#step-4-locate-the-commands","title":"Step 4: Locate the Commands","text":"<p>In this step you shift from analysis of the domain to the first stages of system design.  Up until this point, you are simply trying to understand how the events in the domain relate to one another - this is why the participation of domain experts is so critical.  However, to build a system that implements the business process that you are interested in, you have to move on to the question of how these events come into being.</p> <p>Commands are the most common mechanism by which events are created.  The key to finding commands is to ask the question: \"Why did this event occur?\". In this step, the focus of the process moves to the sequence of actions that lead to events. Your goal is to find the causes for which the events record the effects. Expected event trigger types are:</p> <ul> <li>A human operator makes a decision and issues a command</li> <li>Some external system or sensor provides a stimulus</li> <li>An event results from some policy - typically automated processing of a precursor event</li> <li>The completion of some determined period of elapsed time.</li> </ul> <p>The triggering command is identified in a blue (sticky) note. Command may become a microservice operation exposed via API. The human persona issuing the command is identified and shown in a yellow note. Some events may be created by applying business policies. The diagram below illustrates the manufacturer actor using the place a shipment order command to create a shipment order placed event, as well as .</p> <p></p> <p>It is possible to chain events and commands as presented in the \"one view\" figure above in the concepts section.</p>"},{"location":"methodology/event-storming/#step-5-describe-the-data","title":"Step 5: Describe the Data","text":"<p>You can't truly define a command without understanding the data that is needed for the command to execute in order to produce the event. You can identify several types of data during this step.  First, users (personas) need data from the user interface in order to make decisions before executing a command.  That data forms part of the read model in a CQRS implementation. For each command and event pair, you add a data description of the expected attributes and data elements needed to take such a decision. Here is a simple example for a <code>shipment order placed</code> event created from a <code>place a shipment order action</code>.</p> <p></p> <p>Another important part of the process that becomes more fully fleshed out at this step is the description of policies that can trigger the generation of an event from a previous event (or set of events).</p> <p>Assess if the data element is a main business entity, uniquely identified by a key, supported by multiple commands. It  has a life span over the business process. This will lead to develop an entity life cycle analysis.</p> <p>This first level of data definition helps to assess the microservice scope and responsibility as you start to see commonalities emerge from the data used among several related events.  Those concepts become more obvious in the next step.</p>"},{"location":"methodology/event-storming/#step-6-identify-the-aggregates","title":"Step 6: Identify the Aggregates","text":"<p>In DDD, entities and value objects can exist independently, but often, the relations are such that an entity or a value object has no value without its context.  Aggregates provide that context by being those \"roots\" that comprise one or more entities and value objects that are linked together through a lifecycle. The following diagram illustrates a detailed example of aggregates for shipment of a temperature sensitive product overseas.</p> <p></p> <p>In event storming, we may not be able to get this level of detail during the first workshop, but aggregates emerge through the process by grouping events and commands that are related together.  This grouping not only consists of related data (entities and value objects) but also related actions (commands) that are connected by the lifecycle of that aggregate. Aggregates ultimately suggest microservice boundaries.</p> <p>In the container shipment example, you can see that you can group several commands and event pairs (with their associated data) together that are related through the lifecycle of an order for shipping.</p> <p></p>"},{"location":"methodology/event-storming/#step-7-define-bounded-context","title":"Step 7: Define Bounded Context","text":"<p>In this step, you define terms and concepts with a clear meaning valid in a clear boundary and you define the context within which a model applies. (The term definition can change outside of the business unit for which an application is developed). The following items may be addressed:</p> <ul> <li>Which team owns the model?</li> <li>Which part of the model transit between team organization?</li> <li>What are the different code bases foreseen we need to implement?</li> <li>What are the different data schema ? (database or json or xml schemas)</li> </ul> <p>Here is an example of bounded context that will, most likely, lead to a microservice:</p> <p></p> <p>Keep the model strictly consistent within these bounds.</p>"},{"location":"methodology/event-storming/#step-8-looking-forward-with-insight-storming","title":"Step 8: Looking forward with insight storming","text":"<p>In event atorming for Event Driven Architecture (EDA) solutions it is helpful to include an additional method step at this point identifying useful predictive analytics insights.</p> <p>Insights storming extends the basic methodology by looking forward and considering what if you could know in advance that an event is going to occur. How would this change your actions, and what would you do in advance of that event actually happening? You can think of insight storming as extending the analysis to Derived Events.  Rather than being the factual recording of a past event, a derived event is a forward-looking or predictive event, that is, \"this event is probably going to happen at some time in the next n hours\u201d.</p> <p>By using this forward-looking insight combined with the known business data from earlier events,   human actors and event triggering policies can make better decisions about how to react to new events as they occur. Insight storming amounts to asking workshop participants the question: \"What data would be helpful at each event trigger to assist the human user or automated event triggering policy make the best possible decision of how and when to act?\"</p> <p>An important motivation that drives the use of an event-driven architecture is that it simplifies design and realization of highly responsive systems that react immediately and intelligently, that is, in a personalized and context-aware way, and optimally to new events as they occur.  This immediately suggests that predictive analytics and models to generate predictive insights have an important role to play. Predictive analytic insights are effectively probabilistic statements about which future events are likely to occur and what are the likely properties of those events. These probabilistic statements are typicaly generated by using models created by data scientists or using AI or ML. Correlating or joining independently gathered sources of information can also generate important predictive insights or be input to predictive analytic models.</p> <p>Business owners and stakeholders in the event storming workshop can offer good intuitions in several areas:</p> <ul> <li>Which probabilistic insights are likely to lead to improved or optimal decision making and action?<ul> <li>The action could take the form of an immediate response to an event when it occurs.</li> <li>The action could be proactive behavior to avoid an undesirable event.</li> </ul> </li> <li>What combined sources of information are likely to help create a model to predict this insight?</li> </ul> <p>With basic event storming, you look backwards at each event because an event is something that has already happened. When you identify data needed for an actor or policy to decide when and how to issue a command, there is a tendency to restrict consideration to properties of earlier known and captured business events. In insight storming you extend the approach to explicitly look forward and consider what is the probability that a particular event will occur at some future time and what would be its expected property values? How would this change the best action to take when and if this event occurs? Is there action we can take now proactively in advance of an expected undesirable event to prevent it happening or mitigate the consequences?</p> <p>The insight method step amounts to getting workshop participants to identify derived events and the data sources needed for the models that generate them.  Adding an insight storming step using the questions above into the workshop will improve decision making and proactive behavior in the resulting design. Insights can be published into a bus and subscribed to by any decision step guidance.</p> <p>By identifying derived events, you can integrate analytic models and machine learning into the designed solution. Event and derived event feeds can be processed, filtered, joined, aggregated, modeled and scored to create valuable predictive insights.</p> <p>Use the following new notations for the insight step:</p> <ul> <li>Pale blue stickies for derived events.</li> <li>Parallelogram shape to show when events and derived events are combined to enable deeper insight models and predictions. Identify predictive insights as early as possible in the development life cycle. The best opportunity to do this is to add this step to the event storming workshop.</li> </ul> <p>The two diagrams below show the results of the insight storming step for the use case of container shipment analysis.  The first diagram captures insights and associated linkages for each refrigerated container, identifying when automated changes to the thermostat settings can be made, when unit maintenance should be scheduled and when the container contents must be considered spoiled.</p> <p></p> <p>The second diagram captures insights that could trigger recommendations to adjust ship course or speed in response to expected severe weather forcasts for the route ahead or predicted congestion and expected docking and unloading delays at the next port of call.</p> <p></p>"},{"location":"methodology/event-storming/#design-iteration","title":"Design iteration","text":"<p>Attention we are not proposing to apply a waterfall approach, but before starting the deeper implementation with iterations, we want to spend sometime to define in more details what we want to build, how to organize the CI/CD projects and pipeline, select the development, test and product plaform, and define epics, user stories, components, microservices... This iteration can take from few hours to a week, depending on the expected MVP goals.</p> <p>For an event-driven solution a MVP for a single application should not take more than 3 to 4 iterations.</p>"},{"location":"methodology/event-storming/#event-storming-to-user-stories-and-epics","title":"Event storming to user stories and epics","text":"<p>In agile methodology, creating user stories or epics is one of the most important elements in project management. The commands and policies related to events can be easily described as user stories, because commands and decisions are done by actors. The actor could be a system as well. For the data you must model the \"Create, Read, Update, Delete\" operations as user stories, mostly supported by a system actor.</p> <p></p> <p>An event is the result or outcome of a user story. Events can be added as part of the acceptance criteria of the user stories to verify that the event really occurs.</p>"},{"location":"methodology/event-storming/#applying-to-the-container-shipment-use-case","title":"Applying to the container shipment use case","text":"<p>The K Container Shipment use case demonstrates an implementation solution to validate the event-driven architecture. The container shipment analysis example, shows event storming and design thinking main artifacts, including artifacts for the monitoring of refrigerated containers.</p>"},{"location":"methodology/event-storming/#some-practice-notes","title":"Some practice notes","text":"<ul> <li>you can apply event storming at different level: for example at the beginning of a project to understand the high level process at stake. with a big group of people, you will stay at the high level. But it can be used to model a lower level microservice, to assess event consumed and produced.</li> </ul>"},{"location":"methodology/event-storming/#further-readings","title":"Further Readings","text":"<ul> <li>Introduction to event storming from Alberto Brandolini </li> <li>Event Storming Guide</li> <li>Wikipedia Domain Driven Design</li> <li>Eric Evans: \"Domain Driven Design - Tacking complexity in the heart of software\"</li> <li>Domain drive design with event storming introduction video</li> <li>Patterns related to Domain Driven Design by Martin Fowler</li> <li>Applying DDD and event storming for event-driven microservice implementation from our own work</li> </ul>"},{"location":"methodology/governance/","title":"Event driven solution governance","text":"<p>Digital businesses are real-time, responsive, scalable and flexible while focussing on delivering outstanding customer experience. API and Microservices focus has achieved a lot in enabling this transformation, supporting real-time interactions and increasing levels of agility through decoupling applications. But digital business requires more, it needs to become more timesensitive, contextual and event-driven in nature. Events is how the modern digital business operates, and eventdriven architecture allows IT to align to this.</p> <p>Governance involves a lot of different view points, so in this section we will cover what we think are important to govern the development, deployment and maintenance of event-driven solution. The first important question to address is do I need to use event-driven solution? This is what the fit for purpose is about.</p> <p>Most EDA adoption starts from the adoption of some new programming model based on the reactive manifesto, or by selecting a modern middleware to support loosely coupled microservice, like Kafka, all this in the context of one business application.  From the first successful project, it is important to consider adopting a more strategic approach:</p> <ul> <li>to select technologies</li> <li>to apply common architecture and design patterns</li> <li>to adopt data models governance and control data lineage</li> <li>to apply common methodology to quickly onboard new business initiative, and be able to deploy new capability on top of the EDA in question of days or weeks. </li> </ul>"},{"location":"methodology/governance/#fit-for-purpose","title":"Fit for Purpose","text":"<p>In EDA context the fit for purpose needs to help responding to the high level question on when to use event-driven solution for a new application, but also to address when to use modern pub/sub event backbone versus queuing products, what data stream processing to use, what are the different use cases and benefits.</p> <p>We have developed the content in a separate note as the subject can become long to describe.</p>"},{"location":"methodology/governance/#architecture-patterns","title":"Architecture patterns","text":"<p>We have already describe in this section as set of event-driven architecture patterns that can be leveraged while establishing EDA practices which includes how to integrate with data in topic for doing feature engineering. An example of Jupiter notebook can also be reused for this as described in this data collection article.</p> <p>Legacy integration and coexistence between legacy applications or mainframe transactional application and microservices is presented in this section.</p> <p>The modern data pipeline is also an important architecture pattern where data ingestion layer is the same as the microservice integration middleware and provide buffering capability as well as stateful data stream processing.</p> <p>From an implementation point of view the following design patterns are often part of the EDA adoption:</p> <ul> <li>Event sourcing</li> <li>Command Query Responsibility Segregation</li> <li>Saga pattern</li> <li>Transactional outbox</li> <li>Event reprocessing with dead letter</li> </ul> <p>You can also review the other microservice patterns in this note.</p>"},{"location":"methodology/governance/#getting-developers-on-board","title":"Getting developers on board","text":""},{"location":"methodology/governance/#data-lineage","title":"Data lineage","text":"<p>Data governance is a well established practices in most companies. Solutions need to address the following needs:</p> <ul> <li>Which sources/producers are feeding data?</li> <li>What is the data schema?</li> <li>How to classify data</li> <li>Which process reads the data and how is it transformed?</li> <li>When was the data last updated?</li> <li>Which apps/ users access private data?</li> </ul> <p>In the context of event-driven architecture, one focus will be to ensure re-use of event topics,  control the schema definition, have a consistent and governed ways to manage topic as service,  domain and event data model definition, access control, encryption and traceability.  As this subject is very important, we have started to address it in a separate 'data lineage' note.</p>"},{"location":"methodology/governance/#deployment-strategy","title":"Deployment Strategy","text":"<p>Hybrid cloud, Containerized</p> <p>security and access control Strimzi - cruise control active - active topic as self service</p>"},{"location":"methodology/governance/#ongoing-maintenance","title":"Ongoing Maintenance","text":"<p>Procedures performed regularly to keep a system healthy Cluster rebalancing  Add new broker - partition reassignment</p> <p>Product migration</p>"},{"location":"methodology/governance/#operational-monitoring","title":"Operational Monitoring","text":"<p>assess partition in heavy load assess broker in heavy load assess partition leadership assignment</p> <p>Problem identification System health confirmation</p>"},{"location":"methodology/governance/#error-handling","title":"Error Handling","text":"<p>Procedures, tools and tactics to resolve problems when they arise</p>"},{"location":"news/","title":"What's new","text":""},{"location":"news/#09062022","title":"09/06/2022","text":"<ul> <li>update to mkdocs, update to journeys, reference architecture.</li> </ul>"},{"location":"news/#06012022","title":"06/01/2022","text":"<ul> <li>Added Tech Academy site for enablement: EDA Tech Academy</li> </ul>"},{"location":"news/#04042022","title":"04/04/2022","text":"<ul> <li>Update to fit for purpose, adding business and technical use cases.</li> </ul>"},{"location":"news/#01212022","title":"01/21/2022","text":"<ul> <li>Add Flow Architecture summary</li> <li>Update to reactive messaging section</li> <li>Update to Avro and schema registry technical note.</li> </ul>"},{"location":"news/#12012021","title":"12/01/2021","text":"<ul> <li>Update to the reference architecture diagram to add App Connect Enterprise as a source to messaging services</li> <li>Update to Flink note on exactly once delivery</li> </ul>"},{"location":"news/#11042021","title":"11/04/2021","text":"<ul> <li>Add kubernetes deployment diagram for messaging as a service</li> <li>Update MQ sink connector tutorial for lab 1 and lab 2.</li> <li>Update to Kafka Connect framework content</li> </ul>"},{"location":"news/#10202021","title":"10/20/2021","text":"<ul> <li>Integrate SOA and EDA co-existence section</li> </ul>"},{"location":"news/#10122021","title":"10/12/2021","text":"<ul> <li>Learning Journey 101 content to get you started on EDA.</li> <li>Step by step tutorial to deploy Event Streams 2021.0.3 release on OpenShift</li> <li>Generic business use cases for EDA adoption</li> </ul>"},{"location":"news/#10022021","title":"10/02/2021","text":"<ul> <li>Update to the reference architecture diagram to illustrate messaging as a service as the backbone for EDA.</li> <li>How Automation fits on top of EDA? some explanations are here.</li> <li>Split producer into basic and advanced concepts</li> <li>Split consumer content into basics and advanced concepts</li> </ul>"},{"location":"news/#09202021","title":"09/20/2021","text":"<ul> <li>Apache Flink introduction and need to know article</li> </ul>"},{"location":"news/#07302021","title":"07/30/2021","text":"<ul> <li>Devising data models: when you need to think about data views and data representation depending of the needs and semantic.</li> </ul>"},{"location":"patterns/api-mgt/","title":"API Management - AsyncAPI","text":"<p>This page summarizes some of the best practices for introducing API Management in development practices and architecture patterns within an enterprise setting.</p>"},{"location":"patterns/api-mgt/#moving-from-a-pure-api-gateway-to-an-api-management-system","title":"Moving from a Pure API Gateway to an API Management system","text":"<p>An API Gateway helps provide security, control, integration, and optimized access to a full range of mobile, web, application programming interface (API), service-oriented architecture (SOA), B2B and cloud workloads. A gateway is used in the following patterns:</p> <ul> <li>As a Security Gateway, placed between the consumer facing firewall and the system of records facing firewall (DMZ). It is used for both policy enforcement and consistent security policies across business channels.</li> <li>As an API Gateway, both as an internal and external gateway, with centralized service governance and policy enforcement, and with traffic monitoring.</li> <li>To provide connectivity (HTTP) and mediation (XML, JSON) services in the internal network, close to the system of record. </li> </ul> <p>API Management gives enterprises greater flexibility when reusing the functionality of API integrations and helps save time and money without trading off security. An API Management system supports a broader scope of features for API lifecycle management, including: </p> <ul> <li>API lifecycle management to activate, retire, or stage an API product.</li> <li>API governance with security, access, and versioning.</li> <li>Analytics, dashboards, and third party data offload for usage analysis.</li> <li>API socialization based on a portal for the developer community that allows self-service and discovery.</li> <li>An API developer toolkit to facilitate the creation and testing of APIs.</li> </ul>"},{"location":"patterns/api-mgt/#classical-pain-points","title":"Classical Pain Points","text":"<p>Some of the familiar pain points that indicate the need for a broader API Management product include:</p> <ul> <li>Current API details like endpoints, request/response message format, error conditions, test messages, and SLAs are not easily available or not well documented.</li> <li>Difficult to tell which subscribers are really using the API and how often, without building a custom solution.</li> <li>Difficult to differentiate between business-critical subscribers versus low value subscribers.</li> <li>Managing different lines of business and organizations is complex.</li> <li>No dynamic scaling built into the solution, which often means making hardware investments for maximum load or worst availability scenarios.</li> <li>Difficult to evolve the APIs, such as moving from SOAP based web services to RESTful services to GraphQL</li> <li>No support for AsyncAPI to automate and formalize the documentation or code generation of any event-driven APIs.</li> <li>The need to ensure consistent security rules</li> <li>Integrating CI/CD pipelines with the API lifecycle</li> </ul>"},{"location":"patterns/api-mgt/#enterprise-apis-across-boundaries","title":"Enterprise APIs across boundaries","text":"<p>If you consider a typical API Management product, it includes a set of components as presented in the figure below that could be deployed on an on-premise Kubernetes-based platform or in several Cloud provider regions. APIs served by different applications or microservices can be deployed in multiple regions but still be managed by one central API Management server.</p> <p></p> <p>Here is how it works:</p> <ol> <li>An API developer signs on to the API Management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the sync API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to the API Management system. He can also create Async APIs from a messaging system by binding channels to topics or queues and define the message payload.</li> <li>The API Product Manager or API owner signs on to the API Management cloud services account and accesses the API Management component. She includes the sync API endpoint to existing API products, and plans and specifies access controls. She publishes the API to the developer portal for external discovery by application developers.</li> <li>An application developer accesses the developer portal and uses search to discover any available APIs.</li> <li>The application developer uses the API in an application and deploys that application to the device</li> <li>The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway also validates access policies with API Management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the backend. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API Management system.</li> <li>API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics.</li> </ol> <p>Deployment across cloud providers could look like the diagram below, using API Connect in Cloud Pak for Integration:</p> <p></p> <p>On the left side (green boxes), the consumers of the API register to a Developer portal to get the metadata about the API they want to consume.  The Developer portal also allows them to test the APIs to better understand how they work. The consumers register their applications as API subscribers. These applications can run on the cloud or on-premise. </p> <p>The API Gateway services colocated with the target application services to reduce latency. These would be deployed as StatefulSet on an OpenShift cluster, which means as a set of pods with consistent identities. Identities are defined as:</p> <ul> <li>Network: A single stable DNS and hostname.</li> <li>Storage: As many VolumeClaims as requested.</li> </ul> <p>The StatefulSet guarantees that a given network identity will always map to the same storage identity. </p> <p>An API Gateway acts as a reverse proxy, and, in this case, exposes the <code>Inventory APIs</code>, enforcing user authentication and security policies,  and handling traffic monitoring, rate limiting, and statistics. The API Gateway can also perform  transformations and aggregate various services to fulfill a request. </p> <p>The Developer Portals can be separated, or centralized depending on API characteristics exposed from different  clouds (e.g. different Developer Portals for internal and external APIs). In the example above, the portal  is deployed onto the Cloud Provider as a container inside an OpenShift cluster. The Analytic service is   also a StatefulSet and gets metrics from the gateway.</p> <p>The application microservices are accessing remote services and this traffic can also go to the API gateway.  Those services will also integrate with existing backend services running on-premise, whether they are deployed  or not on OpenShift. </p> <p>In the diagram above, the management service for the API Management product is depicted as running on-premise  to illustrate that it is a central deployment to manage multiple gateways.</p> <p>Gateway services, Developer Portal services, and Analytics services are scoped to a single region, unlike the  Management System, which can communicate across availability zones.</p> <p>The different API Management services run on OpenShift which can help ensure high availability of each of  the components. </p>"},{"location":"patterns/api-mgt/#open-api","title":"Open API","text":"<p>Within the API Management system, the OpenAPI document can be created top-down using a Swagger-based UI or bottom up using Annotation in the Java JAXRS resource classes.  Either way, the API can be uploaded to the API Management product.</p> <p>The important parts are to define the operations exposed and the request / response structure of the data model. </p>"},{"location":"patterns/api-mgt/#support-for-async-api","title":"Support for Async API","text":"<p>Cloud Pak for Integration (CP4I) 2021.1, which includes APIConnect V10, also provides some support for the AsyncAPI specification. </p> <p>AsyncAPI is an open source initiative that focuses on making Event-Driven Architectures (EDAs) as easy to work with as REST APIs.</p> <p>The AsyncAPI specification (currently at 2.0.0) establishes standards for events and EDAs, covering everything \"from documentation to code generation,  and from discovery to event management\" asyncapi.com/docs. </p> <p>The goal is to enable the creation of better tooling in the message-driven space, better governance of asynchronous APIs,  and standardization in documentation for asynchronous APIs. In short, Async API is to Message-driven architectures  what OpenAPI is to REST APIs. </p> <p>While OpenAPI is the recommended practice for RESTful APIs, adopting AsyncAPI is the recommended practice  for event-driven APIs.</p>"},{"location":"patterns/api-mgt/#asyncapi-documents","title":"AsyncAPI Documents","text":"<p>An AsyncAPI document is a file in either YAML or JSON format that defines and annotates the different components of an event-driven API.  For example, AsyncAPI can formally describe how to connect to a Kafka cluster, the details of the Kafka topics (channels in AsyncAPI),  and the type of data in messages. AsyncAPI includes both formal schema definitions and space for free-text descriptions See this blog.</p> <p>Here is what it looks like: </p> <p></p> <p>You may have noticed the similarities with OpenAPI. AsyncAPI was initially an adaptation of OpenAPI, which does not include support for the Message Queuing Telemetry Transport (MQTT) and the Advanced Message Queuing Protocol (AMQP). </p> <p>The creator of the AsyncAPI specification, Fran M\u00e9ndez, describes what he did at first with just OpenAPI to make up for the lack of MQTT and AMQP support: \"paths were AMQP topics, GET was SUBSCRIBE, and POST was PUBLISH--and ugly hack at best...\". This forced him to write additonal code to support the necessary EDA-based documentation and code generation.</p> <p>Many companies use OpenAPI, but in real-world situations, systems need formalized documentation and code generation support for both REST APIs and events. </p> <p>Here are the structural differences (and similarities) between OpenAPI and AsyncAPI:</p> <p></p> <p>Source: https://www.asyncapi.com/docs/getting-started/coming-from-openapi</p> <p>Note a few things:</p> <ul> <li>AsyncAPI is compatible with OpenAPI schemas, which is quite useful since the information flowing in the events is very similar to the one the REST APIs have to handle in requests and responses.</li> <li>The message payload in AsyncAPI does not have to be an AsyncAPI/OpenAPI schema; it can be any value such as an Apache Avro schema, which is considered to be one of the better choices for stream data, where data is modeled as streams (see the section titled Why Use Avro for Kafka below). </li> <li>The AsyncAPI server object is almost identical to its OpenAPI counterpart with the exception that scheme has been renamed to protocol and AsyncAPI introduces a new property called protocolVersion.</li> <li>AsyncAPI channel parameters are the equivalent of OpenAPI path parameters, except that AsyncAPI does not have the notion of query and cookie, and header parameters can be defined in the message object. </li> </ul>"},{"location":"patterns/api-mgt/#describing-kafka-with-asyncapi","title":"Describing Kafka with AsyncAPI","text":"<p>It is also important to understand how to use AsyncAPI from the perspective of a Kafka user. The following section summarizes what is described in more detail in this article written by Dale Lane.</p> <p>First, there are some minor differences in terminology between Kafka and AsyncAPI that you should note: </p>"},{"location":"patterns/api-mgt/#comparing-kafka-and-asyncapi-terminology","title":"Comparing Kafka and AsyncAPI Terminology","text":""},{"location":"patterns/api-mgt/#the-asyncapi-document","title":"The AsyncAPI Document","text":"<p>Considering the structure in the diagram above, let's look at some of the parts of an AsyncAPI document:</p>"},{"location":"patterns/api-mgt/#info","title":"Info","text":"<p>The Info section has three parts which represent the minimum required information about the application: title, version, and description (optional), used as follows:</p> <pre><code>asyncapi: 2.0.0\n...\ninfo:\n  title: Account Service\n  version: 1.0.0\n  description: This service is in charge of processing user signups\n</code></pre> <p>In the description field you can use markdown language for rich-text formatting.</p>"},{"location":"patterns/api-mgt/#id","title":"Id","text":"<p>The Id is the application identifier. It can be a URN (recommended) or URL. The important thing is that it must be a unique ID for your document. Here is an example:</p> <pre><code>asyncapi: '2.0.0'\n...\nid: 'urn:uk:co:usera:apimgmtdemo:inventoryevents'\n...\n</code></pre>"},{"location":"patterns/api-mgt/#servers","title":"Servers","text":"<p>The servers section allows you to add and define which servers client applications can connect to,  for sending and receiving messages (for example, this could be a list of server objects, each uniquely identifying a Kafka broker).</p> <p>Here is an example, where you have three Kafka brokers in the same cluster:</p> <pre><code>servers:\n  broker1:\n    url: andy-broker-0:9092\n    protocol: kafka\n    protocolVersion: '1.0.0'\n    description: This is the first broker\n  broker2:\n    url: andy-broker-1:9092\n    protocol: kafka\n    protocolVersion: '1.0.0'\n    description: This is the second broker\n  broker3:\n    url: andy-broker-2:9092\n    protocol: kafka\n    protocolVersion: '1.0.0'\n    description: This is the third broker\n</code></pre> <p>The example above uses the <code>kafka</code> protocol for the different brokers, which is a popular protocol for streaming solutions, but the protocol can be any.  For example, the most common protocols include: <code>mqtt</code> which is widely adopted by the Internet of Things and mobile apps, <code>amqp</code>, which is popular  for its reliable queueing, <code>ws</code> for WebSockets, frequently used in browsers, and <code>http</code>, which is used in HTTP streaming APIs.</p>"},{"location":"patterns/api-mgt/#difference-between-the-amqp-and-mqtt-protocols","title":"Difference Between the AMQP and MQTT Protocols","text":"<ul> <li> <p>AMQP was mainly popularized by RabbitMQ. It provides reliable queuing, topic-based publish-and-subscribe messaging, flexible routing, transactions, and security.  The main reasons to use AMQP are reliability and interoperability. AMQP exchanges route messages directly in fanout form, by topic, and also based on headers.</p> </li> <li> <p>MQTT The design principles and aims of MQTT are much more simple and focused than those of AMQP. it provides publish-and-subscribe messaging (no queues, in spite of the name) and was specifically designed for resource-constrained devices and low bandwidth,  high latency networks such as dial up lines and satellite links. Basically, it can be used effectively in embedded systems.</p> </li> </ul> <p>When using a broker-centric architecture such as Kafka or MQ, you normally specify the URL of the broker.  For more classic client-server models, such as REST APIs, your server should be the URL of the server.</p> <p>One limitation in AsyncAPI documents is that you cannot include multiple Kafka clusters (such as a production cluster and clusters for dev, test, and staging environments)  in a single document.</p> <p>One workaround is to list all brokers for all clusters in the same document, and then rely on the description or extension  fields to explain which ones are in which cluster. This is, however, not recommended because it could  interfere with code  generators or other parts of the AsyncAPI ecosystem which may consider them as all being  members of one large cluster.</p> <p>So, as a best practice, avoid this workaround and stick to one cluster per AsyncAPI document.</p> <p>NOTE: As with OpenAPI, you can add additional attributes to the spec using the x- prefix, which identifies an entry  as your own extension to the AsyncAPI specs.</p>"},{"location":"patterns/api-mgt/#security","title":"Security","text":"<p>If the Kafka cluster doesn\u2019t have auth enabled, the protocol used should be <code>kafka</code>. Otherwise,  if client applications are required to provide credentials, the protocol should be <code>kafka-secure</code>. </p> <p>To identify the type of credentials, add a security section to the server object. The value you put there  is the name of a securityScheme object you define in the components section.</p> <p>The types of security schemes that you can specify aren\u2019t Kafka-specific. Choose the value that describes  your type of approach to security.</p> <p>For example, if you\u2019re using SASL/SCRAM, which is a username/password-based approach to auth, you could describe  this as <code>userPassword</code>. Here is an example:</p> <pre><code>asyncapi: 2.0.0\n...\nservers:\n  broker1:\n    url: localhost:9092\n    description: Production server\n    protocol: kafka-secure\n    protocolVersion: '1.0.0'\n    security:\n    - saslScramCreds: []\n...\ncomponents:\n  securitySchemes:\n    saslScramCreds:\n      type: userPassword\n      description: Info about how/where to get credentials\n      x-mykafka-sasl-mechanism: 'SCRAM-SHA-256'\n</code></pre> <p>The description field allows you to explain the security options that Kafka clients need to use.  You could also use an extension (with the -x prefix).</p>"},{"location":"patterns/api-mgt/#channels","title":"Channels","text":"<p>All brokers support communication through multiple channels (known as topics, event types, routing keys, event names or other terms depending on the system). Channels are assigned a name or identifier.</p> <p>The channels section of the specification stores all of the mediums where messages flow through.  Here is a simple example:</p> <pre><code>channels:\n  hello:\n    publish:\n      message:\n        payload:\n          type: string\n          pattern: '^hello .+$'\n</code></pre> <p>In this example, you only have one channel called hello. An app would subscribe to this channel  to receive hello {name} messages. Notice that the payload object defines how the message must be structured.  In this example, the message must be of type string and match the regular expression <code>'^hello .+$'</code> in the  format hello {name} string.</p> <p>Each topic (or channel) identifies the operations that you want to describe in the spec.  Here is another example:</p> <pre><code>asyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    description: This is my Kafka topic\n    subscribe:\n      operationId: someUniqueId\n      summary: Interesting messages\n      description: You can get really interesting messages from this topic\n      tags:\n      - name: awesome\n      - name: interesting\n      ...\n</code></pre> <p>For each operation, you can provide a unique id, a short one-line text summary, and a more detailed description  in plain text or markdown formatting.</p>"},{"location":"patterns/api-mgt/#bindings","title":"Bindings","text":"<p>AsyncAPI puts protocol-specific values in sections called bindings.</p> <p>The bindings sections allows you to specify the values that Kafka clients should use to perform the operation.  The values you can describe here include the consumer group id and the client id.</p> <p>If there are expectations about the format of these values, then you can describe those by using  regular expressions: </p> <pre><code>asyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    description: This is my Kafka topic\n    subscribe:\n      ...\n      bindings:\n        kafka:\n          groupId: \n            type: string\n            pattern: '^[A-Z]{10}[0-5]$'\n          clientId:\n            type: string\n            pattern: '^[a-z]{22}$'\n          bindingVersion: '0.1.0'\n</code></pre> <p>You can instead specify a discrete set of values, in the form of enumerations:</p> <pre><code>asyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    description: This is my Kafka topic\n    subscribe:\n      ...\n      bindings:\n        kafka:\n          groupId: \n            type: string\n            enum:\n            - validone\n            - validtwo\n          clientId:\n            type: string\n            enum:\n            - validoption\n          bindingVersion: '0.1.0'\n</code></pre>"},{"location":"patterns/api-mgt/#messages","title":"Messages","text":"<p>A message is how information is exchanged via a channel between servers and applications. According to the AsyncAPI specifications, a message must contain a payload and may also contain headers. The headers may be subdivided   into protocol-defined headers and header properties defined by the application which can act as supporting  metadata. The payload contains the data, defined by the application, which must be serialized into a supported format (JSON, XML, Avro, binary, etc.). Because a message is a generic mechanism, it can support multiple interaction patterns such as event, command, request, or response.</p> <p>As with all the other levels of the spec, you can provide background and narrative in a description field for the message:</p> <pre><code>asyncapi: '2.0.0'\n...\nchannels:\n  my.topic.name:\n    ...\n    subscribe:\n      ...\n      message:\n        description: Description of a single message\n</code></pre>"},{"location":"patterns/api-mgt/#summary","title":"Summary","text":"<p>In short, the following diagram summarizes the sections described above:</p> <p></p> <p>For more information, the official AsyncAPI specifications can be found here.</p>"},{"location":"patterns/api-mgt/#why-use-avro-for-kafka","title":"Why Use Avro for Kafka?","text":"<p>Apache Avro is \"an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks\" (https://www.confluent.io/blog/avro-kafka-data/). Avro is a great fit for stream data because it has the following features:</p> <ul> <li>Direct mapping to and from JSON, but typically much faster than JSON, with much smaller encodings</li> <li>Compact format, making it more efficient for high-volume usage</li> <li>Bindings for a wide variety of programming languages</li> <li>A rich, extensible schema language defined in pure JSON</li> </ul>"},{"location":"patterns/api-mgt/#a-simple-api-management-demo","title":"A simple API Management Demo","text":"<p>Besides a new, event-driven approach to its API model, ACME Inc needs a way to securely provide self-service access to different versions of its APIs,  to enable their developers to discover and easily use these APIs, and to be able to redirect API calls based on several criteria.</p> <p>IBM API Connect (APIC) is a complete and scalable API Management platform that allows them to do these things, in addition to exposing, managing, and monetizing  APIs across clouds. API Connect is also available with other capabilities as an IBM Cloud Pak\u00ae solution.</p> <p>The API Management demo demonstrates three main areas of interest: </p> <ul> <li>Version Control</li> <li>API Documentation &amp; Discovery</li> <li>Redirecting to different APIs based on certain criteria</li> </ul>"},{"location":"patterns/api-mgt/#types-of-apis","title":"Types of APIs","text":"<p>An API is a set of functions that provide some business or technical capability and can be called by applications by using a defined protocol.  Applications are typically mobile or web applications, and they use the HTTP protocol. An API definition is composed of paths, and can be one  of the following types:</p>"},{"location":"patterns/api-mgt/#rest-api","title":"REST API","text":"<p>A REST API is a defined set of interactions that uses the HTTP protocol, typically by using JSON or XML as the data format that is exchanged.  For example, a data request might use an HTTP GET method, and a data record might use an HTTP POST method. The choice of data format depends  on the type of application that is calling the API. JSON is commonly used for web pages or mobile applications that present a user interface (by using JavaScript or HTML), whereas XML has been traditionally used for machine-to-machine scenarios, although that is changing.</p> <p>In IBM API Connect, you can create REST APIs by using user interface functions to create models and data sources (top-down).  These are then used by your REST API definition and exposed to your users. API Connect also supports a bottom-up approach where  you can import existing APIs into the system and benefit from the API Management capabilities of the product.</p> <p>Alternatively, you can expose and secure your existing APIs by using a Proxy or Invoke policy.</p> <p>In either case, you can configure your API definition either by using the API Manager, or by writing an OpenAPI  definition file and publishing it using either API Manager or the command line interface.</p>"},{"location":"patterns/api-mgt/#soap-api","title":"SOAP API","text":"<p>API Connect also allows you to create SOAP API definitions that are based on an existing Web Services Description Language (WSDL) file.  You can use this facility to benefit from the capabilities that are provided by API Connect, which include analytics and mapping between variables.  You can also expose the API by using the Developer Portal for any existing SOAP services in your organization, including any SOAP services that are part of a service-oriented architecture (SOA) or Enterprise Service Bus (ESB) infrastructure.</p> <p>You can create SOAP API definitions through either the command line interface, or through the API Manager UI.</p>"},{"location":"patterns/api-mgt/#graphql","title":"GraphQL","text":"<p>GraphQL is a query language for APIs that gives an application client greater control over what data it retrieves in an API request when compared with a REST API request. IBM\u00ae API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API. The Developer Portal also supports testing GraphQL APIs. See this dedicated section.</p>"},{"location":"patterns/api-mgt/#asyncapi","title":"AsyncAPI","text":"<p>We already address in detail in previous sections.</p>"},{"location":"patterns/api-mgt/#api-manager","title":"API Manager","text":"<p>You can manage your APIs by using API Connect's API Manager UI:</p> <p></p> <p>The API Manager UI allows you to manage private internal APIs as well as public external APIs. API Manager is an on-premises offering that provides the capabilities required to externalize and manage your services as REST or SOAP APIs.</p>"},{"location":"patterns/api-mgt/#developer-portal","title":"Developer Portal","text":"<p>The Developer Portal is a convenient place to share APIs with application developers. After a Developer Portal has been enabled through the API Manager, and one or more API Products have been published,  application developers can browse and use the APIs from the Developer Portal dashboard, as shown below:</p> <p></p> <p>The Developer Portal can be used as is when it is first enabled, or it can be customized to fit the corporate branding and design requirements of a particular organization.  You can configure the Developer Portal for test and development purposes, or for internal use only.</p>"},{"location":"patterns/api-mgt/#create-capability","title":"Create Capability","text":"<p>Developer can leverage API Connect's Create capability, to build APIs (top-down) with a built-in Swagger (OpenAPI) editor or using a simple guided model-driven approach.  APIC allows developers to create a new API from scratch or an API based on the schema of an existing data source, such as a database.</p>"},{"location":"patterns/api-mgt/#creating-a-rest-proxy-api-from-an-existing-target-service","title":"Creating a REST proxy API from an existing target service","text":"<p>As mentioned earlier, it is also possible to create a REST proxy API from an existing target service (bottom-up) that you import into the API Connect system. Essentially, if you have an existing REST service that you want to expose through an IBM API Connect API definition, you can create a proxy API and specify the target endpoint by using the API Manager.</p>"},{"location":"patterns/api-mgt/#explore-capability","title":"Explore Capability","text":"<p>Developers can use API Connect's Explore capability to quickly examine their new APIs and try their operations.</p> <p>Developers can add their APIs to the API Connect server and have the choice of running them on the cloud or on-premise, as mentioned above.  This can be done through API Connect's UI or with the provided CLI. </p> <p>Developers and Administrators can then connect to the API Connect Server and see their running APIs, including those that were created as well as  those that were added based on existing data sources. </p>"},{"location":"patterns/api-mgt/#subscribing-to-an-api-in-the-developer-portal","title":"Subscribing to an API in the Developer Portal","text":"<p>To subscribe to an API, from the Developer Portal, the develper clicks <code>API Products</code> to find and subscribe to any available APIs:</p> <p></p> <p>In the example above, an API called FindBranch Version 2.0, which is contained in the FindBranch Auto Products product is available. Clicking <code>Subscribe</code> enables the developer to use the API:</p> <p></p> <p>Under the Application heading, the developer can click <code>Select App</code> for the new application:</p> <p></p> <p>From the next screen below, the developer can click Next:</p> <p></p> <p>Doing this shows the screen below, which confirms to the developer that his or her applicatin is now subscribed to the selected API under the selected plan. Pressing <code>Done</code> in the next screen completes the subscription.</p> <p></p>"},{"location":"patterns/api-mgt/#testing-an-api-in-the-developer-portal","title":"Testing An API in the Developer Portal","text":"<p>To test an API, the developer clicks <code>API Products</code> in the Developer Portal dashboard to show all available products:</p> <p></p> <p>Clicking a product, such as FindBranch auto product, for example, and then clicking the FindBranch API from the provided list shows the available API operations. </p> <p></p> <p>The developer can click <code>GET/details</code>, for instance, to see the details of the GET operation: </p> <p></p> <p>Clicking the <code>Try it</code> tab and pressing <code>Send</code> allows the developer to test the API to better understand how it works:</p> <p></p> <p>As you can see above, this shows the request/response from invoking the API. API Connect shows a returned response of 200 OK and the message body, indicating that the REST API operation call was successful.</p>"},{"location":"patterns/api-mgt/#api-product-managers","title":"API Product Managers","text":"<p>Administrators or API Product Managers can make changes such as adding corporate security polices or transformations before publishing the APIs. They can also control the visibility and define the rate limit for the APIs. </p> <p>Once the APIs are ready, API Product Managers can expose them for developer consumption and self-service in the Developer Portal. Developers who have signed up can discover and use any APIs which were exposed. </p> <p>Because the APIs were documented while they were being created using OpenAPI notation, developers can not only view the APIs but also try them out, as demonstrated in the demo. API Connect provides source code examples  in several different languages that show how to use the API operations. Languages supported include Curl, Ruby, Python, Java, Go, and Node.js.</p>"},{"location":"patterns/api-mgt/#testing","title":"Testing","text":"<p>Besides the small individual tests that you can run from the Developer Portal to understand how an API works, as explained above, it is possible to use API Connect Test and Monitor to effortlessly generate  and schedule API test assertions. This browser based tool also provides dashboards for monitoring API test assertions and for receiving alerts on the health of your APIs.</p> <p>You can use an HTTP Client within API Connect Test and Monitor to generate a request to an API and get the expected results:</p> <p></p> <p>You enter any required parameters and authorization token and press <code>Send</code> to send the request. </p> <p>API Connect Test and Monitor then shows you the response payload as a formatted JSON object:</p> <p></p> <p>To generate a test, you then click <code>Generate Test</code>, and in the dialog that appears name the test and add it to a project.  When you click the checkmark in the upper right corner of the dialog, API Connect Test and Monitor automatically generates test assertions based on the response payload.</p> <p>You can add, delete, or modify these assertions, or even add new ones from a broad selection:</p> <p></p> <p>You can also reorder the assertions by dragging and dropping them:</p> <p></p> <p>Although coding is not required, if you wish, you change the underlying code by clicking <code>CODE</code> to enter the code view:</p> <p></p> <p>Once you have your test arranged the way you want it, you can run it to generate a test report, which highlights successes and failures:</p> <p></p>"},{"location":"patterns/api-mgt/#test-scheduling","title":"Test Scheduling","text":"<p>IBM API Connect Test and Monitor also lets you automate test scheduling at regular intervals. To do this, you first save any test you may have open and exit out of the composer. Then publish your test to prepare it for scheduling:</p> <p></p> <p>Once your test is published, you can click <code>Schedule</code> to schedule regular test intervals. Test and Monitor allows you to tweak the test schedule as necessary. You can create a new run, name it, and choose the times you wish to run your tests:</p> <p></p> <p>Clicking <code>Save Run</code> schedules the tests. </p> <p>Finally, you can access the dashboard to monitor the health of your API:</p> <p></p>"},{"location":"patterns/api-mgt/#graphql-apis","title":"GraphQL APIs","text":"<p>IBM\u00ae API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API.  The Developer Portal also supports testing GraphQL APIs. </p>"},{"location":"patterns/api-mgt/#advantages-of-graphql-over-rest-apis","title":"Advantages of GraphQL over REST APIs","text":"<p>GraphQL provides the following particular advantages over REST APIs:</p> <ul> <li>The application client can request only the data that it needs. For example, when retrieving bank account records, request only the account number and current balance for each account, but not the customer name and address details.  With a REST API request, either the backend REST service must provide separate endpoints for different data subsets, or the application client must retrieve the complete records and then discard the unwanted data.</li> <li>The application client can retrieve multiple related resources in a single request. For example, a customer's bank account record might include an array that references other finance products that the customer holds.  If an application client wants to retrieve the bank account details for a specific customer, and details of the other finance products for that customer, then with a REST API the client would first retrieve the bank account details,  then make separate requests for each of the other products. A GraphQL API can be designed to allow the client to retrieve all this information in a single request.</li> <li>However, this flexibility presents rate limiting challenges, because two seemingly very similar requests might return vastly different amounts of data, and what might have required multiple REST API requests, each counting towards the rate limit, might require only a single GraphQL API request. It is important therefore that rate limiting controls are imposed that reflect the amount of data that is retrieved. API Connect extends the GraphQL standard by providing, in a GraphQL API definition, the ability to configure a range of settings that are used to calculate the complexity of a GraphQL request and an associated cost that counts towards the rate limit.</li> </ul>"},{"location":"patterns/api-mgt/#creating-a-graphql-proxy-api","title":"Creating a GraphQL proxy API","text":"<p>The  demo also showed how API Connect supports GraphQL. To create a GraphQL proxy API from an existing GraphQL server, click <code>Add</code>, then <code>API (from REST, GraphQL or SOAP)</code>:</p> <p></p> <p>Then select, <code>From existing GraphQL service (GraphQL proxy)</code>:</p> <p></p> <p>Give it a name and complete the rest of the parameters, especially the GraphQL Server URL field which, of course, cannot be left blank. </p> <p></p> <p>Notice that, once you enter the URL, API Connect will find the service and automatically detect the schema. </p> <p></p> <p>You can click <code>Next</code> and API Connect will attempt to connect to the GraphQL server, introspect it, and pull in any objects (in this case, queries and operations) that it can find on that server:</p> <p></p> <p>The gateway will sometimes provide warnings (17 in the example above), which are really just recommendations on how you can optimize the query. </p> <p>Clicking Next allows you to then activate the API for publishing:</p> <p></p> <p>Finally, clicking <code>Next</code> shows a screen such as the one below with checkmarks:</p> <p></p> <p>This shows that the gateway was able to connect to the GraphQL server and detect its schema, paths, and operations, validate them, and automatically populate the canvas with different policies, including security policies:</p> <p></p>"},{"location":"patterns/api-mgt/#further-readings","title":"Further Readings","text":"<ul> <li>IBM Redbook on Agile Integration</li> <li>A Demo of Event Endpoint Management - Cloud Pak for Integration</li> </ul>"},{"location":"patterns/cep/","title":"Situational decision","text":"<p>Enterprises need to identify and act on event streams to address specific business situations. The processing logic groups complex combinations of business events that have or have not occurred in a time frame and at runtime, recognize those situations and react to them with prescribed business actions. Situation detection is an asynchronous, near real-time, and uses temporal and geospatial characteristics of the events to take action. </p> <p>As an example, in account hijacking use case, the processing will involve consuming events from  user_login, password_changed, address_changed, new beneficiary added, money_transfert_initiated, in this order and with time windows contraints. The action will freeze the money transfert, alert the account owner, identify the IP address of the logged person, add more data for investigation, or alert operation team...  </p>"},{"location":"patterns/cep/#react-to-multiple-complex-events-with-prescribed-business-actions","title":"React to multiple complex events with prescribed business actions","text":"<p>Situational decision automation refers to detecting a situation, or an event, and then deciding how to act. The interresting part of this procesing is the integration of multiple event streams and the need to keep state in a context of a business entity. Context is important. The context can be a business entity on which a set of events are important to address. In previous example, the Account entity may be the context on which we want to address the events to better understand the event flow with time constraint. How to keep those business entity state related to real time processing is an implementation challenge. To take business decision, you need more information than just looking at the event payload. </p> <p>From a modeling point of view, you need to create a model of business entities and their associated business events, define the target situation, and define the rules to be applied for the situation. Event storming helps to identify events and business entities and their relationships. But we need to enhance the method with rule harvesting, situation modeling and action definition. The model and its associated logic are deployed to a situational processing service. The decisions that are made by the situational processing service can be refined and enhanced by using predictive analytics models.</p> <p>By using situational decision automation as part of an event-driven solution, you can process continuous event streams to derive insights and intelligence. You can analyze an event stream and then extract event data from it so that data scientists can understand and derive machine learning models. You can run analytical processes and machine learning models against the event stream and match complex event patterns across streams and time windows to help you decide and act. The Situational decision automation architecture can help you to build and implement such a solution.</p>"},{"location":"patterns/cep/#benefits-of-situational-decision-automation","title":"Benefits of situational decision automation","text":"<p>By adopting decision automation for events, you can optimize your business processes and realize these benefits:</p> <ul> <li>Spot risks and opportunities by detecting and predicting event patterns, and react to events as they happen</li> <li>Make more informed decisions by applying business rules and business logic on data streams</li> <li>Improve business responsiveness and minimize compliance risks</li> </ul>"},{"location":"patterns/cep/#the-reference-architecture","title":"The reference architecture","text":"<p>To make it simple, situational decision acts on event streams, so needs to be connected to event backbones. The reference architecture may look like in the following figure where complex event processing component like Apache Flink will continuously process data streams and generate synthetic events a separate service can act on:</p> <p></p>"},{"location":"patterns/cqrs/","title":"Command-Query Responsibility Segregation (CQRS)","text":""},{"location":"patterns/cqrs/#problems-and-constraints","title":"Problems and Constraints","text":"<p>A domain model encapsulates domain data with the behavior for maintaining the correctness of that data as it is modified, structuring the data based on how it is stored in the database and to facilitate managing the data. Multiple clients can independently update the data concurrently. Different clients may not use the data the way the domain model structures it, and may not agree with each other on how it should be structured.</p> <p>When a domain model becomes overburdened managing complex aggregate objects, concurrent updates, and numerous cross-cutting views, how can it be refactored to separate different aspects of how the data is used?</p> <p>An application accesses data both to read and to modify it. The primitive data tasks are often expressed as create, read, update, and delete (CRUD); using them is known as CRUDing the data. Application code often does not make much distinction between the tasks; individual operations may mix reading the data with changing the data as needed.</p> <p>This simple approach works well when all clients of the data can use the same structure and contention is low. A single domain model can manage the data, make it accessible as domain objects, and ensure updates maintain its consistency. However, this approach becomes inadaquate when different clients want different views across multiple sets of data, when the data is too widely used, and/or when multiple clients updating the data my unknowlingly conflict with each other.</p> <p>For example, in a microservices architecture, each microservice should store and manage its own data, but a user interface may need to display data from several microservices. A query that gathers bits of data from multiple sources can be inefficient (time and bandwidth consumed accessing multiple data sources, CPU consumed transforming data, memory consumed by intermediate objects) and must be repeated each time the data is accessed.</p> <p>Another example is an enterprise database of record managing data required by multiple applications. It can become overloaded with too many clients needing too many connections to run too many threads performing too many transactions--such that the database becomes a performance bottleneck and can even crash.</p> <p>Another example is maintaining consistency of the data while clients concurrently make independent updates to the data. While each update may be consistent, they may conflict with each other. Database locking ensures that the updates don't change the same data concurrently, but doesn't ensure multiple independent changes result in a consistent data model.</p> <p>When data usage is more complex than a single domain model can facilitate, a more sophisticated approach is needed.</p>"},{"location":"patterns/cqrs/#solution-and-pattern","title":"Solution and Pattern","text":"<p>Refactor a domain model to separate operations for querying data and operations for updating data so that they may be handled independently.</p> <p>The CQRS pattern strictly segregates operations that read data from operations that update data. An operation can read data (the R in CRUD) or can write data (the CUD in CRUD), but not both.</p> <p>This separation can make using data much more manageable in several respects. The read operations and the write operations are simpler to implement because their functionality is more finely focused. The operations can be developed independently, potentially by separate teams. The operations can be optimized independently and can evolve independently, following changing user requirements more easily. These optimized operations can scale better, perform better, and security can be applied more precisely.</p> <p>The full CQRS pattern uses separate read and write databases. In doing so, the pattern segregates not just the APIs for accessing data or the models for managing data, but even segregates the database itself into two, a read/write database that is effectively write-only and one or more read-only databases.</p> <p>The adoption of the pattern can be applied in phases, incrementally from existing code. To illustrate this, we will use four stages that could be used incrementally or a developer can go directly from stage 0 to 3 without considering the others:</p> <ol> <li>Stage 0: Typical application data access</li> <li>Stage 1: Separate read and write APIs</li> <li>Stage 2: Separate read and write models</li> <li>Stage 3: Separate read and write databases</li> </ol>"},{"location":"patterns/cqrs/#typical-application-data-access","title":"Typical application data access","text":"<p>Before even beginning to apply the pattern, let\u2019s consider the typical app design for accessing data. This diagram shows an app with a domain model for accessing data persisted in a database of record, i.e. a single source of truth for that data. The domain model has an API that at a minimum enables clients to perform CRUD tasks on domain objects within the model.</p> <p></p> <p>The domain model is an object representation of the database documents or records. It is comprised of domain objects that represent individual documents or records and the business logic for managing and using them. Domain-Driven Design (DDD) models these domain objects as entities\u2014\u201cobjects that have a distinct identity that runs through time and different representations\u201d\u2014and aggregates\u2014\u201ca cluster of domain objects that can be treated as a single unit\u201d; the aggregate root maintains the integrity of the aggregate as a whole.</p> <p>Ideally, the domain model\u2019s API should be more domain-specific than simply CRUDing of data. Instead, it should expose higher-level operations that represent business functionality like <code>findCustomer()</code>, <code>placeOrder()</code>, <code>transferFunds()</code>, and so on. These operations read and update data as needed, sometimes doing both in a single operation. They are correct as long as they fit the way the business works.</p>"},{"location":"patterns/cqrs/#separate-read-and-write-apis","title":"Separate read and write APIs","text":"<p>The first and most visible step in applying the CQRS pattern is splitting the CRUD API into separate read and write APIs. This diagram shows the same domain model as before, but its single CRUD API is split into retrieve and modify APIs.</p> <p></p> <p>The two APIs share the existing domain model but split the behavior:</p> <ul> <li>Read: The retrieve API is used to read the existing state of the objects in the domain model without changing that state. The API treats the domain state as read only.</li> <li>Write: The modify API is used to change the objects in the domain model. Changes are made using CUD tasks: create new domain objects, update the state in existing ones, and delete ones that are no longer needed. The operations in this API do not return result values, they return success (ack or void) or failure (nak or throw an exception). The create operation might return the primary of key of the entity, which can be generated either by the domain model or in the data source.</li> </ul> <p>This separation of APIs is an application of the Command Query Separation (CQS) pattern, which says to clearly separate methods that change state from those that don\u2019t. To do so, each of an object\u2019s methods can be in one of two categories (but not both):</p> <ul> <li>Query: Returns a result. Does not change the system\u2019s state nor cause any side effects that change the state.</li> <li>Command (a.k.a. modifiers or mutators): Changes the state of a system. Does not return a value, just an indication of success or failure.</li> </ul> <p>With this approach, the domain model works the same and provides access to the data the same as before. What has changed is the API for using the domain model. Whereas a higher-level operation might previously have both changed the state of the application and returned a part of that state, now each such operation is redesigned to only do one or the other.</p> <p>When the domain model splits its API into read and write operations, clients using the API must likewise split their functionality into querying and updating functionality. Most new web based applications are based in the single page application, with components and services that use and encapsulate remote API. So this separation of backend API fits well with modern web applications.</p> <p>This stage depends on the domain model being able to implement both the retrieve and modify APIs. A single domain model requires the retrieve and modify behavior to have similar, corresponding implementations. For them to evolve independently, the two APIs will need to be implemented with separate read and write models.</p>"},{"location":"patterns/cqrs/#separate-read-and-write-models","title":"Separate read and write models","text":"<p>The second step in applying the CQRS pattern is to split the domain model into separate read and write models. This doesn\u2019t just change the API for accessing domain functionality, it also changes the design of how that functionality is structured and implemented. This diagram shows that the domain model becomes the basis for a write model that handles changes to the domain objects, along with a separate read model used to access the state of the app.</p> <p></p> <p>Naturally, the read model implements the retrieve API and the write model implements the modify API. Now, the application consists not only of separate APIs for querying and updating the domain objects, there\u2019s also separate business functionality for doing so. Both the read business functionality and the write business functionality share the same database.</p> <p>The write model is implemented by specializing the domain model to focus solely on maintaining the valid structure of domain objects when changing their state and by applying any business rules.</p> <p>Meanwhile, responsibility for returning domain objects is shifted to a separate read model. The read model defines data transfer objects (DTOs) designed specifically for the model to return just the data the client wants in a structure the client finds convenient. The read model knows how to gather the data used to populate the DTOs. DTOs encapsulate little if any domain functionality, they just bundle data into a convenient package that can easily be transmitted using a single method call, especially between processes.</p> <p>The read model should be able to implement the retrieve API by implementing the necessary queries and executing them. If the retrieve API is already built to return domain objects as results, the read model can continue to do so, or better yet, implements DTO types that are compatible with the domain objects and returns those. Likewise, the modify API was already implemented using the domain model, so the write model should preserve that. The write model may enhance the implementation to more explicitly implement a command interface or use command objects.</p> <p>This phase assumes that the read and write models can both be implemented using the same database of record the domain model has been using. To the extent this is true, the implementations of reading and writing can evolve independently and be optimized independently. This independence may become increasingly limited since they are both bound to the same database with a single schema or data model. To enable the read and write models to evolve independently, they may each need their own database.</p>"},{"location":"patterns/cqrs/#separate-read-and-write-databases","title":"Separate read and write databases","text":"<p>The third step in applying the CQRS pattern\u2014-which implements the complete CQRS pattern solution\u2014-is splitting the database of record into separate read and write databases. This diagram shows the write model and read model, each supported by its own database. The overall solution consists of two main parts: the write solution that supports updating the data and the read solution that supports querying the data. The two parts are connected by the event bus.</p> <p></p> <p>The write model has its own read/write database and the read model has its own read-only database. The read/write database still serves as the database of record (the single source of truth for the data) but is mostly used write-only: mostly written to and rarely read. Reading is offloaded onto a separate read database that contains the same data but is used read-only.</p> <p>The query database is effectively a cache of the database of record, with all of the inherit benefits and complexity of the Caching pattern. The query database contains a copy of the data in the database of record, with the copy structured and staged for easier access by the clients using the retrieve API. As a copy, overhead is needed to keep the copy synchronized with changes in the original. Latency in this synchronization process creates eventual consistency, during which the data copy is stale.</p> <p>The separate databases enable the separate read and write models and their respective retrieve and modify APIs to truly evolve independently. Not only can the read model or write model\u2019s implementation change without changing the other, but how each stores its data can be changed independently.</p> <p>This solution offers the following advantages:</p> <ul> <li>Scaling: The query load is moved from the write database to the read database. If the database of record is a scalability bottleneck and a lot of the load on it is caused by queries, unloading those query responsibilities can significantly improve the scalability of the combined data access.</li> <li>Performance: The schemas of the two databases can be different, enabling them to be designed and optimized independently for better performance. The write database can be optimized for data consistency and correctness, with capabilities such as stored procedures that fit the write model and assist with data updates. The read database can store the data in units that better fit the read model and are better optimized for querying, with larger rows requiring fewer joins.</li> </ul> <p>Notice that the design for this stage is significantly more complex than the design for the previous stage. Separate databases with copies of the same data may make data modeling and using data easier, but they require significant overhead to synchronize the data and keep the copies consistent.</p> <p>CQRS employs a couple of design features that support keeping the databases synchronized:</p> <ul> <li>Command Bus for queuing commands (optional): A more subtle and optional design decision is to queue the commands produced by the modify API, shown in the diagram as the command bus. This can significantly increase the throughput of multiple apps updating the database, as well as serialize updates to help avoid--or at least detect--merge conflicts. With the bus, a client making an update does not block synchronously while the change is written to the database. Rather, the request to change the database is captured as a command (Design Patterns) and put on a message queue, after which the client can proceed with other work. Asynchronously in the background, the write model processes the commands at the maximum sustainable rate that the database can handle, without the database ever becoming overloaded. If the database becomes temporarily unavailable, the commands queue and will be processed when the database becomes available once more.</li> <li>Event Bus for publishing update events (required): Whenever the write database is updated, a change notification is published as an event on the event bus. Interested parties can subscribe to the event bus to be notified when the database is updated. One such party is an event processor for the query database, which receives update events and processes them by updating the query database accordingly. In this way, every time the write database is updated, a corresponding update is made to the read database to keep it in sync.</li> </ul> <p>The connection between the command bus and the event bus is facilitated by an application of the Event Sourcing pattern, which keeps a change log that is suitable for publishing. Event sourcing maintains not only the current state of the data but also the history of how that current state was reached. For each command on the command bus, the write model performs these tasks to process the command:</p> <ul> <li>Logs the change</li> <li>Updates the database with the change</li> <li>Creates an update event describing the change and publishes it to the event bus</li> </ul> <p>The changes that are logged can be the commands from the command bus or the update events published to the event bus</p>"},{"location":"patterns/cqrs/#considerations","title":"Considerations","text":"<p>Keep these decisions in mind while applying this pattern:</p> <ul> <li>Client impact: Applying CQRS not only changes how data is stored and accessed, but also changes the APIs that clients use to access data. This means that each client must be redesigned to use the new APIs.</li> <li>Riskiness: A lot of the complexity of the pattern solution involves duplicating the data in two databases and keeping them synchronized. Risk comes from querying data that is stale or downright wrong because of problems with the synchronization.</li> <li>Eventual consistency: Clients querying data must expect that updates will have latency. In a microservices architecture, eventual data consistency is a given and acceptable in many of cases.</li> <li>Command queuing: Using a command bus as part of the write solution to queue the commands is optional but powerful. In addition to the benefits of queuing, the command objects can easily be stored in the change log and easily be converted into notification events. (In the next section, we illustrate a way to use event bus to queue commands as well.)</li> <li>Change log: The log of changes to the database of record can be either the list of commands from the command bus or the list of event notifications published on the event bus. The Event Sourcing pattern assumes it\u2019s a log of events, but that pattern doesn\u2019t include the command bus. An event list may be easier to scan as a history, whereas a command list is easier to replay.</li> <li>Create keys: Strick interpretation of the Command Query Separation (CQS) pattern says that command operations do not have return types. A possible exception is commands that create data: An operation that creates a new record or document typically returns the key for accessing the new data, which is convenient for the client. However, if the create operation is invoked asynchronously by a command on a command bus, the write model will need to perform a callback on the client to return the key.</li> <li>Messaging queues and topics: While messaging is used to implement both the command bus and event bus, the two busses use messaging differently. The command bus guarantees exactly once delivery. The event bus broadcasts each event to all interested event processors.</li> <li>Query database persistence: The database of record is always persistent. The query database is a cache that can be a persistent cache or an in-memory cache. If the cache is in-memory and is lost, it must be rebuilt completely from the database of record.</li> <li>Security: Controls on reading data and updating data can be applied separately using the two parts of the solution.</li> </ul>"},{"location":"patterns/cqrs/#combining-event-sourcing-and-cqrs","title":"Combining event sourcing and CQRS","text":"<p>The CQRS application pattern is frequently associated with event sourcing: when doing event sourcing and domain driven design, we event source the aggregates or root entities. Aggregate creates events that are persisted. On top of the simple create, update and read by ID operations, the business requirements want to perform complex queries that can't be answered by a single aggregate. By just using event sourcing to be able to respond to a query like \"what are the orders of a customer\", then we have to rebuild the history of all orders and filter per customer. It is a lot of computation. This is linked to the problem of having conflicting domain models between query and persistence.</p> <p>As introduced in previous section, creations and updates are done as state notification events (change of state), and are persisted in the event log/store. The following figure, presents two separate microservices, one supporting the write model, and multiple other supporting the queries:</p> <p></p> <p>The query part is separate processes that consume change log events and build a projection for future queries. The \"write\" part may persist in SQL while the read may use document oriented database with strong indexing and query capabilities. Or use in-memory database, or distributed cache... They do not need to be in the same language. With CQRS and ES the projections are retroactives. New query equals implementing new projection and read the events from the beginning of time or the recent committed state and snapshot. Read and write models are strongly decoupled and can evolve independently. It is important to note that the 'Command' part can still handle simple queries, primary-key based, like get order by id, or queries that do not involve joins.</p> <p>The event backbone, use a pub/sub model, and Kafka is a good candidate as an implementation technology.</p> <p>With this structure, the <code>Read model</code> microservice will most likely consume events from multiple topics to build the data projection based on joining those data streams. A query, to assess if the cold-chain was respected on the fresh food order shipment, will go to the voyage, container metrics, and order to be able to answer this question. This is where CQRS shines.</p> <p>We can note that, we can separate the API definition and management in a API gateway.</p> <p>The shipment order microservice is implementing this pattern.</p> <p>Some implementation items to consider:</p> <ul> <li>Consistency (ensure the data constraints are respected for each data transaction): CQRS without event sourcing has the same consistency guarantees as the database used to persist data and events. With Event Sourcing the consistency could be different, one for the write model and one for the read model. On write model, strong consistency is important to ensure the current state of the system is correct, so it leverages transaction, lock and sharding. On read side, we need less consistency, as they mostly work on stale data. Locking data on the read operation is not reasonable.</li> <li>Scalability: Separating read and write as two different microservices allows for high availability. Caching at the read level can be used to increase performance response time, and can be deployed as multiple standalone instances (Pods in Kubernetes). It is also possible to separate the query implementations between different services. Functions as service / serverless are good technology choices to implement complex queries.</li> <li>Availability: The write model sacrafices consistency for availability. This is a fact. The read model is eventually consistent so high availability is possible. In case of failure the system disables the writing of data but still is able to read them as they are served by different databases and services.</li> </ul> <p>With CQRS, the write model can evolve over time without impacting the read model, as long as the event model doesn't change. The read model requires additional tables, but they are often simpler and easier to understand.</p> <p>CQRS results in an increased number of objects, with commands, operations, events,... and packaging in deployable components or containers. It adds potentially different type of data sources. It is more complex.</p> <p>Some challenges to always consider:</p> <ul> <li>How to support event structure version management?</li> <li>How much data to keep in the event store (history)?</li> <li>How to adopt data duplication which results to eventual data consistency?.</li> </ul> <p>The CQRS pattern was introduced by Greg Young, and described in Martin Fowler's work on microservices.</p> <p>As you can see in previous figure, as soon as we see two arrows from the same component, we have to ask ourselves how does it work: the write model has to persist Order in its own database and then sends OrderCreated event to the topic... Should those operations be atomic and controlled with transaction? We detail this in next section.</p>"},{"location":"patterns/cqrs/#keeping-the-write-model-on-mainframe","title":"Keeping the write model on Mainframe","text":"<p>It is very important to note that the system of records and transaction processing is still easier to run on mainframe to support strong consistency. But with the move to cloud native development, it does not mean we have to move all the system of records to the cloud. Data pipelines can be put in place, but CQRS should help by keeping the write model on the current system of records and without impacting the current MIPS utilization move data in the eventual consistency workd of the cloud native, distributed computing world.</p> <p></p> <p>In the figure above, the write model follows the current transaction processing on the mainframce, change data capture push data to Event backbone for getting real time visibility into the distributed world. The read is the costly operation, dues to the joins to be done to build the projection views needed to expose data depending of the business use cases. This is more true with distributed microservices. All the write operations for the business entities kept in the mainframe's system of records are still done via the transaction.  Reference data can be injected in one load job to topic and persisted in the event store so streaming applications can leverage them by joining with transactional data. </p>"},{"location":"patterns/cqrs/#the-consistency-challenges","title":"The consistency challenges","text":"<p>As introduced in the previous section, there is a potential problem of data inconsistency: once a command saves changes into the database, the consumers do not see the new or updated data until event notification completes processing.</p> <p>With traditional Java service, using JPA and JMS, the save and send operations can be part of the same XA transaction and both succeed or fail.</p> <p>With event sourcing pattern, the source of trust is the event source, which acts as a version control system, as shown in the diagram below.</p> <p></p> <p>The steps for syncronizing changes to the data are:</p> <ol> <li>The write model creates the event and publishes it</li> <li>The consumer receives the event and extracts its payload</li> <li>The consumer updates its local datasource with the payload data</li> <li>If the consumer fails to process the update, it can persist the event to an error log</li> <li>Each error in the log can be replayed</li> <li>A command line interface replays an event via an admin API, which searches in the topic using this order id to replay the save operation</li> </ol> <p>This implementation causes a problem for the <code>createOrder(order): string</code> operation: The Order Service is supposed to return the new order complete with the order id that is a unique key, a key most likely created by the database. If updating the database fails, there is no new order yet and so no database key to use as the order ID. To avoid this problem, if the underlying technology supports assigning the new order's key, the service can generate the order ID and use that as the order's key in the database.</p> <p>It is important to clearly study the Kafka consumer API and the different parameters on how to support the read offset. We are addressing those implementation best practices in our consumer note.</p>"},{"location":"patterns/cqrs/#cqrs-and-change-data-capture","title":"CQRS and Change Data Capture","text":"<p>There are other ways to support this dual operations level:</p> <ul> <li>When using Kafka, Kafka Connect has the capability to subscribe to databases via JDBC, allowing to poll tables for updates and then produce events to Kafka.</li> <li>There is an open-source change data capture solution based on extracting change events from database transaction logs, Debezium that helps to respond to insert, update and delete operations on databases and generate events accordingly. It supports databases like MySQL, Postgres, MongoDB and others.</li> <li>Write the order to the database and in the same transaction write to an event table (\"outbox pattern\"). Then use a polling to get the events to send to Kafka from this event table and delete the row in the table once the event is sent.</li> <li>Use the Change Data Capture from the database transaction log and generate events from this log. The IBM Infosphere CDC product helps to implement this pattern. For more detail about this solution see this product tour.</li> </ul> <p>The CQRS implementation using CDC will look like in the following diagram:</p> <p></p> <p>What is important to note is that the event needs to be flexible on the data payload. We are presenting a event model in the reference implementation.</p> <p>On the view side, updates to the view part need to be idempotent.</p>"},{"location":"patterns/cqrs/#delay-in-the-view","title":"Delay in the view","text":"<p>There is a delay between the data persistence and the availability of the data in the Read model. For most business applications, it is perfectly acceptable. In web based data access most of the data are at stale.</p> <p>When there is a need for the client, calling the query operation, to know if the data is up-to-date, the service can define a versioning strategy. When the order data was entered in a form within a single page application like our kc- user interface, the \"create order\" operation should return the order with its unique key freshly created and the Single Page Application will have the last data. Here is an example of such operation:</p> <pre><code>@POST\npublic Response create(OrderCreate dto) {\n    Order order = new Order(UUID.randomUUID().toString(), dto.getProductID(),...);\n    // ...\n    return Response.ok().entity(order).build()\n}\n</code></pre>"},{"location":"patterns/cqrs/#schema-change","title":"Schema change","text":"<p>What to do when we need to add attribute to event?. So we need to create a versioninig schema for event structure. You need to use flexible schema like json schema, Apache Avro or protocol buffer and may be, add an event adapter (as a function?) to translate between the different event structures.</p>"},{"location":"patterns/cqrs/#code-reference","title":"Code reference","text":"<ul> <li>The following project includes two sub modules, each deployable as a microservice to illustrate the command and query part: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms</li> </ul> Further readings <ul> <li>https://www.codeproject.com/Articles/555855/Introduction-to-CQRS</li> <li>http://udidahan.com/2009/12/09/clarified-cqrs</li> <li>https://martinfowler.com/bliki/CQRS.html</li> <li>https://microservices.io/patterns/data/cqrs.html</li> <li>https://community.risingstack.com/when-to-use-cqrs</li> <li>https://dzone.com/articles/concepts-of-cqrs</li> <li>https://martinfowler.com/bliki/CommandQuerySeparation.html</li> <li>https://www.martinfowler.com/eaaCatalog/domainModel.html</li> <li>https://dddcommunity.org/learning-ddd/what_is_ddd/</li> <li>https://martinfowler.com/bliki/EvansClassification.html</li> <li>https://martinfowler.com/bliki/DDD_Aggregate.html</li> <li>https://martinfowler.com/eaaCatalog/dataTransferObject.html</li> <li>https://en.wikipedia.org/wiki/Command_pattern</li> <li>https://www.pearson.com/us/higher-education/program/Gamma-Design-Patterns-Elements-of-Reusable-Object-Oriented-Software/PGM14333.html</li> </ul>"},{"location":"patterns/data-pipeline/","title":"Data Intensive Application","text":"<p>In this article we are highlighting some practices to design and develop data intensive application in the context of microservice solution. This is strongly linked to the adoption of event-driven microservices, but addresses the data consistency and eventual data consistency discussions, as well as the establishment of a data fabric services.</p>"},{"location":"patterns/data-pipeline/#context","title":"Context","text":"<p>A typical modern business solution will include a set of microservices working together in choreography to exchange data. The adoption of event-driven microservices, with all the related design patterns, is described in separate articles that you can read here. </p> <p>When zooming to a particular data intensive microservice we will find a set of important data centric features that may look like in the diagram below, which presents one component of a bigger distributed system.</p> <p></p> <p>The services involved include:</p> <ul> <li>API to define the service contract: OpenAPI or AsynchAPI</li> <li>Databases to store data for long term - document oriented or SQL based</li> <li>Caches to speed up retrieving data for expensive queries</li> <li>Search indexes to support search on a corpus</li> <li>Stream processing to pub/sub messages, which are now also considered as long duration datastore (Kafka).</li> <li>Interactive queries on top of data streams, and aggregates.</li> <li>Message contract and schemas</li> <li>Unified data service which includes big data storage, but also data catalog and queries on data at rest.</li> </ul>"},{"location":"patterns/data-pipeline/#subjects-of-concern","title":"Subjects of concern","text":"<p>When designing such application we need to address a set of important subjects:</p> <ul> <li>What is the type of workload on the database? Read heavy, global access query, write heavy or balanced. </li> <li>Expected throughtput? Is there any fluctuation during the day? does it need to scale over time?</li> <li>How much data to store? what will be the pattern for this size (Always grow)? How content is access via which expected security and access control?</li> <li>What will be the expected durability? Week or forever?</li> <li>Expected latency and number of concurrent users?</li> <li>How the data model is modeled, does it need to support relationship integrity? Join queries? Structured or semi-structured content?</li> <li>Do we need strong schema, or more flexible one?</li> <li>Do we need search on unstructured data? NoSql?</li> <li>How to ensure data correctness and completeness?</li> <li>How to address good performance when exposing data, even when app is running slowly?</li> <li>How to scale and address increase in transaction volume and data size?</li> <li>What data to expose to other services via messaging ? Which formats?</li> <li>What data to expose to other services via APIs ?</li> <li>How to support application reliability when some components are not performing within their SLA? How to be fault-tolerant?</li> <li>How to test fault-tolerance?</li> <li>How does adding horizontal compute power impact the data access?</li> <li>How to support disaster recovery?</li> </ul> <p>In modern big data applications, hardware redundancy is not suffisant, the design needs to support unexpected faults, to avoid cascading failures or to support new version deployment with rolling-upgrade capability. </p> <p>When addressing scalability and load growth, we need to define the load parameters:  number of transactions per second, the number of read and write operations,  the number of active sessions, ... on average and at peak. Each microservice in a  solution will have its own load characteristics. </p> <p>From there, we need to address the following issues:</p> <ul> <li>How does load growth impact performance while keeping existing compute resources?</li> <li>What is the increase of compute resource needed to support same performance while load growth?</li> </ul> <p>The solution problem is a combination of different characteristics to address: read volume, write volume, data store volume, data complexity and size, response time, access logic...</p> <p>For batch processing the measurement is the throughput: number of records per second or time to process n records.  For real-time processing the response time measures the time to get a response from a client's point of view after sending a request.</p> <p>When defining service level agreement, it is important to use the median response time and a percentile of outliers.  An example the median could be at 300ms at P99 (99/100) under 1s.</p> <p>Tail latencies, or high percentiles of response time, impact directly user experience and cost money.</p>"},{"location":"patterns/data-pipeline/#distributed-data","title":"Distributed data","text":"<p>Adopting microservice architecture, means distributed systems and distributed data. The main motivations for that are scalability (load data operations could not be supported by one server), high availability (by sharing the same processing between multiple machines), and reducing latency to distribute data close to the end users.</p> <p>Vertical scaling is still bounded by hardware resources, so at higher load we need to support horizontal scaling by adding more machines to the cluster or cross multiple clusters.  When adding machines, we may want to adopt different techniques for data sharing: </p> <ul> <li>shared memory</li> <li>shared storage</li> <li>shared nothing: cpu, memory and disk are per node. Cluster manages node orchestration over network. This architecture brings new challenges. </li> </ul> Compendium <ul> <li>Designing data intensive application - Martin Kleppmann</li> <li>Assemble the team to support a data-driven project - author: Stacey Ronaghan</li> <li>The valuation of data - author: Neal Fishman</li> <li>Define business objectives - author: Neal Fishman</li> <li>Recognize the value of data - author: Neal Fishman</li> <li>Translate a business problem into an AI and Data Science solution - authors: Tommy Eunice, Edd Biddle, Paul Christensen</li> <li> <p>Prepare your data for AI and data science - authors: Edd Biddle, Paul Christensen</p> </li> <li> <p>Define your data strategy -authors: Beth Ackerman, Paul Christensen</p> </li> <li>Normalize data to its atomic level - author: Neal Fishman</li> <li>Understand data needs to support AI and Data Science solutions - authors: Tommy Eunice, Edd Biddle, Paul Christensen</li> <li>Run thought experiments by using hypothesis-driven analysis - author: Edd Biddle, Paul Christensen</li> <li>Deliver a singular data function - author: Neal Fishman</li> <li>Construct your data topology - authors: Neal Fishman, Paul Christensen</li> <li>Build your data lake design - author: Paul Christensen</li> <li>Put AI and data science to work in your organization - authors: Edd Biddle, Paul Christensen</li> <li>Look behind the curtain of AI - author: Edd Biddle</li> <li>Select and develop an AI and data science model - author: Edd Biddle</li> <li>Enhance and optimize your AI and data science models - author: Edd Biddle</li> <li>Establish data governance - author: Neal Fishman</li> <li>Deploy an AI model - author: Sujatha Perepa </li> </ul>"},{"location":"patterns/dlq/","title":"Dead Letter Queue","text":""},{"location":"patterns/dlq/#event-reprocessing-with-dead-letter-pattern","title":"Event reprocessing with dead letter pattern","text":"<p>With event driven microservice, it is not just about pub/sub: there are use cases where the microservice needs to call existing service via an HTTP or RPC call. The call may fail. So what should be the processing to be done to retry and gracefully fail by leveraging the power of topics and the concept of dead letter.</p> <p>This pattern is influenced by the adoption of Kafka as event backbone and the offset management offered by Kafka. Once a message is read from a Kafka topic by a consumer the offset can be automatically committed so the consumer can poll the next batch of events, or in most the case manually committed, to support business logic to process those events.</p> <p>The figure below demonstrates the problem to address: the <code>reefer management microservice</code> get order event to process by allocating a reefer container to the order. The call to update the container inventory fails.</p> <p></p> <p>At first glance, the approach is to commit the offset only when the three internal operations are succesful: write to reefer database (2), update the container inventory using legacy application service (3), and produce new event to <code>orders</code> (4) and <code>containers</code> (5) topics. Step (4) and (5) will not be done as no response from (3) happened.</p> <p>In fact a better approach is to commit the read offset on the orders topic, and then starts the processing: the goal is to do not impact the input throughput. In case of step (2) or (3) fails the order data is published to an <code>order-retries</code> topic (4) with some added metadata like number of retries and timestamp.</p> <p></p> <p>A new order retry service or function consumes the <code>order retry</code> events (5) and do a new call to the remote service using a delay according to the number of retries already done: this is to pace the calls to a service that has issue for longer time. If the call (6) fails this function creates a new event in the <code>order-retries</code> topic with a retry counter increased by one. If the number of retry reaches a certain threshold then the order is sent to <code>order-dead-letter</code> topic (7) for human to work on. A CLI could read from this dead letter topic to deal with the data, or retry automatically once we know the backend service works. Using this approach we delay the call to the remote service without putting too much preassure on it.</p> <p>We have implemented the dead letter pattern when integrating the container manager microservice with an external BPM end point. The integration test detail is in this note and the integration test here.</p> <p>For more detail we recommend this article from Uber engineering: Building Reliable Reprocessing and Dead Letter Queues with Apache Kafka.</p>"},{"location":"patterns/event-sourcing/","title":"Event Sourcing","text":""},{"location":"patterns/event-sourcing/#problems-and-constraints","title":"Problems and Constraints","text":"<p>Most business applications are state based persistent where any update changes the previous state of business entities. The database keeps the last committed update. But some business application needs to explain how it reaches its current state. For that the application needs to keep history of business facts. Traditional domain oriented implementation builds a domain data model and map it to a RDBMS. As an example, in the simple <code>Order model</code> below, the database record will keep the last state of the Order entity, with the different addresses and the last ordered items in separate tables.</p> <p></p> <p>If you need to implement a query that looks at what happened to the order over a time period, you need to change the model and add historical records, basically building a log table.</p> <p>Designing a service to manage the life cycle of this order will, most of the time, add a \"delete operation\" to remove data.  For legal reason, most businesses do not remove data. As an example, a business ledger has to include new record(s) to compensate a previous transaction. There is no erasing of previously logged transactions. It is always possible to understand what was done in the past. Most business application needs to keep this capability.</p>"},{"location":"patterns/event-sourcing/#solution-and-pattern","title":"Solution and Pattern","text":"<p>Event sourcing persists the state of a business entity, such an Order, as a sequence of state-changing events or immuttable \"facts\" ordered over time. Event sourcing has its roots in the domain-driven design community. </p> <p></p> <p>When the state of a system changes, an application issues a notification event of the state change. Any interested parties can become consumers of the event and take required actions.  The state-change event is immutable stored in an event log or event store in time order.  The event log becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for business analysts to gain insights into the business.</p> <p>You can see the \"removing an item\" event in the log is a new event. With this capability, we can count how often a specific product is removed for the shopping cart.</p> <p>In some cases, the event sourcing pattern is implemented completely within the event backbone.  Kafka topic and partitions are the building blocks for event sourcing. However, you can also consider implementing the pattern with an external event store, which provides optimizations for how the data may be accessed and used. For example IBM Db2 Event store can provide the handlers and event store connected to the backbone and can provide optimization for down stream analytical processing of the data.</p> <p>An event store needs to store only three pieces of information:</p> <ul> <li>The type of event or aggregate.</li> <li>The sequence number of the event.</li> <li>The data as a serialized entity.</li> </ul> <p>More data can be added to help with diagnosis and audit, but the core functionality only requires a narrow set of fields. This gives rise to a very simple data design that can be heavily optimized for appending and retrieving sequences of records.</p> <p>With a central event logs, as provides by Kafka, producers append events to the log, and consumers read them from an offset (a sequence number).</p> <p></p> <p>To get the final state of an entity, the consumer needs to replay all the events, which means replaying the changes to the state from the last committed offset or from the last snapshot or the origin of \"time\".</p>"},{"location":"patterns/event-sourcing/#advantages","title":"Advantages","text":"<p>The main goal is to be able to understand what happens to a business entity over time. But there are a set of interesting things that can be done:</p> <ul> <li>We can rebuild the data view within a microservice after it crashes, by reloading the event log.</li> <li>As events are ordered with time, we can apply complex event processing with temporal queries, time window operations, and looking at non-event.</li> <li>Be able to reverse the state and correct data with new events.</li> </ul>"},{"location":"patterns/event-sourcing/#considerations","title":"Considerations","text":"<p>When replaying the events, it may be important to avoid generating side effects. A common side effect is to send a notification on state change to other consumers. So the consumer of events need to be adapted to the query and business requirements. For example, if the code needs to answer to the question: \"what happened to the order ID 75 over time?\" then there is no side effect, only a report can be created each time the consumer runs.</p> <p>Sometime it may be too long to replay hundreds of events. In that case we can use snapshot, to capture the current state of an entity, and then replay events from the most recent snapshot. This is an optimization technique not needed for all event sourcing implementations. When state change events are in low volume there is no need for snapshots.</p> <p>Kafka is supporting the event sourcing pattern with the topic and partition. In our reference implementation we are validating event sourcing with Kafka in the Order microservices and demonstrated in this set of test cases.</p> <p>The event sourcing pattern is well described in this article on microservices.io. It is a very important pattern to support eventual data consistency between microservices and for data synchronization between system as the event store becomes the source of truth.</p> <p>See also this event sourcing article from Martin Fowler, where he is also using ship movement examples. Our implementation differs as we are using Kafka topic as event store and use different entities to support the container shipping process: the Order, the Container and Voyage entities...</p> <p>Another common use case, where event sourcing helps, is when developers push a new code version that corrupts the data: being able to see what was done on the data, and being able to reload from a previous state helps fixing problems.</p>"},{"location":"patterns/event-sourcing/#command-sourcing","title":"Command sourcing","text":"<p>Command sourcing is a similar pattern as the event sourcing one, but the commands that modify the states are persisted instead of the events. This allows commands to be processed asynchronously, which can be relevant when the command execution takes a lot of time. One derived challenge is that the command may be executed multiple times, especially in case of failure. Therefore, it has to be idempotent ( making multiple identical requests has the same effect as making a single request). Finally, there is a need also to perform validation of the command to avoid keeping wrong commands in queue. For example, <code>AddItem</code> command is becoming <code>AddItemValidated</code>, then once persisted to a database it becomes an event as <code>ItemAdded</code>. So mixing command and event sourcing is a common practice.</p> <ul> <li>Business transactions are not ACID and span multiple services, they are more a serie of steps, each step is supported by a microservice responsible to update its own entity. We talk about \"eventual data consistency\".</li> <li>The event backbone needs to guarantee that events are delivered at least once and the microservices are responsible to manage their offset from the stream source and deal with inconsistency, by detecting duplicate events.</li> <li>At the microservice level, updating data and emitting event needs to be an atomic operation, to avoid inconsistency if the service crashes after the update to the datasource and before emitting the event. This can be done with an eventTable added to the microservice datasource and an event publisher that reads this table on a regular basis and change the state of the event once published. Another solution is to have a database transaction log reader or miner responsible to publish event on new row added to the log.</li> <li>One other approach to avoid the two-phase commit and inconsistency is to use an Event Store or Event Sourcing pattern to keep track of what is done on the business entity with enough information to rebuild the data state. Events are becoming facts describing state changes done on the business entity.</li> </ul>"},{"location":"patterns/event-sourcing/#code-repository","title":"Code repository","text":"<p>All the microservices implementing the Reefer management solution is using event sourcing, as we use kafka with long persistence. The order management service is using CQRS combined with event sourcing: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms and an integration test validate the pattern here.</p>"},{"location":"patterns/event-sourcing/#compendium","title":"Compendium","text":"<ul> <li>Martin Fowler - event sourcing pattern</li> <li>Microservice design pattern from Chris Richardson</li> <li>Greg Young video on event sourcing at the goto; conference</li> </ul>"},{"location":"patterns/intro/","title":"Introduction","text":"<p>In this set of articles, we will detail some of the most import event-driven patterns that can be utilised during your event-driven microservice implementation.</p> <p>Adopting messaging (Pub/Sub) as a microservice communication approach involves using, at least, the following patterns:</p> <ul> <li>Decompose by subdomain: The domain-driven design approach is useful to identify and classify business functions and the corresponding microservices that would be associated with them. With the event storming method, aggregates help to find those subdomains of responsibility. (Source Chris Richardson - Microservices Patterns)</li> <li>Database per service: Each service persists data privately and is accessible only via its API. Services are loosely coupled limiting impact to other services when schema changes occur in the database. The chosen database technology is driven by business requirements. (Source Chris Richardson - Microservices Patterns) The implementation of transactions that span multiple services is complex and enforces using the Saga pattern. Queries that span multiple entities are a challenge and CQRS represents an interesting solution.</li> <li>Strangler pattern: Used to incrementally migrate an existing, monolithic, application by replacing a set of features to a microservice but keep both running in parallel. Applying a domain driven design approach, you may strangle the application using bounded context. But then as soon as this pattern is applied, you need to assess the co-existence between existing bounded contexts and the new microservices. One of the challenges will be to define where the write and read operations occurs, and how data should be replicated between the contexts. This is where event driven architecture helps.</li> <li>Event sourcing: persists, to an append log, the states of a business entity, such as an Order, as a sequence of immutable state-changing events.</li> <li>Command Query Responsibility Segregation: helps to separate queries from commands and help to address queries with cross-microservice boundary.</li> <li>Saga pattern: Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice, interested in other business entities, subscribes to those events and it can update its own state and business entities on receipt of these events. Business entity keys need to be unique and immutable.</li> <li>Event reprocessing with dead letter: event driven microservices may have to call external services via a synchronous call. We need to process failure in order to get response from those services using event backbone.</li> <li>Transactional outbox: A service command typically needs to update the database and send messages/events. The approach is to use an outbox table to keep the message to sent and a message relay process to publish events inserted into database to the event backbone. (Source Chris Richardson - Microservices Patterns)</li> </ul>"},{"location":"patterns/intro/#strangler-pattern","title":"Strangler pattern","text":""},{"location":"patterns/intro/#problem","title":"Problem","text":"<p>How to migrate a monolithic application to a microservice based architecture without doing the huge effort of redeveloping the application from a blank slate. Replacing and rewriting an existing application can be a huge investment. Rewriting a subset of business functions while running current application in parallel may be relevant and reduce risk and velocity of changes.</p> <p>The figure below illustrates a typical mainframe application, with external Java based user interface connected to the mainframe via iop/corba and with three different applications to manage product, order and customer.</p> <p></p>"},{"location":"patterns/intro/#solution","title":"Solution","text":"<p>The approach is to use a \"strangler\" interface to dispatch a request to new or old features. Existing features to migrate are selected by trying to isolate sub components.</p> <p>One of main challenges is to isolate the data store and disover how the new microservices and the legacy application are accessing the shared data. Continuous data replication can be a solution to propagate write model to read model. Write model will most likely stays on the monolitic application, change data capture can be used, with event backbone to propagate change to read model.</p> <p>The facade needs to be scalable and not a single point of failure. It needs to support new APIs (RESTful) and old API (most likely SOAP).</p> <p>The following figure illustrates an implementation using an event driven solution with data replication to synchronize the write model to the read model on the mainframe.</p> <p></p>"},{"location":"patterns/intro/#transactional-outbox","title":"Transactional outbox","text":"<p>When distributed transaction is not supported by the messaging middleware (like current Kafka version), it is important to ensure consistency between the records in the database and the events published. In the reference implementation we used the approach to publish to the topic as soon as an order is received via the API and then the same code, is consuming this events to persist to the database. With this approach if write to the topic operation fails, the application can return an error to the user, if the write operation to the database fails, the code can reload from the non-committed record. </p> <p>But there is another solution presented by the transactional outbox. For detailed information about this pattern see the documentation of the pattern in Chris Richardson's site: Transactional outbox.</p> <p>To summarize this pattern, the approach is to use an <code>outbox</code> table to keep the messages to sent and a message relay process to publish events inserted into database to the event backbone. In modern solution this relay is a change data capture agent. The following schema illustrates the approach:</p> <p></p> <p>To get a concrete example of this pattern we have developed a deep dive lab using Quarkus Debezium outbox pattern and a DB2 database.</p>"},{"location":"patterns/realtime-analytics/","title":"Realtime Analytics","text":"<p>One of the essential elements of modern event-driven solutions is the ability to process continuous event streams to derive real time insights and intelligence.</p> <p>In this section we will take more detailed look at what this means in terms of required capabilities and the technology choices that are available to provide these as part of the Event Driven Architecture.</p>"},{"location":"patterns/realtime-analytics/#streaming-analytics-near-real-time-analytics","title":"Streaming analytics (near real-time analytics)","text":"<p>Streaming analytics provides the capabilities to look into and understand the events flowing through unbounded real-time event streams. Streaming applications process the event flow and allow data and analytical functions to be applied to information in the stream. Streaming applications are written as multistep flows across the following capabilities:</p> <ul> <li>Ingest many sources of events.</li> <li>Prepare data by transforming, filtering, correlating, aggregating on some metrics and leveraging other data sources for data enrichment.</li> <li>Detect and predict event patterns using scoring and classification.</li> <li>Decide by applying business rules and business logic.</li> <li>Act by directly executing an action, or in event-driven systems publishing an event notification or command.</li> </ul> <p></p>"},{"location":"patterns/realtime-analytics/#basic-streaming-analytics-capabilities","title":"Basic streaming analytics capabilities","text":"<p>To support the near real-time analytical processing of the unbounded event streams, the following capabilities are essential to the event stream processing component:</p> <ul> <li>Continuous event ingestion and analytical processing.</li> <li>Processing across multiple event streams.</li> <li>Low latency processing, where data do not have to be stored.</li> <li>Processing of high-volume and high-velocity streams of data.</li> <li>Continuous query and analysis of the feed.</li> <li>Correlation across events and streams.</li> <li>Windowing and stateful processing.</li> <li>Query and analysis of stored data.</li> <li>Development and execution of data pipelines.</li> <li>Development and execution of analytics pipelines.</li> <li>Scoring of machine learning models in line in the near real-time event stream processing.</li> </ul>"},{"location":"patterns/realtime-analytics/#support-for-near-real-time-analytics-and-decision-making","title":"Support for near real-time analytics and decision-making","text":"<p>Beyond the basic capabilities, consider supporting other frequently-seen event stream types and processing capabilities in your event stream processing component. By creating functions for these stream types and processes in the streaming application code, you can simplify the problem and reduce the development time.</p> <p>These capabilities include the following:</p> <ul> <li> <p>Geospatial</p> <ul> <li>Location-based analytics</li> <li>Geofencing &amp; map matching</li> <li>Spatio-temporal hangout detection</li> </ul> </li> <li> <p>Time series analysis</p> <ul> <li>Timestamped data analysis</li> <li>Anomaly detection &amp; forecasting</li> </ul> </li> <li> <p>Text analytics</p> <ul> <li>Natural Language Processing &amp; Natural Language Understanding</li> <li>Sentiment analysis &amp; entity extraction</li> </ul> </li> <li> <p>Video and audio</p> <ul> <li>Speech-to-text conversion</li> <li>Image recognition</li> </ul> </li> <li> <p>Rules</p> <ul> <li>Decisions described as business logic</li> </ul> </li> <li> <p>Complex Event Processing (CEP)</p> <ul> <li>Temporal pattern detection</li> </ul> </li> <li> <p>Entity Analytics</p> <ul> <li>Relationships between entities</li> <li>Probabilistic matching</li> </ul> </li> </ul>"},{"location":"patterns/realtime-analytics/#application-programming-languages-and-standards","title":"Application programming languages and standards","text":"<p>Few standards exist for event stream applications and languages. Typically, streaming engines have provided language-specific programming models tied to a specific platform.  The commonly used languages include the following:</p> <ul> <li>Python supports working with data and is popular with data scientists and data engineers.</li> <li>Java is the pervasive application development language. Kafka Streams offers a DSL to support most of the event streaming processing implementation.</li> <li>Scala adds functional programming and immutable objects to Java.</li> </ul> <p>Other platform specific languages have emerged when near real-time processing demands stringent performance requirements real time processing performance is required.</p> <p>More recently Google initiated the Apache Beam project https://beam.apache.org/ to provide a unified programming model for streaming analytics applications.</p> <p>Beam is a higher-level unified programming model that provides a standard way of writing streaming analytics applications in many supported languages, including Java, Python, Go and SQL.</p> <p>Streaming analytics engines typically support this unified programming model through a Beam runner that takes the code and converts it to platform-native executable code for the specific engine.</p> <p>See https://beam.apache.org/documentation/runners/capability-matrix/ for details of supporting engines and the capabilities.  Leading engines include Google Cloud DataFlow, Apache Flink, Apache Spark, Apache Apex, and IBM Streams.</p>"},{"location":"patterns/realtime-analytics/#run-time-characteristics","title":"Run time characteristics","text":"<p>In operational terms streaming analytics engines must receive and analyze arriving data continuously:</p> <ul> <li> <p>The \"Feed Never Ends\"</p> <ul> <li>The collection is unbounded.</li> <li>Not a request response set based model.</li> </ul> </li> <li> <p>The \"Firehose Doesn\u2019t Stop\"</p> <ul> <li>Keep drinking and keep up.</li> <li>The processing rate is greater than or equal to the feed rate.</li> <li>The analytics engine must be resilient and self-healing.</li> </ul> </li> </ul> <p>These specialized demands and concerns, which are not found in many other information processing environments, have led to highly-optimized runtimes and engines for stateful, parallel processing of analytical workloads across multiple event streams.</p>"},{"location":"patterns/realtime-analytics/#products","title":"Products","text":""},{"location":"patterns/realtime-analytics/#streaming-analytics","title":"Streaming Analytics","text":"<p>The market for streaming analytics products is quite confused with lots of different offering and very few standards to bring them together.  The potential product selection list for the streaming analytics component in the event driven architecture would need to consider:</p> <p>Top Open Source projects:</p> <ul> <li>Flink - real time streaming engine, both real time and batch analytics in one tool.</li> <li>Spark Streaming - micro batch processing through spark engine.</li> <li>Storm - Has not shown enough adoption.</li> <li>Kafka Streams - new/emerging API access for processing event streams in Kafka using a graph of operators</li> </ul> <p>Major Cloud Platform Providers support:</p> <ul> <li>Google Cloud DataFlow \u2013 proprietary engine open source streams application language ( Beam )</li> <li>Azure Stream Analytics \u2013 proprietary engine , SQL interface</li> <li>Amazon Kinesis - proprietary AWS</li> </ul> <p>IBM offerings</p> <ul> <li>IBM Streams/streaming Analytics (High performing parallel processing engine for real time analytics work loads)</li> <li>IBM Event streams (Kafka based event log/streaming platform)</li> </ul> <p>Evaluation of the various options, highlights</p> <ul> <li>The proprietary engines from the major providers, Google, MicroSoft, Amazon and IBM Streams continue to provide significant benefits in terms of performance and functionality for real time analysis of high volume realtime event streams.</li> <li>Kafka streams provides a convenient programming model for microservices to interact with the event stream data, but doesnt provide the optimized stream processing engine required for high volume real time analytics.</li> </ul> <p>Our decision for the Event Driven Architecture is to include:</p> <ul> <li>IBM streams as the performant, functionally rich real time event stream processing engine</li> <li>Event Streams (Kafka Streams), for manipulation of event streams within microservices</li> </ul> <p>IBM streams also supports Apache Beam as the open source Streams Application language,  which would allow portability of streams applications across, Flink, Spark, Google DataFlow..</p>"},{"location":"patterns/realtime-analytics/#an-anomaly-detection-in-event-driven-solutions","title":"An anomaly detection in event-driven solutions","text":"<p>We have implemented a separate solution based on the fresh food delivery to do anomaly detection for Refrigerated container in this repository with the following high level component view:</p> <p></p>"},{"location":"patterns/saga/","title":"Saga","text":"<p>Updated 06/18/2022</p>"},{"location":"patterns/saga/#problems-and-constraints","title":"Problems and Constraints","text":"<p>With the adoption of one data source per microservice, there is an interesting challenge on how to support long running transaction cross microservices. With event backbone two phase commit is not an option.</p>"},{"location":"patterns/saga/#solution-and-pattern","title":"Solution and Pattern","text":"<p>Introduced in 1987 by Hector Garcaa-Molrna Kenneth Salem paper the Saga pattern helps to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions.</p> <p>With microservice each transaction updates data within a single service, each subsequent steps may be triggered by previous completion. The following figure, based on our solution implementation, illustrates those concepts for an order transaction:</p> <p></p> <p>When the order is created, the business process says, we need to allocate a \"voyage\" (a Vessel container carrier), assign refrigerator containers and update the list of containers to load on the ship.  Those actions / commands are chained. The final state (in this schema, not in the reality, as the process has more steps)  is the Order assigned state in the order microservice.</p> <p>With a unique application implementation, the integrity between order, voyage and container tables will be done via transactions.  With distributed system we could not easily apply two phase commit transaction so the Saga pattern will help.</p> <p>SAGA pattern supports two types of implementation: Choreography and Orchestration.</p>"},{"location":"patterns/saga/#services-choreography","title":"Services choreography","text":"<p>With Choreography each service produces and listens to other service\u2019s events and decides if an action should be taken or not.</p> <p></p> <p>The first service executes a transaction to its own data store and then publishes an event ( OrderCreated event (1)) as fact about its business entity update.  It maintains the business entity status, (order.status) to the <code>Pending</code> state until the saga is completed. This event is listened by one or more services which execute local  transactions and publish new events (VoyageAllocated (3), ReeferAssigned (4), PaymentProcessed (5)). The distributed transaction ends when the last service executes its local transaction or when a service does not publish any events or the event published is not  polled by any of the saga\u2019s participants. For example, the Order microservice gets all the events from the other services and changed the Order state to be <code>Accepted</code>.</p> <p>In case of failure, the source microservice is keeping state and timer to monitor for the expected completion events.</p> <p></p> <p>When a message from any service is missing, the source service, needs to trigger a compensation process:</p> <p></p> <p>Rolling back a distributed transaction does not come for free. Normally you have to implement another operation/transaction to compensate for what has been done before. This will be a new event sent by the service responsible of the transaction integrity. In the order example, in the rare case where one of the service is not able to provide a positive response, no voyage found, or no Reefer container found, then the order needs to change to 'Uncompleted' status, and an event to the orders topic will claim the orderID is now uncompleted (OrderUncompleted event Step 1 above) . Any service that has something allocated for this orderId will 'unroll' their changes in their own data source  (Steps 2,3,4 below).</p> <p>Also it is important to note, that if one of the service is taking time to answer this may not be a problem as the order is in pending state. If the business requirement stipulates to address an order within a small time period then the compensation process may start. Uncompleted orders can be reviewed by a business user for manual handling. Email can be automatically sent to the customer about issue related to his order. There are a lot of different ways to handle order issue at the business level.</p> <p>See the eda-saga-choreography repository for our last implementation based on Quarkus, and IBM Event Streams.</p>"},{"location":"patterns/saga/#services-orchestration","title":"Services orchestration","text":"<p>With orchestration, one service is responsible to drive each participant on what to do and when. As we do not want to loose any message as part of this orchestration the technology of choice to support strong consistency and exactly once delivery, is to use IBM MQ, as illustrated by the following figure:</p> <p></p> <p>An example of Saga implementation using MQ is described in this repository and the orchestration implemenation with MQ is in the eda-kc-order-cmd-mq repo.</p> <p>An alternate approach is to use Kafka with producer using full acknowledge, idempotence, and a batch size of 1, and different topics mostly configured as queue: one consumer in each consumer group, manual commit, poll one message at a time.</p> <p></p> <p>It uses the different topics to control the saga by issuing event commands to the different service. It uses the event backbone as a queue processing to  support the asynchronous invocations. In this case the event should be exactly once delivered and idempotent. Each participant produces response in their  context and to the order topic. The orchestration layer needs to keep a state machine and acts once all the expected responses are received.</p> <p>If anything fails, the orchestrator is also responsible for coordinating the compensation process by sending rollback events with orderID and their respective impacted entity key (voyageID, reeferID, transactionID). Each  participant will undo its previous operations. Orchestrator is\u00a0a State Machine where each transformation corresponds to a command or message.</p> <p>See also this article from Chris Richardson on the Saga pattern.</p> <p>We have implemented the choreography saga pattern in the order management, voyage and refeer management microservices within the EDA reference implementation solution with a detailed explanation of the integration tests to validate the happy path, and the exception path with compensation.</p>"},{"location":"patterns/saga/#repositories-to-demonstrate-the-saga-patterns","title":"Repositories to demonstrate the Saga patterns","text":"<ul> <li> <p>eda-saga-choreography repository</p> <ul> <li>Order service</li> <li>Reefer service</li> <li>Voyage service</li> </ul> </li> <li> <p>eda-saga-orchestration repository includes subfolders with the 3 services.</p> </li> </ul>"},{"location":"patterns/topic-replication/","title":"Topic Replication","text":"<p>As an introduction to the scope of the data replication in the context of distributed system, we encourage to read our summary in this article.</p> <p>In this section we are going to address the data replication in the context of Kafka clusters, topic mirroring, using Kafka Mirror Maker 2.</p>"},{"location":"patterns/topic-replication/#problem-statement","title":"Problem statement","text":"<p>We suppose we need to replicate data in Kafka topics between different Kafka clusters running in different availability zones or data centers. There are multiple motivations for such replication. We can list at least the followings:</p> <ul> <li>Support disaster recovery, using different data centers to replicate microservice generated data in an active - passive mode.</li> <li>Move data closer to end user to improve performance and latency, this is more an active - active model, where the same microservices are deployed on the different data centers.</li> <li>The need to build on-premise data aggregation or data lake layer to perform batch analytics jobs from data gathered from different remote environments.</li> <li>Isolate secure data from on-premise cluster, with data encryption and data transformation to remove personal identifiable information, but still exchange such data between environments.</li> </ul>"},{"location":"patterns/topic-replication/#produce-to-two-data-centers","title":"Produce to two data centers","text":"<p>The simplest way to ensure data replication is to have producers writing to two data centers. When using Kafka the producer code needs to connect to two set brokers and write to the two target topics. The diagram below illustrates this approach:</p> <p></p> <p>The problem of this approach is the code needs to write in two clusters and address connectivity issue, and dual writes that may not happen in cohesive way.</p> <p>An alternate is to have the layer on top of the producers doing the replication itself so producers on both data centers are getting the same source of data and can inject on their related topic:</p> <p></p> <p>In this approach the producer code is classical and less intruisive to the type of deployment.</p>"},{"location":"patterns/topic-replication/#data-mirroring","title":"Data mirroring","text":"<p>Data mirroring is the technology to consume from one topic and publish to a remote topic, transparently to the core business application. This is an asynchronous mechanism, leveraging the append log mechanism from Kafka. MirrorMaker 2.0 is a new feature as part of Kafka 2.4 to support data replication between clusters. It is based on Kafka Connect framework, and it supports data and metadata replication, like the topic configuration, the offset and the consumer checkpoints are synchronously replicated to the target cluster. We have done extensive testing with MirrorMaker 2 with two different environments:</p> <ul> <li>One running a local, on-premise cluster using Kafka 2.5 open source packaging, like Strimzi vanilla Kafka, and IBM Event Streams on Cloud.</li> <li>One Cloud Pak for Integration Event Streams deployment to  IBM Event Streams on Cloud - managed service.</li> </ul> <p></p> <p>In the figure above, Mirror Maker 2.0 is used to do bi-directional replication between topics defined in both clusters. The grey topic is replicated from right to left. The light blue topic is replicated from left to right. Microservices on both data centers can consume messages from both topics.</p> <p>Then we want to address two main scenarios:</p> <ul> <li>Active - passive, which addresses a disaster recovery approach where consumers reconnect to a new cluster after the first one fails.</li> <li>Active - active deployments where participant clusters have producers and consumers and some topics are replicated so the messages are processed by different consumers on the two data centers.</li> </ul> <p>We address the active active scenarios for the two test environments in a separate note</p>"},{"location":"patterns/topic-replication/#active-passive","title":"Active - Passive","text":"<p>To support active - passive, one site has producers and consumers on local topics, and topic data are replicated to the passive cluster without online consumers. The tricky parts for this approach is the retention period in each topic, and then the backend systems impacted by the recovery. So the data replication from active - passive, embraces a larger scope than just topic mirroring. Still knowing how MirrorMaker 2 is doing offset replication is important.</p> <p>MirroMaker 2 tracks offset per consumer group. There are two topics created on the target cluster to manage the offset mapping between the source and target clusters and the checkpoints of the last committed offset in the source topic/partitions/consumer group. When a producer sends its record it gets the offset in the partition the record was saved.</p> <p>In the diagram below we have a source topic A - partition 1 with the last write offset done by a producer to be 5, and the last read committed offset by the consumer assigned to partition 1 being 3. The last replicated offset 3 is mapped as 12 in the target partition on the remote topic. offset # do not match between mirrored partitions. So if the consumer needs to reconnect to the target cluster, it will read from the last committed offset which is 12 in this environment. This information is saved in the <code>checkpoint</code> topic.</p> <p></p> <p>Offset synch are emitted at the beginning of the replication and when there is a situation which leads that the numbering sequencing diverges. For example the normal behavior increases the offset by one 2,3,4,5,6,7 which is mapped to the offsets 12,13,14,15,16,... If the write operation for offset 20 at the source is a 17 on the target then MM 2 emits a new offset synch records to the <code>offset-synch</code> topic, so the offset replication is not done all the time, only when diverging to improve performance.</p> <p>The <code>checkpoint</code> and <code>offset_synch</code> topics enable replication to be fully restored from the correct offset position on failover.</p> <p></p>"},{"location":"patterns/topic-replication/#active-active","title":"Active - Active","text":"<p>For active/ active producers are writing to their topics locally to the local cluster on both data centers. The code is the same, and deployed in both environments. The main consideration is what is their own datasource. If for example the datasource is a click stream data from webapp running locally with different end users, then the local consumer may do something specific for this data center, that the consumer on the other side do not need to know and process. So the consumers that really want to consume from local topic and replicated one, have very special requirements, for example, computing aggregates and joins. In this case the processing is active - active with different semantic. The other example is when we want to have the same data eventually consistent on the backend, the consumer are the same, writing to the same backend system, on both side, but consuming from two different topics. The following diagram illustrates this approach. The data in both backend will be eventual consistent overtime.</p> <p></p>"},{"location":"scenarios/overview/","title":"Event-Driven Architecture Scenarios","text":""},{"location":"scenarios/overview/#kafka-connect-scenarios","title":"Kafka Connect Scenarios","text":"Scenario Description Link Realtime Inventory An end-to-end data pipeline lab scenario, connecting multiple components of a realtime inventory system via Kafka Connect. Scenarios - Realtime Inventory"},{"location":"scenarios/overview/#reference-implementations","title":"Reference Implementations","text":"Scenario Description Link Shipping fresh food over sea (external) The EDA solution implementation using event driven microservices in different language, and demonstrating different design patterns. EDA reference implementation solution Vaccine delivery at scale (external) An EDA and cross cloud pak solution Vaccine delivery at scale Real time anomaly detection (external) Develop and apply machine learning predictive model on event streams Refrigerator container anomaly detection solution Real time inventory (external) Develop a real time inventory from sale events with Kafka streams. (GitOps) RT inventory"},{"location":"scenarios/saga-orchestration/","title":"SAGA Orchestration, powered by IBM MQ and  Event Streams","text":"<p>An IBM Cloud Pak for Integration - Event Streams use case</p> <p> Updated 02/22/2022 - Work in progress </p>"},{"location":"scenarios/saga-orchestration/#introduction","title":"Introduction","text":"<p>The Saga pattern helps to support a long running transaction that can be broken up to a collection of sub transactions that  can be interleaved any way with other transactions. The SAGA orchestration is done by the order service that sends commands to drive each SAGA  participant on what to do and when. To support strong consistency and exactly once delivery we are using Queues managed by IBM MQ. The Saga will be started by adding a new order, or updating major characteristics of an existing order.  The demonstration illustrates the happy path, where each participants respond positively, and one incomplete path, where the order will not  be satisfied because of lack of refrigerator containers. So the compensation logic will roll back the Vessel assignment.</p> <p>The following figure illustrates the component of this demonstration:</p> <p></p> <ul> <li>The order microservice implements the SAGA orchestration, and Create Read operations for the Order Entity.</li> <li>The voyage microservice implements one of the participant of the SAGA and manages Vessel itineraries.  This project illustrates how you can use the AMQP JMS client from Apache Qpid to interact with IBM MQ AMQP 1.0 servers in a Quarkus application using the Quarkus Qpid JMS extension.</li> <li>The reefer microservice</li> </ul>"},{"location":"scenarios/saga-orchestration/#use-case-guided-tour","title":"Use Case Guided Tour","text":""},{"location":"scenarios/saga-orchestration/#full-demo-narration","title":"Full Demo Narration","text":""},{"location":"scenarios/saga-orchestration/#developer-corner","title":"Developer Corner","text":""},{"location":"snippets/","title":"Snippets","text":"<p>This folder is for commonly used bits of information that need a single source of truth. Some examples include:</p> <ol> <li>Code blocks that may be used in several locations</li> <li>Topics that may need to be included in multiple places (e.g. \"create a service\")</li> <li>Use your best judgement, wherever content needs to be re-used easily across the site</li> </ol> <p>For information on how to use Snippets, see the pymdown-extension documentation</p> <p>As of 09/08/2022 we are not using this capability yet.</p>"},{"location":"technology/advanced-kafka/","title":"Kafka Advanced Concepts","text":""},{"location":"technology/advanced-kafka/#high-availability","title":"High Availability","text":"<p>As a distributed cluster, Kafka brokers ensure high availability to process records.  Topic has replication factor to support not loosing data in case of broker failure.  You need at least 3 brokers to ensure availability and a replication factor set to 3 for each  topic, so no data should be lost. But for production deployment, it is strongly recommended to use 5 brokers cluster to ensure the quorum is always set,  even when doing product upgrade. Replica factor can still be set to 3.</p> <p>The brokers need to run on separate physical machines, and when cluster extends over  multiple availability zones, a rack awareness  configuration can be defined. The rack concept can represent an availability zone, data center,  or an actual rack in your data center. Enabling rack awareness is done in a cluster definition (see Strimzi):</p> <pre><code>spec:\n  kafka:\n    rack:\n      topologyKey: topology.kubernetes.io/zone\n</code></pre> <p>For Zookeeper 3,5,7 nodes are needed depending of the number of objects to manage. You can start with 3 and add nodes overtime.</p>"},{"location":"technology/advanced-kafka/#replication-and-partition-leadership","title":"Replication and partition leadership","text":"<p>Partition enables data locality, elasticity, scalability, high performance, parallelism,  and fault tolerance. Each partition is replicated at least 3 times and allocated in different  brokers. One replicas is the leader. In the case of broker failure (broker 1 in figure below),   one of the existing replica in the remaining running brokers will take the leader role as soon   as the broker is identified as not responding:</p> <p></p> <p>The record key in each record sent by producer, determines the partition allocation in Kafka:  the records with the same key (hashcode of the key to be exact) will be in the same partition  (It is possible to use more complex behavior via configuration and even code).</p> <p>As Kafka is keeping its cluster states in Apache Zookeeper, you also  need to have at least a three node cluster for zookeeper.  From version 2.8, it is possible to do not use Zookeeper, but not yet for production deployment:  the brokers are using a Raft quorum algorithm, and an internal topic to keep state and metadata (See the process.roles  property from the broker configuration).</p> <p>Writes to Zookeeper are only be performed on changes to the membership of consumer groups or on changes to the Kafka cluster topology itself. Assuming you are using the most recent Kafka version, it is possible to have a unique zookeeper cluster for multiple kafka clusters. But the latency between Kafka and Zookeeper needs to be under few milliseconds (&lt; 15ms) anyway.</p> <p>Zookeepers and Brokers should have high availability communication via dual network, and each broker and node allocated  on different racks and blades.</p> <p></p> <p>To contact the cluster, consumers and producers are using a list of bootstrap server names (also named <code>advertiser.listeners</code>). The list is used for cluster discovery, it does not need to keep the full set of server names or IP addresses. A Kafka cluster has exactly one broker that acts as the controller.</p> <p>Per design Kafka aims to run within a single data center. But it is still recommended to use multiple racks connected with low latency dual networks. With multiple racks you will have better fault tolerance, as one rack failure will impact only one broker. There is a configuration property  to assign kafka broker using rack awareness. (See the broker configuration from the product documentation).</p> <p>As introduced on the topic introduction section, data are replicated between brokers. The following diagram illustrates  the best case scenario where followers fetch data from the partition leader,  and acknowledge the replications:</p> <p></p> <p>Usually replicas is done in-sync, and the configuration settings specify the number of <code>in-sync</code> replicas needed:  for example, a replica 3 can have a minimum in-sync of 2, to tolerate 1 (= 3-2) out of sync replica (1 broker outage).</p> <p>The leader maintains a set of in-sync-replicas (ISR) broker list: all the nodes which are up-to-date with the leader\u2019s  log, and actively acknowledging new writes. Every write goes through the leader and is propagated to every node in the In Sync Replica set.</p> <p>Followers consume messages from the leader just as a normal Kafka consumer would and apply them to their own log. Having the followers pull from the leader has the nice property of allowing the follower to naturally batch together log entries they are applying to their log.</p> <p>Once all nodes in the ISR have acknowledged the request, the partition leader broker considers it committed,  and can acknowledge to the producer.</p> <p>A message is considered committed when all in-sync replicas for that partition have applied this message to their log.</p> <p>If a leader fails, followers elect a new one. The partition leadership is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition.  The Kafka client library automatically reconnects to the new leader, although you will see  increased latency while the cluster settles: later  we will see this problem with partition rebalancing. Any replica in the ISR is eligible to be elected leader.</p> <p></p> <p>When a leader waits to get acknowledge before committing a message there will be more potential leaders. With (#failure + 1) replicas there is no data lost. But there is a risk of having the single broker separated from the Zookeeper cluster when network partition occurs.  To tolerate\u00a0f\u00a0failures, both the majority  vote and the ISR approach will wait for the same number of replicas to acknowledge before committing a message.</p> <p>Having higher replicas number like 5, will duplicate 5 times the data (more disk used) and will  impact throughput as data is sent 1+4 times over the network. Get acknowledgement will take a little bit more time too.</p> <p>Another important design distinction is that Kafka does not require that crashed nodes recover with all their data intact.\u00a0 Kafka protocol by allowing a replica to rejoin the ISR, ensures that before rejoining, it must fully re-sync again even if it lost unflushed data in its crash.</p> <p>When a producer sends message, it can control how to get the response from the committed message:  wait for all replicas to succeed, wait for one acknowledge, fire and forget. Consumers receive only committed messages.</p> <p>Always assess the latency requirements and consumers needs. Throughput is linked to the number  of partitions within a topic and having more consumers running in parallel. Consumers and producers should better run on separate servers than the brokers nodes.  Running in parallel, also means the order of event arrivals will be lost. Most of the time, consumers are processing events from a unique partition and Kafka records  to partition assignment will guarantee that records with the same key hashcode will   be in the same partition. So orders are preserved within a partition.   But if consumer needs to read from multiple partitions and if ordered records is needed,   the consumer needs to rebuild the order by implementing adhoc logic, based on time stamp.</p> <p>For high availability, assess any potential single point of failure, such as server,  rack, network, power supply... We recommend reading  this event stream article for planning your kafka on Kubernetes installation.</p> <p>For the consumers code maintenance, the re-creation of the consumer instance within the consumer  group will trigger a partition rebalancing. This includes all the state of the aggregated data  calculations that were persisted on disk. Until this process is finished real-time events are  not processed. It is possible to limit this impact by setting the <code>group.initial.rebalance.delay.ms</code>  to delay the rebalancing process once one instance of the consumer dies.  Nevertheless the rebalancing will still occur when the updated consumer will rejoin the consumer  group. </p> <p>When consumers are stream processing using Kafka streams, it is important to note that  during the rollover the downstream processing will see a lag in event arrival:  the time for the consumer to reread from the last committed offset. So if the end-to-end timing  is becoming important, we need to setup a standby consumer group (group B).  This consumer group has different name, but does the same processing logic, and is consuming  the same events from the same topic as the active consumer group (group A).  The difference is that they do not send events to the downstream topic until they are set  up active. So the process is to set group B active while cluster A is set inactive.  The downstream processing will not be impacted. Finally to be exhaustive, the control of the segment size for the change log topic, may be considered to  avoid having the stream processing doing a lot of computation to reload its state when it restarts.</p> <p>To add new broker, you can deploy the runtime to a new server / rack / blade, and give a  unique broker ID. Broker will process new topic, but it is possible to use thr kafka-reassign-partitions  tool to migrate some existing  topic/ partitions to the new server. The tool is used to reassign partitions across brokers.  An ideal partition distribution would ensure even data load and partition sizes across all brokers.</p>"},{"location":"technology/advanced-kafka/#high-availability-in-the-context-of-kubernetes-deployment","title":"High Availability in the context of Kubernetes deployment","text":"<p>The combination of Kafka with Kubernetes seems to be a sound approach, but it is not that easy to achieve.  Kubernetes workloads prefer to be stateless, Kafka is a stateful platform and manages its own brokers, and replications across known servers.  It knows the underlying infrastructure. In Kubernetes, nodes and pods may change dynamically. Clients need to be able to access each of the broker  directly once they get the connection metadata. Having a kubernetes service object which will round robin across all brokers in the cluster will not work with Kafka.</p> <p>The figure below illustrates a Kubernetes deployment, where Zookeeper and kafka brokers are allocated to 3 worker nodes,  with some event driven microservices deployed in separate worker nodes. Those microservices are consumers and producers of events from one  to many topics.</p> <p></p> <p>The advantages of deploying Kafka on Kubernetes cluster is to facilitate the management of  stateful sets, by scheduling both the persistence volume and broker pods in a clean rolling rehydration.  Services add a logical name to access brokers for any deployed workload within the cluster.  The virtual network also enables transparent TLS communication between components.</p> <p>For any Kubernetes deployment real high availability is constrained by the application / workload  deployed on it. The Kubernetes platform supports high availability by having at least the following configuration:</p> <ul> <li>At least three master nodes (always an odd number of nodes). One is active at master, the others are in standby. The election of the master is using the quorum algorithm.</li> <li>At least three worker nodes, but with Zookeeper and Kafka clusters, we may need to have at least three more nodes as we do not want to have Zookeeper and Kafka brokers  sharing the same host with other pods if the Kakfa traffic is supposed to grow.</li> <li>Externalize the management stack to three manager nodes</li> <li>Shared storage outside of the cluster to support private image registry, audit logs, and statefulset data persistence (like the Kakfa broker file systems).</li> <li>Use <code>etcd</code> cluster: See recommendations from this article.  The virtual IP manager assigns virtual IP addresses to master and proxy nodes and monitors the health of the cluster.  It leverages <code>etcd</code> for storing information, so it is important that <code>etcd</code> is high available too and connected to low latency network below 10ms.</li> </ul> <p>Traditionally disaster recovery and high availability were always consider separated subjects.  Now active/active deployment where workloads are deployed in different data centers,  is becoming a common IT's request.</p> <p>For sure, you need multiple Kafka Brokers, which will connect to the same ZooKeeper Ensemble running  at least five nodes (you can tolerate the loss of one server during the planned maintenance of another server). One Zookeeper server acts as a lead and the two others as stand-by.</p> <p>The diagram above illustrates a simple deployment where Zookeeper servers and Kafka brokers are running in pods, in different worker nodes. It is a viable solution to start deploying solution on top of Kafka. When you have bigger cluster, it may be interesting to separate Zookeeper from Kafka nodes to limit the risk of failover, as Zookeeper keeps state of the Kafka cluster topology and metadata. You will limit to have both the Zookeeper leader and one Kafka broker dying at the same time. We use Kubernetes anti-affinity to ensure they are scheduled onto separate worker nodes that the ones used by Zookeeper. It uses the labels on pods with a rule like: <code>**Kafka** pod should not run on same node as zookeeper pods</code>.</p> <p>Here is an example of such spec:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: with-pod-affinity\nspec:\n  affinity:\n    podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            labelSelector:\n            matchExpressions:\n            - key: name\n              operator: In\n              values:\n              - gc-zookeeper\n          topologyKey: kubernetes.io/hostname\n</code></pre> <p>We recommend reading the \"running zookeeper in k8s tutorial\" for understanding such configuration.</p> <p>For optimum performance, provision a fast storage class for persistence volume.</p> <p>Kafka uses the <code>log.dirs</code> property to configure the driver to persist logs. So you need to define multiple volumes/ drives to support <code>log.dirs</code>.</p> <p>Zookeeper should not be used by other applications deployed in k8s cluster, it has to be dedicated for one Kafka cluster only.</p> <p>In a multi-cluster configuration being used for disaster recovery purposes, messages sent between clusters will have different offsets in the two clusters. It is usual to use timestamps for position information when restarting applications for recovery after a disaster.</p> <p>For Kafka streaming with stateful processing like joins, event aggregation and correlation coming from multiple partitions, it is not easy to achieve high availability cross clusters: in the strictest case every event must be processed by the streaming service exactly once. Which means:</p> <ul> <li>producer emits data to different sites and be able to re-emit in case of failure. Brokers are known by producer via a list of hostnames and port numbers.</li> <li>communications between Zookeepers and cluster nodes are redundant and safe for data losses</li> <li>consumers ensure idempotence... They have to tolerate data duplication and manage data integrity in their persistence layer.</li> </ul> <p>Within Kafka's boundary, data will not be lost, when doing proper configuration, also to support high availability the complexity moves to the producer and the consumer implementation.</p> <p>Kafka configuration is an art and you need to tune the parameters by use case:</p> <ul> <li>Partition replication for at least 3 replicas. Recall that in case of node failure,  coordination of partition re-assignments is provided with ZooKeeper.</li> <li>End to end latency needs to be measured from producer (when a message is sent) to consumer (when it is read). A consumer is able to get a message when the brokers finish replicating to all in-synch replicas.</li> <li>Use the producer buffering capability to pace the message to the broker. Can use memory or time based threshold via producer properties.</li> <li>Define the number of partitions to drive consumer parallelism. More consumers running in parallel the higher is the throughput. When using multiple partitions the global ordering of message is lost.</li> <li>Assess the retention hours to control when old messages in topic can be deleted. It is possible to keep messages forever, and for some application it makes fully sense.</li> <li>Control the maximum message size the server can receive.</li> </ul> <p>Zookeeper is not CPU intensive and each server should have a least 2 GB of heap space and 4GB reserved. Two CPUs per server should be sufficient. Servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on persistent storage. To prevent the WAL from growing without bound, ZooKeeper servers periodically snapshot their in memory state to storage. Use fast and dynamically provisioned persistence storage for both WAL and snapshot.</p>"},{"location":"technology/advanced-kafka/#performance-considerations","title":"Performance Considerations","text":"<p>Performance will vary depending of the current Kafka broker nodes load: in Kubernetes deployment, with small production topology, nodes may be shared with other pods.  It is recommended to control the environment with dedicated nodes for Kafka to achieve higher throughput. Performance will always depend on numerous factors including message throughput, message size, hardware, configuration settings, ...</p> <p>Performance may be linked to different focuses:</p> <ul> <li>Resilience: ensuring replication and not loosing data</li> <li>Throughput: ensuring message processing performance</li> <li>Payload size: support larger message</li> </ul>"},{"location":"technology/advanced-kafka/#resilience","title":"Resilience","text":"<p>When defining a topic, we need to specify the replicas factor to match the be at least 3 and then set the minimum number of in-sync replicas that specifies how may replicas must acknowledge a write to satisfy a producer that requests acknowledgments from all replicas. (<code>min.insync.replicas</code>).</p> <p>The replication of message data between brokers can consume a lot of network bandwidth so isolating replication traffic from application traffic can benefit performance. To achieve this, all replication traffic is configured to flow on a dedicated internal network.</p>"},{"location":"technology/advanced-kafka/#throughput","title":"Throughput","text":"<p>To achieve higher throughput the messages are not replicated across brokers and the acknowledgement can be set to only one broker. Expose resiliency to failures.</p> <p>The number of producers and consumers are aligned, and the number of partitions matches the number of consumers. All consumers are in the same consumer group. Measurement has to be done from the producer code. With 12 producers on a 3 brokers cluster and small payload (128 bytes), with 24 consumers the measured throughput is around 2.3 M messages / second.</p>"},{"location":"technology/advanced-kafka/#payload-size","title":"Payload size","text":"<p>From measurement tests done using Kafka producer performance tool, there is a 1/log(s) curve, where below 10k bytes the performances are correct and then slowly degrade from 3000 msg /s (10k bytes msg) to 65 msg/s (515kb msg).</p> <p>To do performance test the event-streams-sample-producer github provides producer tool in Java, using a group of threads to run in multi cores machine. This project can be dockerized, and deployed in k8s. It uses the Kafka tool named: <code>ProducerPerformance.java</code> in the jar:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n  &lt;artifactId&gt;kafka-tools&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"technology/advanced-kafka/#parameter-considerations","title":"Parameter considerations","text":"<p>There are a lot of factors and parameters that needs to be tuned to improve performance at the brokers  threading level (<code>num.replica.fetchers, num.io.threads, num.network.threads, log.cleaner.threads</code> ) and the  pod resources constraints. See the configuration documentation.</p>"},{"location":"technology/advanced-kafka/#openshift-specifics","title":"Openshift specifics","text":"<p>When exposing the Kafka broker via Routes, the traffic is encrypted with TLS, so client needs to deal with TLS  certificates and encryption. Routes are exposed via DNS and HAProxy router. The router will act as middleman  between Kafka clients and brokers, adding latency, and it can become bottleneck. The traffic generated by  client needs to be sized and in case of the router needs to be scaled up, and even isolate the routing  by adding a separate router for the Kafka routes.</p>"},{"location":"technology/advanced-kafka/#disaster-recovery","title":"Disaster Recovery","text":"<p>With the current implementation it is recommended to have one cluster per data center / availability zone.  Consumers and producers are co-located to the brokers cluster. When there are needs to keep some part of the  data replicated in both data centers, you need to assess what kind of data can be aggregated, and if Kafka  mirroring tool can be used. The tool consumes from a source cluster, from a given topic, and produces  to a destination cluster with the same named topic. It keeps the message key for partitioning, so order is preserved.</p> <p></p> <p>The above diagram is using Kafka MirrorMaker 2 with a master to slave deployment. Within the data center 2,  the brokers are here to manage the topics and events. When there is no consumer running, nothing happen.  Consumers and producers can be started when DC1 fails. This is the active/passive model. In fact,  we could have consumers within the DC2 processing topics to manage a read-only model, keeping in memory  their projection view, as presented in the CQRS pattern.</p> <p>The second solution is to use one MirrorMaker 2 in each site, for each topic. This is an active - active  topology: consumers and producers are on both sites. But to avoid infinite loop, we need to use naming convention  for the topic, or only produce in the cluster of the main topic. Consumers consume from the replicated topic.</p> <p></p> <p>When you want to deploy solution that spreads over multiple regions to support global streaming, you need  to address the following challenges:</p> <ul> <li>How do you make data available to applications across multiple data centers?</li> <li>How to serve data closer to the geography?</li> <li>How to be compliant on regulations, like GDPR?</li> <li>How to address no duplication of records?</li> </ul> <p>Kafka 2.4 introduces the capability  for a consumer to read messages from the closest replica using some rack-id and specific algorithm.  This capability will help to extend the cluster to multiple data center and avoid having consumers going over  WAN communication.</p>"},{"location":"technology/advanced-kafka/#solution-considerations","title":"Solution Considerations","text":"<p>There are a set of design considerations to assess for each Kafka solution:</p>"},{"location":"technology/advanced-kafka/#topics","title":"Topics","text":"<p>Performance is more a function of number of partitions than topics. Expect that each topic has at least one partition. When considering latency you should aim for limiting to hundreds of topic-partition per broker node.</p> <p>What of the most important question is what topics to use?. What is an event type? Should we use one topic to support multiple event types? Let define that an event type is linked to a main business entity like an Order, a ship, a FridgeredContainer. OrderCreated, OrderCancelled, OrderUpdated, OrderClosed are events linked to the states of the Order. The order of those events matter. So the natural approach is to use one topic per data type or schema, specially when using the topic as Event Sourcing where event order is important to build the audit log. You will use a unique partition to support that. The orderID is the partition key and all events related to the order are in the same topic.</p> <p>The important requirement to consider is the sequencing or event order. When event order is very important then use a unique partition, and use the entity unique identifier as key. Ordering is not preserved across partitions.</p> <p>When dealing with entity, independent entities may be in separate topics, when strongly related one may stay together.</p> <p>Other best practices:</p> <ul> <li>When event order is important use the same topic and use the entity unique identifier as partition key.</li> <li>When two entities are related together by containment relationship then they can be in the same topic.</li> <li>Different entities are separated to different topics.</li> <li>It is possible to group topics in coarse grained one when we discover that several consumers are listening to the same topics.</li> <li>Clearly define the partition key as it could be an compound key based on multiple entities.</li> </ul> <p>With Kafka stream, state store or KTable, you should separate the changelog topic from the others.</p>"},{"location":"technology/advanced-kafka/#producers","title":"Producers","text":"<p>When developing a record producer you need to assess the following:</p> <ul> <li>What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size.</li> <li>Can the producer batch events together to send them in batch over one send operation?</li> <li>Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size</li> <li>Assess once to exactly once delivery requirement. Look at idempotent producer.</li> </ul> <p>See implementation considerations discussion</p>"},{"location":"technology/advanced-kafka/#consumers","title":"Consumers","text":"<p>From the consumer point of view a set of items need to be addressed during design phase:</p> <ul> <li>Do you need to group consumers for parallel consumption of events?</li> <li>What is the processing done once the record is processed out of the topic? And how a record is supposed to be consumed?.</li> <li>How to persist consumer committed position? (the last offset that has been stored securely)</li> <li>Assess if offsets need to be persisted outside of Kafka?. From version 0.9 offset management is more efficient, and synchronous or asynchronous operations can be done from the consumer code.</li> <li>Does record time sensitive, and it is possible that consumers fall behind, so when a consumer restarts he can bypass missed records?</li> <li>Do the consumer needs to perform joins, aggregations between multiple partitions?</li> </ul> <p>See implementation consideration discussion</p>"},{"location":"technology/avro-schemas/","title":"Apache Avro, Data Schemas and Schema Registry","text":"<p>Updates 6/20/2022</p> <p>This chapter describes what and why Avro and Schema registry are important elements of any event-driven solutions.</p>"},{"location":"technology/avro-schemas/#why-this-is-important","title":"Why this is important","text":"<p>Loosely coupling and asynchronous communication between applications does not mean there is no contract to enforce some constraint between producer and consumer.  When we talk about contract we can first think about schema as we did with XSD. In the world of JSON, JSON schema and Avro schemas can be used to define data structure of the message. As there is a need to get metadata around messaging, cloudevents is well accepted and adopted as a specification to describe event data. Also AsyncAPI establishes standards for events and messaging in the asynchronous world with an API view, so combining message schema, channels and binding definitions so  we have most of the needed information for a consumer to access a data stream or queue. </p> <p>So the contract is defined with a schema. From an EDA design point of view, the producer owns the definition of the schema as it owns the main business entity life cycle, events are generated from. Producer will make sure the message complies with the schema at hand for serializing.</p> <p>On top of those specifications, there are technologies to support the contract management in the form of schema registry and API manager. The following figure gives us the foundations for integration between producer, schema registry and consumers.</p> <p></p> <p>Schema Registry provides producer and consumer APIs so that these can project whether the event they are about to produce or consume is compatible with previous versions or compatible with the version they are expecting. For that, both producers and consumers require the schema definition at hand at serialization and deserialization time. This can be done either by:</p> <ol> <li>Reading the schema from a local local resource to your producer such as a file, variable, property (or kubernetes construct such as configmap or secret).</li> <li>Retrieving the schema definition from the Schema Registry given a name/ID.</li> </ol> <p>When the producer wants to send an event to a Kafka topic, two things happen:</p> <ol> <li>The producer makes sure the event to be sent complies to the schema. Otherwise, it errors out.</li> <li>Project whether the event they are about to produce or consume is compatible with previous versions or compatible with the version they are expecting.</li> </ol> <p>For the first action, the producer already holds the two things it needs: the schema definition the event to be sent needs to comply with and the event itself. The producer is therefore able to carry out that compliance check. However, for the second action, where the producer needs to make sure the schema definition your event complies with is compatible with the existing schema definition for the topic in question (if any), the producer might need to contact the schema registry.</p> <p>Producers (and consumers) maintain a local cache with the schema definitions (and their versions) along with their unique global IDs (all of these retrieved from the schema registry) for the topics they want to produce/consume events to/from. If the producer has a schema definition in its local cache that matches the schema definition at hand for the serialization of the event, it simply sends the event as the schema definitions match and the event complies with such schema definition. However, if the producer does not have any schema definition in its local cache that matches the schema defintion at hand for the serialization of the event, whether because these are different versions or the producer simply does not have any schema definition in its cache, it contacts the schema registry.</p> <ul> <li> <p>If a schema defintion for the topic in question that matches the schema definition at hand at serialization time in the producer already exists in the schema registry, the producer simply retrieves the global unique ID for that schema definition to locally cache it along with the schema definition itself for future events to be sent so that it avoids contacting the schema registry again.</p> </li> <li> <p>If a schema definition for the topic in question that matches the schema definition at hand at serialization time in the producer does not already exist in the schema registry, the producer must be able to register the schema definition it has got at hand for the topic in question at serialization time in the schema registry so that such schema definition is now available for any consumer wanting or needing to consume events that comply with such schema definition. For that, the producer must be configured to be able to auto-register schemas and also be provided with the appropriate credentials to register schema definitions from the schema registry perspective if this implements any type of RBAC mechanism (as it is the case for IBM Event Streams).</p> </li> <li>If the producer is not configured to auto-register schema definitions or its credentials does not allow it to register schema definitions, then the send event action will fail until there is a schema definition for the topic in question that mateches the schema definition at hand at serialization time in the producer registered in the schema registry.</li> <li>If the producer is configured to auto-register schema definitions and its credentials allows it to register schema definitions, the schema registry validates the compatibility of the schema definition at hand at serialization time in the producer with existing schema definitions for the topic in questions (if any) to make sure that, if this schema definition to be registered is a newer version of any existing schema definition, it is complatible so that no consumer gets affected by this new schema definition version. Afer the schema definition or newer version of an exisint schema definition is registered in the schema registry, the producer retrieves its global unique ID to mantain its local cache up to date.</li> </ul> <p>Warning</p> <p>We stongly recommend that producers are not allowed to register schema definitions in the schema registry for better governance and management of schema definitions. It is highly recommended that schema definitions registration is done by someone responsible for such task with appropriate role (such as an admin, an API manager, an asynchronous API manager, a development manager, an operator, etc).</p> <p>Once the producer has a schema definition in its local cache, along with its global unique id, that matches the schema definition at hand for serialization of the event to be sent, it produces the event to the Kafka topic using the appropriate <code>AvroKafkaSerializer</code> class. The schema definition global unique ID gets serialized along with the event so that the event does not need to travel along with its schema definition as it was the case in older messaging or eventing techonolgies and systems.</p> <p>By default, when a consumer reads an event, the schema definition for such event is retrieved from the schema registry by the deserializer using the global unique ID, which is specified in the event being consumed. The schema definition for an event is retrieved from the schema registry only once, when an event comes with a global unique ID that can not be found in the schema definition cache the consumer maintains locally. The schema definition global unique ID can be located in the event headers or in the event payload, depending on the configuration of the producer application. When locating the global unique ID in the event payload, the format of the data begins with a magic byte, used as a signal to consumers, followed by the global unique ID, and the message data as normal. For example:</p> <pre><code># ...\n[MAGIC_BYTE]\n[GLOBAL_UNIQUE_ID]\n[MESSAGE DATA]\n</code></pre> <p>Be aware that non Java consumers use a C library that might require the schema definition at hand for the deserializer too (see https://github.com/confluentinc/confluent-kafka-python/issues/834) as opposed to letting the deserializer retrieve the schema definition from the schema registry as explained above. The strategy would be to have the schema loaded from the schema registry via API.</p>"},{"location":"technology/avro-schemas/#schema-registry","title":"Schema Registry","text":"<p>With a pure open-source strategy, Event Streams within Cloud Pak for integration is using Apicu.io as schema registry.  The Event Streams product documentation is doing an excellent job to  present the schema registry, we do not need to rewrite the story, just give you some summary from Apicur.io and links to code samples.</p>"},{"location":"technology/avro-schemas/#apicurio","title":"Apicurio","text":"<p>Apicur.io includes a schema registry to store schema definitions.  It supports Avro, json, protobuf schemas, and an API registry to manage OpenApi and AsynchAPI.</p> <p>It is a Cloud-native Quarkus Java runtime for low memory footprint and fast deployment times. It supports different persistences like Kafka, Postgresql, Infinispan and supports different deployment models.</p>"},{"location":"technology/avro-schemas/#registry-characteristics","title":"Registry Characteristics","text":"<ul> <li>Apicurio Registry is a datastore for sharing standard event schemas and API designs across API and event-driven architectures. In the messaging and event streaming world, data that are published to topics and queues often must be serialized or validated using a Schema.</li> <li>The registry supports adding, removing, and updating the following types of artifacts: OpenAPI, AsyncAPI, GraphQL, Apache Avro, Google protocol buffers, JSON Schema, Kafka Connect schema, WSDL, XML Schema (XSD).</li> <li>Schema can be created via Web Console, core REST API or Maven plugin</li> <li>It includes configurable rules to control the validity and compatibility.</li> <li>Client applications can dynamically push or pull the latest schema updates to or from Apicurio Registry at runtime. Apicurio is compatible with existing Confluent schema registry client applications.</li> <li>It includes client serializers/deserializers (Serdes) to validate Kafka and other message types at runtime.</li> <li>Operator-based installation of Apicurio Registry on OpenShift</li> <li>Use the concept of artifact group to collect schema and APIs logically related.</li> <li>Support search for artifacts by label, name, group, and description</li> </ul> <p>When using Kafka as persistence, special Kafka topic <code>&lt;kafkastore.topic&gt;</code> (default <code>_schemas</code>), with a single partition, is used as a highly available write ahead log.  All schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log.  A Schema Registry instance therefore both produces and consumes messages under the <code>_schemas</code> topic.  It produces messages to the log when, for example, new schemas are registered under a subject, or when updates to  compatibility settings are registered. Schema Registry consumes from the <code>_schemas</code> log in a background thread, and updates its local  caches on consumption of each new <code>_schemas</code> message to reflect the newly added schema or compatibility setting.  Updating local state from the Kafka log in this manner ensures durability, ordering, and easy recoverability.</p> <p>The way Event Streams / Apicur.io has to handle schema association to topics is by schema name. Given we have a topic called orders,  the schemas that will apply to it are avros-key (when using composite key) and orders-value (most likely based on cloudevents and then custom payload).</p>"},{"location":"technology/avro-schemas/#apache-avro","title":"Apache Avro","text":"<p>Avro is an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice.</p>"},{"location":"technology/avro-schemas/#why-apache-avro","title":"Why Apache Avro","text":"<p>There are several websites that discuss the Apache Avro data serialization system benefits over other messaging data protocols. A simple google search will list dozens of them. Here, we will highlight just a few from a Confluent blog post:</p> <ul> <li>It has a direct mapping to and from JSON</li> <li>It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage.</li> <li>It is very fast.</li> <li>It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream.</li> <li>It has a rich, extensible schema language defined in pure JSON</li> <li>It has the best notion of compatibility for evolving your data over time.</li> </ul>"},{"location":"technology/avro-schemas/#data-schemas","title":"Data Schemas","text":"<p>Avro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. This permits each datum to be written with no per-value overheads, making serialization both fast and small. An Avro schema defines the structure of the Avro data format.</p>"},{"location":"technology/avro-schemas/#how-does-a-data-schema-look-like","title":"How does a data schema look like?","text":"<p>Let's see how a data schema to define a person's profile in a bank could look like:</p> <pre><code>{\n  \"namespace\": \"banking.schemas.demo\",\n  \"name\": \"profile\",\n  \"type\": \"record\",\n  \"doc\": \"Data schema to represent a profile for a banking entity\",\n  \"fields \": [\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"surname\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"age\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"account\",\n      \"type\": \"banking.schemas.demo.account\"\n    },\n    {\n      \"name\": \"gender\",\n      \"type\": {\n        \"type\": \"enum\",\n        \"name\": \"genderEnum\",\n        \"symbols\": [\n          \"male\",\n          \"female\"\n        ]\n      }\n    }\n  ]\n}\n</code></pre> Notice: <ol> <li>There are primitive data types like <code>string</code> and <code>int</code> but also complex types like <code>record</code> or <code>enum</code>.</li> <li>Complex type <code>record</code> requires a <code>name</code> attribute but it also can go along with a <code>namespace</code> attribute which is a JSON string that qualifies the name.</li> <li>Data schemas can be nested as you can see for the <code>account</code> data attribute. See below.</li> </ol> <pre><code>{\n  \"namespace\": \"banking.schemas.demo\",\n  \"name\": \"account\",\n  \"type\": \"record\",\n  \"doc\": \"Data schema to represent a customer account with the credit cards associated to it\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"savings\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\": \"cards\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": \"int\"\n      }\n    }\n  ]\n}\n</code></pre> <p>In the picture below we see two messages, one complies with the above Apache Avro data schema and the other does not:</p> <p></p> <p>You might start realising by now the benefits of having the data flowing into your Apache Kafka event backbone validated against a schema. See next section for more.</p> <p>For more information on the Apache Avro Data Schema specification see https://avro.apache.org/docs/current/spec.html</p>"},{"location":"technology/avro-schemas/#benefits-of-using-data-schemas","title":"Benefits of using Data Schemas","text":"<ul> <li>Clarity and Semantics: They document the usage of the event and the meaning of each field in the \"doc\" fields.</li> <li>Robustness: They protect downstream data consumers from malformed  data, as only valid data will be permitted in the topic. They let the producers or consumers of data streams know the right fields are need in an event and what type each field is (contract for microservices).</li> <li>Compatibility: model and handle change in data format.</li> </ul>"},{"location":"technology/avro-schemas/#avro-kafka-and-schema-registry","title":"Avro, Kafka and Schema Registry","text":"<p>In this section we try to put all the pieces together for the common flow of sending and receiving messages through an event backbone  such as kafka having those messages serialized using the Apache Avro data serialization system and complying with their respective messages  that are stored and managed by a schema registry.</p> <p>Avro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. An Avro schema defines  the structure of the Avro data format. Schema Registry defines a scope in which schemas can evolve, and that scope is the subject.  The name of the subject depends on the configured subject name strategy, which by default is set to derive subject name from topic name.</p> <p>In this case, the messages are serialized using Avro and sent to a kafka topic. Each message is a key-value pair. Either the message key or the message value, or both, can be serialized as Avro. Integration with Schema Registry means that Kafka messages do not need to be written with the entire Avro schema. Instead, Kafka messages are written with the schema id. The producers writing the messages and the consumers reading the messages must be using the same Schema Registry to get the same mapping between a schema and schema id.</p>"},{"location":"technology/avro-schemas/#more-reading","title":"More reading","text":""},{"location":"technology/avro-schemas/#articles-and-product-documentation","title":"Articles and product documentation","text":"<ul> <li>IBM Event Streams-  Schemas overview</li> <li>Apicur.io schema registry documentation</li> <li>Confluent schema registry overview</li> <li>Producer code with reactive messaging and apicurio schema registry</li> <li>Consumer code with reactive messaging and apicurio schema registry</li> </ul>"},{"location":"technology/avro-schemas/#labs","title":"Labs","text":"<p>We have developed two labs, one for the IBM Event Streams product that comes with the IBM CloudPak for Integration installed on a RedHat OpenShift cluster and the other for the IBM Event Streams on IBM Cloud offering, to get hands-on experience working with Apache Avro, data schemas and the IBM Event Streams Schema Registry:</p> <ul> <li>IBM Event Streams on IBM Cloud lab</li> <li>IBM Event Streams from IBM CloudPak for Integration lab</li> </ul>"},{"location":"technology/event-streams/","title":"IBM Event Streams","text":"<p>To start playing with Event Streams we propose two set of labs:</p> <ol> <li>Use IBM Event Streams managed service on IBM Cloud</li> <li>Use Event Streams as part of the Cloud Pak for Integration</li> </ol>"},{"location":"technology/event-streams/#demonstrate-event-streams-on-openshift-from-a-to-z","title":"Demonstrate Event Streams on OpenShift from A to Z","text":"<p>Script in separate note</p>"},{"location":"technology/event-streams/#ibm-event-streams-within-the-ibm-cloud-pak-for-integration","title":"IBM Event Streams within the IBM Cloud Pak for Integration","text":"<p>The product documentation on how to install Event Streams on Openshift is where to go to get the last updates.</p> <ol> <li>Lab exercise: A detail step-by-step Event Streams installation on OpenShift</li> <li>Lab exercise: Demonstrate Starter Application</li> <li>Lab exercise: Schema Registry on OpenShift Container Platform</li> <li>Lab exercise: Monitoring IBM Event Streams on OpenShift Container Platform</li> </ol>"},{"location":"technology/event-streams/#ibm-event-streams-on-ibm-cloud","title":"IBM Event Streams on IBM Cloud","text":"<ol> <li>Lab exercise: Provision IBM Event Streams as Managed Services using IBM Cloud console</li> <li>Lab exercise: Addressing Security and access control</li> <li>Lab exercise: How to use Schema Registry with an entreprise plan</li> <li>Lab exercise: Monitoring IBM Event Streams on Cloud</li> </ol>"},{"location":"technology/event-streams/starter-app/","title":"Starter App","text":"<p>This section details walking through the generation of a starter application for usage with IBM Event Streams, as documented in the official product documentation.</p> <p>The Starter application is an excellent way to demonstrate sending and consuming messages. </p>"},{"location":"technology/event-streams/starter-app/#prepare-the-starter-app-configuration","title":"Prepare the starter app configuration","text":"<ol> <li>Log into the IBM Event Streams Dashboard, and from the home page, click the Try the starter application button from the Getting Started page</li> </ol> <ol> <li>Click Download JAR from GitHub. This will open a new window to https://github.com/ibm-messaging/kafka-java-vertx-starter/releases</li> </ol> <ul> <li>Click the link for <code>demo-all.jar</code> from the latest release available. At the time of this writing, the latest version was <code>1.0.0</code>.</li> </ul> <ol> <li>Return to the <code>Configure &amp; run starter application</code> window and click Generate properties.</li> </ol> <ol> <li>In dialog that pops up from the right-hand side of the screen, enter the following information:</li> <li>Starter application name: <code>starter-app-[your-initials]</code></li> <li>Leave New topic selected and enter a Topic name of <code>starter-app-[your-initials]</code>.</li> <li>Click Generate and download .zip</li> </ol> <pre><code>The figure above illustrates that you can download a zip file containing the properties of the application according to the Event-Streams cluster configuration, and a `p12` TLS certificate to be added to a local folder.\n</code></pre> <ol> <li> <p>In a Terminal window, unzip the generated ZIP file from the previous window and move <code>demo-all.jar</code> file into the same folder.</p> </li> <li> <p>Review the extracted <code>kafka.properties</code> to understand how Event Streams has generated credentials and configuration information for this sample application to connect.</p> </li> </ol>"},{"location":"technology/event-streams/starter-app/#run-starter-application","title":"Run starter application","text":"<ol> <li> <p>This Starter application will run locally to the user's laptop with the command:</p> <p><pre><code>java -jar target/demo-all.jar -Dproperties_path=./kafka.properties \n</code></pre> 1. As an alternate method, we have packaged this app in a docker image: <code>quay.io/ibmcase/es-demo</code></p> <pre><code>docker run -ti -p 8080:8080 -v  $(pwd)/kafka.properties:/deployments/kafka.properties -v  $(pwd)/truststore.p12:/deployments/truststore.p12  quay.io/ibmcase/es-demo\n</code></pre> </li> <li> <p>Wait until you see the string <code>Application started in X ms</code> in the output and then visit the application's user interface via <code>http://localhost:8080</code>.</p> </li> </ol> <p></p> <ol> <li> <p>Once in the User Interface, enter a message to be contained for the Kafka record value then click Start producing.</p> </li> <li> <p>Wait a few moments until the UI updates to show some of the confirmed produced messages and offsets, then click on Start consuming on the right side of the application.</p> </li> </ol> <p></p> <ol> <li>In the IBM Event Streams user interface, go to the topic where you send the messages to and make sure messages have actually made it.</li> </ol> <p></p> <ol> <li>You can leave the application running for the rest of the lab or you can do the following actions on the application</li> <li>If you would like to stop the application from producing, you can click Stop producing.</li> <li>If you would like to stop the application from consuming, you can click Stop consuming.</li> <li>If you would like to stop the application entirely, you can input <code>Control+C</code> in the Terminal session where the application is running.</li> </ol> <p>An alternative sample application can be leveraged from the official documentation to generate higher amounts of load.</p>"},{"location":"technology/event-streams/starter-app/#deploy-to-openshift","title":"Deploy to OpenShift","text":"<p>This application can also be deployed to OpenShift. Here are the steps:</p> <ol> <li> <p>Use the same <code>kafka.properties</code> and <code>truststore.p12</code> files you have downloaded with the starter  application to create two kubernetes secrets holding these files in your OpenShift cluster</p> <pre><code>oc create secret generic demo-app-secret --from-file=./kafka.properties\noc create secret generic truststore-cert --from-file=./truststore.p12\n</code></pre> </li> <li> <p>Clone the following GitHub repo that contains the Kubernetes artifacts that will run the starter application.</p> <pre><code>git clone https://github.com/ibm-cloud-architecture/eda-quickstarts.git\n</code></pre> </li> <li> <p>Change directory to where those Kubernetes artefacts are.</p> <pre><code>cd eda-quickstarts/kafka-java-vertz-starter\n</code></pre> </li> <li> <p>Deploy the Kubernetes artefacts.</p> <pre><code>oc apply -k app-deployment\n</code></pre> </li> <li> <p>Get the route to the starter application running on your OpenShift cluster.</p> <pre><code>oc get route es-demo -o=jsonpath='{.status.ingress[].host}'\n</code></pre> </li> <li> <p>Point your browser to that url to work with the IBM Event Streams Starter Application.</p> </li> </ol> <p>The source code for this application is in this git repo: ibm-messaging/kafka-java-vertx-starter.</p> <p>Even though the application is running internally in OpenShift, it uses the external kafka listener as that is how the <code>kafka.properties</code> are provided by IBM Event Streams by default. In an attempt to not overcomplicate this task, it is used what IBM Event Streams provides out of the box.</p>"},{"location":"technology/event-streams/es-cp4i/","title":"Event Streams within Cloud Pak for Integration","text":"<p>Info</p> <p>Updated 10/03/2022 </p> <p>In this tutorial you will learn how to install Event Streams on OpenShift, using a the Administration Console, or using CLI. We propose two installation tutorials: </p> <ul> <li>one using the OpenShift Admin console, then the Event Streams console to create Kafka topics and use the Starter Application, to validate the installation.</li> <li>one using <code>oc CLI</code> from your terminal or from Cloud Shell Terminal.</li> </ul> <p>The Kafka Cluster configuration is for a development or staging environment with no persistence.</p> <p>Once the installation is successful, the final pods running in the <code>eventstreams</code> project are:</p> <p><code>sh dev-entity-operator-69c6b7cf87-bfxbw   2/2     Running   0          43s dev-kafka-0                            1/1     Running   0          2m dev-kafka-1                            1/1     Running   0          2m dev-kafka-2                            1/1     Running   0          2m dev-zookeeper-0                        1/1     Running   0          2m46s dev-zookeeper-1                        1/1     Running   0          3h23m dev-zookeeper-2                        1/1     Running   0          3h23m</code></p> <p>Updated October 07/2021 - Operator Release 2.4 - Product Release 10.4 - Kafka 2.8</p> <p>We recommend to read the following \"structuring your deployment\" chapter from product documentation to give you more insight of the things to consider for deployment. </p> <p>In this tutorial we select to deploy Event Streams in one namespace, and the Operator to monitor multiple namespaces.</p>"},{"location":"technology/event-streams/es-cp4i/#prerequisites","title":"Prerequisites","text":"<ul> <li>Get access to an OpenShift Cluster.</li> <li>Get <code>oc</code> CLI from OpenShift Admin Console</li> </ul>"},{"location":"technology/event-streams/es-cp4i/#install-cloudctl-clis","title":"Install Cloudctl CLIs","text":"<ul> <li> <p>Install IBM Cloud Pak CLI: Cloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs).  This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak. In order to install it, execute the following commands in your IBM Cloud Shell:</p> </li> <li> <p>Download the IBM Cloud Pak CLI (example below is for Mac but  other downloads are available here): </p> </li> </ul> <p><pre><code>curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-darwin-amd64.tar.gz -o cloudctl.tar.gz\n</code></pre>   * Untar it</p> <pre><code>tar -xvf cloudctl.tar.gz\n</code></pre> <ul> <li>Rename it as cloudctl and move to a folder within your PATH:</li> </ul> <pre><code>mv cloudctl-darwin-amd64 cloudctl\n</code></pre> <ul> <li>Make sure your IBM Cloud Pak CLI is in the path: <code>which cloudctl</code></li> <li>Make sure your IBM Cloud Pak CLI works: <code>cloudctl help</code></li> </ul> <p></p> <p>See also the product documentation for prerequisites details</p>"},{"location":"technology/event-streams/es-cp4i/#install-ibm-catalog","title":"Install IBM catalog","text":"<p>Use the EDA Gitops project with the different operator subscriptions and IBM catalog to define the IBM Operator catalog.</p> <pre><code># List existing catalog\noc get catalogsource -n openshift-marketplace\n# Verifi ibm-operator-catalog is present in the list\n# if not add IBM catalog \noc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-gitops-catalog/main/ibm-catalog/catalog_source.yaml\n</code></pre>"},{"location":"technology/event-streams/es-cp4i/#get-ibm-software-entitlement-key-and-create-a-secret","title":"Get IBM Software entitlement key and create a secret","text":"<ul> <li>Obtain IBM license entitlement key.</li> <li> <p>Verify you can access the IBM Image repository with</p> <pre><code>docker login cp.icr.io --username cp --password &lt;key-copied&gt;\n</code></pre> </li> <li> <p>Create a OpenShift secret in the <code>openshift-operator</code> project </p> <pre><code># Get on good project\noc project openshift-operators\n# Verify if secret exists\noc describe secret ibm-entitlement-key\n# Create if needed\noc create secret docker-registry ibm-entitlement-key \\\n    --docker-username=cp \\\n    --docker-server=cp.icr.io \\\n    --namespace=eventstreams \\\n    --docker-password=your_entitlement_key \n</code></pre> </li> </ul> <p>Attention   This secret needs to be present in any namespace where an Event Streams cluster will be deployed.</p>"},{"location":"technology/event-streams/es-cp4i/#install-ibm-foundational-services","title":"Install IBM foundational services","text":"<p>When using security with IAM and other services, Event Streams needs IBM foundations services. It will install it automatically if you install the operator at the cluster level.  In case you need to control the deployment and if Cloud Pak for Integration is not installed yet, you need to install the IBM foundational services operator for that, follow these simple instructions.</p> <p>Create the same ibm-entitlement-key in the <code>ibm-common-services</code>  namespace.</p> <pre><code># verify operator installed\noc get operators -n ibm-common-services\n# -&gt; response:\nibm-common-service-operator.openshift-operators    \nibm-namespace-scope-operator.ibm-common-services   \nibm-odlm.ibm-common-services                       \n</code></pre> <p>Those are the deployments the foundational services are creating:</p> <pre><code>NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\nibm-common-service-webhook             1/1     1            1           68m\nibm-namespace-scope-operator           1/1     1            1           69m\noperand-deployment-lifecycle-manager   1/1     1            1           68m\nsecretshare                            1/1     1            1           68m\n</code></pre> <p>Recalls that operands are what operators manage. </p>"},{"location":"technology/event-streams/es-cp4i/#install-event-streams-using-openshift-console","title":"Install Event Streams Using OpenShift Console","text":"<ul> <li> <p>As Cluster Administrator, create a project using Home &gt; Projects, and create button, and enter <code>eventstreams</code> as project name.</p> <p></p> </li> </ul> <p>The goal is to create a shareable Kafka cluster. </p> <ul> <li>As Administrator, use <code>Openshift console -&gt; Operators &gt; OperatorHub</code> to search for <code>Event Streams</code> operator, then install the operator by selecting the <code>All namespaces on the cluster</code>, the version v2.4. The Operator will monitor all namespaces.</li> </ul> <p></p> <p>The installation of this operator will also include pre-requisites operators like <code>Cloud Pak foundational services (3.11)</code></p> <ul> <li>Once the Operator is ready, go to the <code>Installed Operator</code> under the <code>eventstreams</code> project</li> </ul> <p></p> <ul> <li>Create an Event Streams cluster instance using the operator user interface:</li> </ul> <p></p> <p>Enter the minimum information for a development cluster, like name, license,.. </p> <p></p> <p>Or go to the <code>Yaml view</code> and select one of the proposed Sample:</p> <p></p> <p>The <code>Development</code> configuration should be enough to get you started. For production   deployment see this article.</p> <p>Do not forget to set <code>spec.license.accept</code> to true.</p> <p>Before creating the instance, we recommend you read the security summary    so you can relate the authentication configured by default with what application   needs to use to connect to the Kafka cluster. For example the following declaration stipulates   to use TLS authentication for internal communication, and SCRAM for external connection.</p> <pre><code>```yaml\n    listeners:\n      plain:\n        port: 9092\n        type: internal\n        tls: false\n      external:\n        authentication:\n          type: scram-sha-512\n        type: route\n      tls:\n        authentication:\n          type: tls\n```\n</code></pre> <p>The first deployment of Event Streams, there may be creation of new deployments into the <code>ibm-common-services</code> if those services were not present before. Here is the updated list of <code>ibm-common-services</code> deployments:</p> <pre><code>NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\nauth-idp                               1/1     1            1           27m\nauth-pap                               1/1     1            1           32m\nauth-pdp                               1/1     1            1           32m\ncert-manager-cainjector                1/1     1            1           31m\ncert-manager-controller                1/1     1            1           31m\ncert-manager-webhook                   1/1     1            1           30m\ncommon-web-ui                          1/1     1            1           20m\nconfigmap-watcher                      1/1     1            1           31m\ndefault-http-backend                   1/1     1            1           31m\niam-policy-controller                  1/1     1            1           32m\nibm-cert-manager-operator              1/1     1            1           32m\nibm-common-service-webhook             1/1     1            1           3h12m\nibm-commonui-operator                  1/1     1            1           32m\nibm-iam-operator                       1/1     1            1           33m\nibm-ingress-nginx-operator             1/1     1            1           32m\nibm-management-ingress-operator        1/1     1            1           33m\nibm-mongodb-operator                   1/1     1            1           32m\nibm-monitoring-grafana                 1/1     1            1           32m\nibm-monitoring-grafana-operator        1/1     1            1           33m\nibm-namespace-scope-operator           1/1     1            1           3h12m\nibm-platform-api-operator              1/1     1            1           31m\nmanagement-ingress                     1/1     1            1           23m\nnginx-ingress-controller               1/1     1            1           31m\noidcclient-watcher                     1/1     1            1           32m\noperand-deployment-lifecycle-manager   1/1     1            1           3h11m\nplatform-api                           1/1     1            1           31m\nsecret-watcher                         1/1     1            1           32m\nsecretshare                            1/1     1            1           3h12m\n</code></pre> <p>It could take few minutes to get these pods up and running.</p> <p>The result of this cluster creation should looks like: </p> <p></p> <p>with the list of pod as illustrated at the top of this article.</p> <p>See also Cloud Pak for integration documentation for other deployment considerations.</p>"},{"location":"technology/event-streams/es-cp4i/#adding-users-and-teams","title":"Adding users and teams","text":"<p>You need to be a platform administrator to create users and teams in IAM.</p> <p>To get the admin user credential, use the following command:</p> <pre><code>oc get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' -n ibm-common-services | base64 &amp;&amp; echo \"\"\n</code></pre>"},{"location":"technology/event-streams/es-cp4i/#log-into-event-streams","title":"Log into Event Streams","text":"<p>You can get the User Interface end point by doing the following command.</p> <pre><code>oc get route dev-ibm-es-ui -o jsonpath='{.spec.host}'\n</code></pre> <p>Change <code>dev</code> prefix with the name of your Kafka cluster you defined earlier.</p> <p>Or using the Admin Console </p> <ul> <li>Select the Event Streams Operator in the project where you install it</li> <li>Select Click on the IBM Event Streams Operator and then on the <code>Event Streams</code> option listed at the top bar</li> <li>Click on the IBM Event Streams cluster instance you want to access to its console and see the <code>Admin UI</code> attribute that displays the route to this IBM Event Streams instance's console.   </li> <li>Click on the route link and enter your IBM Event Streams credentials.</li> </ul> <p>Here is the Console home page:</p> <p></p>"},{"location":"technology/event-streams/es-cp4i/#create-event-streams-topics","title":"Create Event Streams Topics","text":"<p>This section is a generic example of the steps to proceed to define a topic with Event Streams on OpenShift.  The example is to define a topic named INBOUND with 1 partition and a replica set to 3.</p> <ol> <li>Log into your IBM Event Streams instance through the UI as explained in the previous section.</li> <li>Click on the Topics option on the navigation bar on the left. </li> <li>In the topics page, click on the <code>Create topic</code> blue button on the top right corner</li> </ol> <p></p> <ol> <li>Provide a name for your topic.</li> </ol> <p></p> <ol> <li>Leave Partitions at 1.</li> </ol> <p></p> <ol> <li>Depending on how long you want messages to persist you can change this.</li> </ol> <p></p> <ol> <li>You can leave Replication Factor at the default 3.</li> </ol> <p></p> <ol> <li>Click Create.</li> <li>Make sure the topic has been created by navigating to the topics section on the IBM Event Streams user inteface you can find an option for in the left hand side menu bar. </li> </ol>"},{"location":"technology/event-streams/es-cp4i/#run-starter-application","title":"Run starter application","text":"<p>See separate note</p>"},{"location":"technology/event-streams/es-cp4i/#install-event-streams-using-clis","title":"Install Event Streams Using CLIs","text":"<p>This section is an alternate of using OpenShift Console. We are using our GitOps catalog repository which defines the different operators and scripts we can use to install Event Streams and other services via scripts. All can be automatized with ArgoCD / OpenShift GitOps.</p> <ul> <li>Create a project to host Event Streams cluster:</li> </ul> <p><pre><code>oc new-project eventstreams\n</code></pre> * Clone our eda-gitops-catalog project</p> <p><pre><code>git clone https://github.com/ibm-cloud-architecture/eda-gitops-catalog.git\n</code></pre> * Create the <code>ibm-entitlement-key</code> secret in this project.</p> <pre><code>oc create secret docker-registry ibm-entitlement-key \\\n      --docker-username=cp \\\n      --docker-server=cp.icr.io \\\n      --namespace=eventstreams \\\n      --docker-password=your_entitlement_key \n</code></pre> <ul> <li>Install Event Streams Operator subscriptions</li> </ul> <pre><code>oc apply -k cp4i-operators/event-streams/operator/overlays/v2.4/\n</code></pre> <ul> <li>Install one Event Streams instance: Instances of Event Streams can be created after the Event Streams operator is installed.  You can use te OpenShift console or our predefined cluster definition:</li> </ul> <pre><code>oc apply -k cp4i-operators/event-streams/instances/dev/\n</code></pre> <p>If you want to do the same thing for a production cluster</p>"},{"location":"technology/event-streams/es-cp4i/#oc-apply-k-cp4i-operatorsevent-streamsinstancesprod-small","title":"<pre><code>oc apply -k cp4i-operators/event-streams/instances/prod-small/\n</code></pre>","text":""},{"location":"technology/event-streams/es-cp4i/#common-operations-to-perform-on-cluster","title":"Common operations to perform on cluster","text":"<p>We list here a set of common operations to perform on top of Event Streams Cluster.</p>"},{"location":"technology/event-streams/es-cp4i/#download-es-cli-plugin","title":"Download ES CLI plugin","text":"<p>From Event Streams Console, go to the \"Find more in the Toolbox\" tile, and then select <code>IBM Event Streams command-line interface</code>, then select your target operating system:</p> <p></p> <p>Initialize the Event Streams CLI plugin (make sure you provide the namespace where your IBM Event Streams instance is installed on as the command will fail if you dont have cluster wide admin permissions)</p> <pre><code>$ cloudctl es init -n eventstreams\n\nIBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-sandbox.gse-ocp.net   \nNamespace:                                     integration   \nName:                                          kafka   \nIBM Cloud Pak for Integration UI address:      https://integration-navigator-pn-integration.apps.eda-sandbox.gse-ocp.net   \nEvent Streams API endpoint:                    https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox.gse-ocp.net   \nEvent Streams API status:                      OK   \nEvent Streams UI address:                      https://kafka-ibm-es-ui-integration.apps.eda-sandbox.gse-ocp.net   \nApicurio Registry endpoint:                    https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox.gse-ocp.net   \nEvent Streams bootstrap address:               kafka-kafka-bootstrap-integration.apps.eda-sandbox.gse-ocp.net:443   \nOK\n</code></pre>"},{"location":"technology/event-streams/es-cp4i/#access-to-event-streams-console-using-cli","title":"Access to Event Streams Console using CLI","text":"<p>In order to log into IBM Event Streams console through the CLI,  we are going to use the <code>oc</code> OpenShift CLI, the <code>cloudctl</code> Cloud Pak CLI and  the <code>es</code> Cloud Pak CLI plugin. </p> <p>We assume you are already logged into your OpenShift cluster.</p> <ol> <li>Get your Cloud Pak Console route (you may need cluster wide admin permissions to do so as the Cloud Pak Console is usually installed in the <code>ibm-common-services</code> namespace by the cluster admins)</li> </ol> <pre><code>$ oc get routes -n ibm-common-services | grep console\ncp-console                       cp-console.apps.eda-sandbox.gse-ocp.net                                                  icp-management-ingress           https      reencrypt/Redirect     None\n</code></pre> <ol> <li>Log into IBM Event Streams using the Cloud Pak console route from the previous step:</li> </ol> <pre><code>$ cloudctl login -a https://cp-console.apps.eda-sandbox.gse-ocp.net --skip-ssl-validation\n\nUsername&gt; user50\n\nPassword&gt;\nAuthenticating...\nOK\n\nTargeted account mycluster Account\n\nEnter a namespace &gt; integration\nTargeted namespace integration\n\nConfiguring kubectl ...\nProperty \"clusters.mycluster\" unset.\nProperty \"users.mycluster-user\" unset.\nProperty \"contexts.mycluster-context\" unset.\nCluster \"mycluster\" set.\nUser \"mycluster-user\" set.\nContext \"mycluster-context\" created.\nSwitched to context \"mycluster-context\".\nOK\n\nConfiguring helm: /Users/user/.helm\nOK\n</code></pre>"},{"location":"technology/event-streams/es-cp4i/#create-topic-with-es-cli","title":"Create Topic with es CLI","text":"<ol> <li>Create the topic with the desi specification.</li> </ol> <pre><code>cloudctl es topic-create --name INBOUND --partitions 1 --replication-factor 3\n</code></pre> <ol> <li>Make sure the topic has been created by listing the topics.</li> </ol> <pre><code>$ cloudctl es topics\nTopic name   \nINBOUND   \nOK\n</code></pre>"},{"location":"technology/event-streams/es-cp4i/#get-kafka-bootstrap-url","title":"Get Kafka Bootstrap Url","text":"<p>For an application to access the Kafka Broker, we need to get the bootstrap URL.</p>"},{"location":"technology/event-streams/es-cp4i/#ui","title":"UI","text":"<p>You can find your IBM Event Streams Kakfa bootstrap url if you log into the IBM Event  Streams user interface, and click on the <code>Connect to this cluster</code> option displayed   on the dashboard. This will display a menu where you will see a url to the left    of the <code>Generate SCAM credentials</code> button. Make sure that you are on    the External Connection.</p> <p></p>"},{"location":"technology/event-streams/es-cp4i/#cli","title":"CLI","text":"<p>You can find your IBM Event Streams and Kakfa bootstrap url when you init the  IBM Event Streams Cloud Pak CLI plugin on the last line:</p> <pre><code>$ cloudctl es init -n eventstreams\n\nIBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-sandbox-delta.gse-ocp.net   \nNamespace:                                     integration   \nName:                                          kafka   \nIBM Cloud Pak for Integration UI address:      https://integration-navigator-pn-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams API endpoint:                    https://kafka-ibm-es-admapi-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams API status:                      OK   \nEvent Streams UI address:                      https://kafka-ibm-es-ui-integration.apps.eda-sandbox-delta.gse-ocp.net   \nApicurio Registry endpoint:                    https://kafka-ibm-es-ac-reg-external-integration.apps.eda-sandbox-delta.gse-ocp.net   \nEvent Streams bootstrap address:               kafka-kafka-bootstrap-integration.apps.eda-sandbox-delta.gse-ocp.net:443   \nOK\n</code></pre>"},{"location":"technology/event-streams/es-cp4i/#generate-scram-service-credentials","title":"Generate SCRAM Service Credentials","text":"<p>For an application to connect to an Event Streams instance through the secured  external listener, it needs SCRAM credentials to act as service credentials to authenticate the application. We also need TLS certificate to encrypt the communication with the brokers.</p>"},{"location":"technology/event-streams/es-cp4i/#ui_1","title":"UI","text":"<ol> <li>Log into the IBM Event Streams user interface as explained previously in this readme and click on the <code>Connect to this cluster</code> option displayed on the dashboard. This will display a menu. Make sure that you are on the External Connection.</li> </ol> <ol> <li>Click on the <code>Generate SCRAM credentials</code> button.</li> </ol> <ol> <li>Introduce a name for your credentials and choose the option that better suits the needs of your applications (this will create RBAC permissions for you credentials so that a service credentials can do only what it needs to do). For this demo, select <code>Produce messages, consume messages and create topics and schemas</code> last option.</li> </ol> <ol> <li>Decide whether your service credentials need to have the ability to access all topics or certain topics only. For this demo, select <code>All Topics</code> and then click Next.</li> </ol> <ol> <li>Decide whether your service credentials need to have the ability to access all consumer groups or certain specific consumer groups only. For this demo, select <code>All Consumer Groups</code> and click Next.</li> </ol> <ol> <li>Decide whether your service credentials need to have the ability to access all transactional IDs or certain specific transactional IDs only. For this demo, select <code>All transaction IDs</code> and click on <code>Generate credentials</code>.</li> </ol> <ol> <li>Take note of the set of credentials displayed on screen.  You will need to provide your applications with these in order to get authenticated  and authorized with your IBM Event Streams instance.</li> </ol> <ol> <li>If you did not take note of your SCRAM credentials or you forgot these, the above will create a <code>KafkaUser</code> object in OpenShift that is interpreted by the IBM Event Streams Operator. You can see this <code>KafkaUser</code> if you go to the OpenShift console, click on <code>Operators --&gt; Installed Operators</code> on the right hand side menu, then click on the <code>IBM Event Streams</code> operator and finally click on <code>Kafka Users</code> at the top bar menu.</li> </ol> <ol> <li>If you click on your <code>Kafka User</code>, you will see what is the Kubernetes secret behind holding your SCRAM credentials details.</li> </ol> <ol> <li>Click on that secret and you will be able to see again your <code>SCRAM password</code> (your <code>SCRAM username</code> is the same name as the <code>Kafka User</code> created or the secret holding your <code>SCRAM password</code>) </li> </ol>"},{"location":"technology/event-streams/es-cp4i/#cli_1","title":"CLI","text":"<ol> <li> <p>Log into your IBM Event Streams instance through the CLI.</p> </li> <li> <p>Create your SCRAM service credentials with the following command (adjust the topics, consumer groups, transaction IDs, etc permissions your SCRAM service credentials should have in order to satisfy your application requirements):</p> </li> </ol> <pre><code>$ cloudctl es kafka-user-create --name test-credentials-cli --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512\n\nKafkaUser name         Authentication   Authorization   Username                                                Secret   \ntest-credentials-cli   scram-sha-512    simple          EntityOperator has not created corresponding username   EntityOperator has not created corresponding secret   \n\nResource type     Name        Pattern type   Host   Operation   \ntopic             *           literal        *      Read   \ntopic             __schema_   prefix         *      Read   \ntopic             *           literal        *      Write   \ntopic             *           literal        *      Create   \ntopic             __schema_   prefix         *      Alter   \ngroup             *           literal        *      Read   \ntransactionalId   *           literal        *      Write   \n\nCreated KafkaUser test-credentials-cli.\nOK\n</code></pre> <ol> <li>List the <code>KafkaUser</code> objects to make sure yours has been created:</li> </ol> <pre><code>$ cloudctl es kafka-users\nKafkaUser name                    Authentication   Authorization   \ntest-credentials                  scram-sha-512    simple   \ntest-credentials-cli              scram-sha-512    simple   \nOK\n</code></pre> <ol> <li>To retrieve your credentials execute the following command:</li> </ol> <pre><code>$ cloudctl es kafka-user test-credentials-cli \nKafkaUser name         Authentication   Authorization   Username               Secret   \ntest-credentials-cli   scram-sha-512    simple          test-credentials-cli   test-credentials-cli   \n\nResource type     Name        Pattern type   Host   Operation   \ntopic             *           literal        *      Read   \ntopic             __schema_   prefix         *      Read   \ntopic             *           literal        *      Write   \ntopic             *           literal        *      Create   \ntopic             __schema_   prefix         *      Alter   \ngroup             *           literal        *      Read   \ntransactionalId   *           literal        *      Write   \nOK\n</code></pre> <ol> <li>Above you can see your <code>SCRAM username</code> under <code>Username</code> and the secret holding your <code>SCRAM password</code> under <code>Secret</code>. In order to retrieve the password, execute the following command:</li> </ol> <pre><code>$ oc get secret test-credentials-cli -o jsonpath='{.data.password}' | base64 --decode \n\n*******\n</code></pre> <p>NEXT: For more information about how to connect to your cluste, read the IBM Event Streams product documentation</p>"},{"location":"technology/event-streams/es-cp4i/#get-event-streams-tls-certificates","title":"Get Event Streams TLS Certificates","text":"<p>In this section we are going to see how to download the TLS certificats to securely connect to our IBM Event Streams instance.</p>"},{"location":"technology/event-streams/es-cp4i/#ui_2","title":"UI","text":"<ol> <li> <p>Log into the IBM Event Streams console user interface as explained before in this readme.</p> </li> <li> <p>Click on the <code>Connect to this cluster</code> option displayed on the dashboard. This will display a menu where you will see a <code>Certificates</code> section:</p> </li> </ol> <p></p> <ol> <li>Depending on what language your application is written into, you will need a <code>PKCS12 certificate</code> or a <code>PEM certificate</code>. Click on <code>Download certificate</code> for any of the options you need. If it is the <code>PKCS12 certificate</code> bear in mind it comes with a password for the truststore. You don't need to write this down as it will display any time you click on <code>Download certificate</code> button.</li> </ol>"},{"location":"technology/event-streams/es-cp4i/#cli_2","title":"CLI","text":"<ol> <li> <p>Log into IBM Event Streams through the CLI as already explained before in this readme.</p> </li> <li> <p>To retrieve the <code>PKCS12 certificate</code> execute the following command:</p> </li> </ol> <pre><code>$ cloudctl es certificates --format p12\nTrustore password is ********\nCertificate successfully written to /Users/testUser/Downloads/es-cert.p12.\nOK\n</code></pre> <ol> <li>To retrieve the <code>PEM certificate</code> execute the following command:</li> </ol> <pre><code>$ cloudctl es certificates --format pem\nCertificate successfully written to /Users/testUser/Downloads/es-cert.pem.\nOK\n</code></pre>"},{"location":"technology/event-streams/es-maas/es-cloud/","title":"Event Streams on Cloud hands on lab","text":"<p>This documentation aims to be a introductory hands-on lab on IBM Event Streams on Cloud with topic creation.</p>"},{"location":"technology/event-streams/es-maas/es-cloud/#index","title":"Index","text":""},{"location":"technology/event-streams/es-maas/es-cloud/#pre-requisites","title":"Pre-requisites","text":"<p>This lab requires the following components to work against:</p> <ol> <li>An IBM Cloud account. Get a IBM Cloud Account by using the register link in https://cloud.ibm.com/login Create a new account is free of charge.</li> </ol> <p></p> <p>On your development workstation you will need:</p> <ol> <li>IBM Cloud CLI (https://cloud.ibm.com/docs/cli?topic=cloud-cli-getting-started)</li> <li>IBM CLoud CLI Event Streams plugin (<code>ibmcloud plugin install event-streams</code>)</li> </ol>"},{"location":"technology/event-streams/es-maas/es-cloud/#login-cli","title":"Login CLI","text":"<pre><code>ibmcoud login\n</code></pre>"},{"location":"technology/event-streams/es-maas/es-cloud/#account-and-resource-group-concepts","title":"Account and resource group concepts","text":"<p>As any other IBM Cloud services, Event Streams can be part of a resources group, is controlled by user roles, and is accessible via API keys. To get familiar with those concepts, it is recommended to study the concepts of IBM account and how it is related to resource group and services. The following diagram is a summary of the objects managed in IBM Cloud:</p> <p></p> <p>To summarize:</p> <ul> <li>Account represents the billable entity, and can have multiple users.</li> <li>Users are given access to resource groups.</li> <li>Applications are identified with a service ID.</li> <li>To restrict permissions for using specific services, you can assign specific access policies to the service ID and user ID</li> <li>Resource groups are here to organize any type of resources (services, clusters, VMs...) that are managed by  Identity and Access Management (IAM).</li> <li>Resource groups are not scoped by location</li> <li>Access group are used to organize a set of users and service IDs into a single entity and easily assign permissions</li> </ul>"},{"location":"technology/event-streams/es-maas/es-cloud/#create-a-event-streams-service-instance","title":"Create a Event Streams service instance","text":"<p>From the IBM Cloud Dashboard page, you can create a new resource, using the right top button <code>Create resource</code>.</p> <p></p> <p>which leads to the service and feature catalog. From there in the <code>services</code> view, select the <code>integration</code> category and then the <code>Event Streams</code> tile:</p> <p></p> <p>You can access this screen from this URL: https://cloud.ibm.com/catalog/event-streams.</p>"},{"location":"technology/event-streams/es-maas/es-cloud/#plan-characteristics","title":"Plan characteristics","text":"<p>Within the first page for the Event Streams creation, you need to select the region, the pricing plan, a service name and the resource group.</p> <p></p> <p>For the region, it is important to note that the 'lite' plan is available only in Dallas, and it used to do some proof of concept. It is recommended to select a region close to your on-premise data center. For the Plan description, the product documentation goes over the different plans in details.</p> <p>The 'multi-tenancy' means the Kafka cluster is shared with other people. The cluster topology is covering multi availability zones inside the same data center. The following diagram illustrates a simple view of this topology with the different network zones and availability zones:</p> <p></p> <p>We will address fine-grained access control in the security lab.</p> <p>As described in the Kafka concept introduction, topic may have partitions. Partitions are used to improve throughput as consumer can run in parallel, and producer can publish to multiple partitions.</p> <p>The plan set a limit on the total number of partitions.</p> <p>Each partition records, are persisted in the file system and so the maximum time records are kept on disks is controlled by the maximum retention period and total size. Those Kafka configurations are described in the topic and broker documentation.</p> <p>Fro the Standard plan, the first page has also a price estimator. The two important concepts used for pricing are the number of  partition instances and the number of GB consumed: each consumer reading from a topic/partition will increase the number of byte consumed. The cost is per month.</p>"},{"location":"technology/event-streams/es-maas/es-cloud/#creating-event-streams-instance-with-ibm-cloud-cli","title":"Creating Event Streams instance with IBM Cloud CLI","text":"<ol> <li>Go to IBM Cloud and click on the user avatar on the top right corner. Then, click on Log in to CLI and API option:</li> </ol> <ol> <li>Copy the <code>IBM Cloud CLI</code> login command</li> </ol> <ol> <li> <p>Open a terminal window, paste and execute the command:</p> <pre><code>$ ibmcloud login -a https://cloud.ibm.com -u passcode -p XsgEKGb84Z\nAPI endpoint: https://cloud.ibm.com\nAuthenticating...\nOK\n\nTargeted account bill s Account (b63...) &lt;-&gt; 195...\nSelect a region (or press enter to skip):\n1. au-syd\n2. in-che\n3. jp-osa\n4. jp-tok\n5. kr-seo\n6. eu-de\n7. eu-gb\n8. us-south\n9. us-south-test\n10. us-east\nEnter a number&gt; 6\nTargeted region eu-de\n\nAPI endpoint:      https://cloud.ibm.com\nRegion:            eu-de\nUser:              A&lt;&gt;\nAccount:           Bill s Account (b63...) &lt;-&gt; 195...\nResource group:    No resource group targeted, use ibmcloud target -g RESOURCE_GROUP\nCF API endpoint:\nOrg:\nSpace:\n</code></pre> </li> <li> <p>List your services with <code>ibmcloud resource service-instances</code> and make sure your IBM Event Streams instance is listed:</p> <pre><code>$ ibmcloud resource service-instances\nRetrieving instances with type service_instance in all resource groups in all locations under account Kedar Kulkarni's Account as ALMARAZJ@ie.ibm.com...\nOK\nName                                   Location   State    Type\nIBM Cloud Monitoring with Sysdig-rgd   us-south   active   service_instance\napikey for simple toolchain            us-east    active   service_instance\naapoc-event-streams                    us-south   active   service_instance\nEvent Streams-wn                       eu-de      active   service_instance\n</code></pre> <p>We can see our instance called: Event Streams-wn</p> </li> <li> <p>Create an Event Streams instance using CLI</p> <pre><code>ibmcloud resource service-instance-create EventStreamsEDA2 messagehub standard us-south\n</code></pre> </li> <li> <p>List your IBM Event Streams instance details with <code>ibmcloud resource service-instance &lt;instance_name&gt;</code>:</p> <p><pre><code>$ ibmcloud resource service-instance Event\\ Streams-wn\nRetrieving service instance Event Streams-wn in all resource groups under account Kedar Kulkarni's Account as ALMARAZJ@ie.ibm.com...\nOK\n\nName:                  Event Streams-wn\nID:                    crn:v1:bluemix:public:messagehub:eu-de:a/b636d1d8...cfa:b05be9...2e687a::\nGUID:                  b05be932...e687a\nLocation:              eu-de\nService Name:          messagehub\nService Plan Name:     enterprise-3nodes-2tb\nResource Group Name:\nState:                 active\nType:                  service_instance\nSub Type:\nCreated at:            2020-05-11T15:54:48Z\nCreated by:            bob.the.builder@someemail.com\nUpdated at:            2020-05-11T16:49:18Z\nLast Operation:\n                    Status    sync succeeded\n                    Message   Synchronized the instance\n</code></pre> Mind the <code>\\</code> character in your IBM Event Streams instance.</p> </li> <li> <p>Initialize your IBM Event Streams plugin for the IBM Cloud CLI with <code>ibmcloud es init</code>:</p> <pre><code>$ ibmcloud es init\n\nSelect an Event Streams instance:\n1. Event Streams-2t\n2. Event Streams-wn\n3. aapoc-event-streams\n4. tutorial\nEnter a number&gt; 2\nAPI Endpoint:   https://mh-tcqsppdpzlrkdmkb.....175-0000.eu-de.containers.appdomain.cloud\nOK\n</code></pre> </li> <li> <p>Check all the CLI commands available to you to manage and interact with your IBM Event Streams instance with <code>$ ibmcloud es</code>:</p> <pre><code>$ ibmcloud es\nNAME:\nibmcloud es - Plugin for IBM Event Streams (build 1908221834)\n\nUSAGE:\nibmcloud es command [arguments...] [command options]\n\nCOMMANDS:\nbroker                 Display details of a broker.\nbroker-config          Display broker configuration.\ncluster                Display details of the cluster.\ngroup                  Display details of a consumer group.\ngroup-delete           Delete a consumer group.\ngroup-reset            Reset the offsets for a consumer group.\ngroups                 List the consumer groups.\ninit                   Initialize the IBM Event Streams plugin.\ntopic                  Display details of a topic.\ntopic-create           Create a new topic.\ntopic-delete           Delete a topic.\ntopic-delete-records   Delete records from a topic before a given offset.\ntopic-partitions-set   Set the partitions for a topic.\ntopic-update           Update the configuration for a topic.\ntopics                 List the topics.\nhelp, h                Show help\n\nEnter 'ibmcloud es help [command]' for more information about a command.\n</code></pre> </li> <li> <p>List your cluster configuration with <code>$ ibmcloud es cluster</code>:</p> <p><pre><code>$ ibmcloud es cluster\nDetails for cluster\nCluster ID                                                      Controller\nmh-tcqsppdpzlrkdmkbgmgl-4c20...361c6f175-0000   0\n\nDetails for brokers\nID   Host                                                                                                     Port   Rack\n0    kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d......22e361c6f175-0000.eu-de.containers.appdomain.cloud   9093   fra05\n1    kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d......22e361c6f175-0000.eu-de.containers.appdomain.cloud   9093   fra02\n2    kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d......22e361c6f175-0000.eu-de.containers.appdomain.cloud   9093   fra04\nNo cluster-wide dynamic configurations found.\n</code></pre> 1. Looking at broker details: <code>ibmcloud es broker  0</code>:</p> <pre><code>ibmcloud es broker  0\n    Details for broker\n    ID   Host                                                                        Port   Rack\n    0    broker-0-t19zgvnykgdqy1zl.kafka.svc02.us-south.eventstreams.cloud.ibm.com   9093   dal10\n\n    Details for broker configuration\n    Name                   Value                                                                                            Sensitive?\n    broker.id              0                                                                                                false\n    broker.rack            dal10                                                                                            false\n    advertised.listeners   SASL_EXTERNAL://broker-0-t19zgvnykgdqy1zl.kafka.svc02.us-south.eventstreams.cloud.ibm.com:9093   false\n    OK\n</code></pre> </li> <li> <p>Get detail view of a broker configuration: <code>ibmcloud es broker-config  0</code></p> </li> </ol> <p>We will see other CLI commands in future labs.</p>"},{"location":"technology/event-streams/es-maas/es-cloud/#coming-back-another-time","title":"Coming back another time","text":"<p>When coming back to the IBM Cloud dashboard the simplest way to find the Event Streams service is to go to the <code>Services</code>:</p> <p></p> <ul> <li>Click on your IBM Event Streams instance:</li> </ul> <p></p> <ul> <li>Click on Launch Dashboard button to open the IBM Event Streams dashboard</li> </ul> <p></p>"},{"location":"technology/event-streams/es-maas/es-cloud/#main-event-streams-dashboard-page","title":"Main Event Streams Dashboard page","text":"<p>Once the instance is created, or when you come back to the service, you reach the <code>manage</code>panel, as illustrated in previous figure.</p> <p>From the Dashboard we can access the Topics and Consumer groups panels.</p> <p></p>"},{"location":"technology/event-streams/es-maas/es-cloud/#create-topic","title":"Create topic","text":"<p>In this section we are going to see how to create, list and delete topics both using the User Interface and then the IBM Event Streams CLI.</p> <ol> <li> <p>Open the IBM Event Streams user interface (go into your IBM Event Streams service within your IBM Cloud portal and click on the launch dashboard button). Once there, click on the Topics tab from the top menu:</p> <p></p> </li> </ol> <p>Let create a <code>demo-topic-ui</code> topic. If you need to revisit the topic concepts, you can read this note. When you go to the topics view you get the list of existing topics.</p> <p></p> <p>From this list an administrator can delete an existing topic or create new one.</p> <ol> <li>The 'create topic' button leads to the step by step process.</li> <li>Switch to the Advanced mode to get access to the complete set of parameters. The first panel is here to define the core configuration</li> </ol> <p></p> <p>Some parameters to understand:</p> <ul> <li>Number of partitions: the default value should be 1. If the data can be partitioned without loosing semantic, you can increase the number of partitions.</li> <li>Retention time: This is how long messages are retained before they are deleted to free up space. If your messages are not read by a consumer within this time, they will be missed. It is mapped to the retention.ms kafka topic configuration.</li> </ul> <p>The bottom part of the configuration page, includes logs, cleanup and indexing.</p> <p></p> <ul> <li> <p>The partition's log parameter section includes a cleanup policy that could be:</p> <ul> <li>delete: discard old segments when their retention time or size limit has been reached</li> <li>compact: retain at least the last known value for each message key within the log of data for a single topic partition. The topic looks like a table in DB.</li> <li>compact, delete: compact the log and remove old records</li> </ul> </li> <li> <p>retention bytes: represents the maximum size a partition (which consists of log segments) can grow to, before old log segments will be discarded to free up space.</p> </li> <li>log segment size is the maximum size in bytes of a single log file.</li> <li>Cleanup segment time - segment.ms controls the period of time after which Kafka will force the log to roll, even if the segment file isn't full, this is to ensure that retention can delete or compact old data.</li> <li>Index - segment.index.bytescontrols the size of the index that maps offsets to file positions.</li> </ul> <p>The log cleaner policy is supported by a log cleaner, which are threads that recopy log segment files, removing records whose key appears in the head of the log.</p> <p>The number of replications is set to three with a <code>min-in-sync</code> replicas of two.</p> <p>A message is considered committed when all in sync replicas for that partition have applied it to their log. The leader maintains a set of in-sync-replicas: all the nodes which are up-to-date with the leader\u2019s log, and actively acknowledging new writes. Every write goes through the leader and is propagated to every node in the In Sync Replica set, or ISR. </p> <p></p> <ol> <li> <p>We can now see our new topic:</p> <p></p> </li> <li> <p>To delete a topic, click on the topic options button at the right end of a topic, click on Delete this topic and then on the Delete button in the confirmation pop-up window:</p> <p></p> </li> <li> <p>The topic should now be deleted:</p> <p></p> </li> </ol>"},{"location":"technology/event-streams/es-maas/es-cloud/#create-topic-with-cli","title":"Create topic with CLI","text":"<ol> <li> <p>List your topics with <code>$ ibmcloud es topics</code>:</p> <p><pre><code>$ ibmcloud es topics\nOK\nNo topics found.\n</code></pre> 1. Create a topic: (Default 1 partition - 3 replicas)</p> <p><pre><code>$ ibmcloud es topic-create --name demo-topic\nCreated topic demo-topic\nOK\n</code></pre> * Execute <code>$ ibmcloud es topic-create --help</code> for more further configuration of your topic creation</p> </li> <li> <p>List topics:</p> <pre><code>$ ibmcloud es topics\nTopic name\ndemo-topic\nOK\n</code></pre> </li> <li> <p>Display details of a topic:</p> <pre><code>$ ibmcloud es topic demo-topic\nDetails for topic demo-topic\nTopic name   Internal?   Partition count   Replication factor\ndemo-topic   false       1                 3\n\nPartition details for topic demo-topic\nPartition ID   Leader   Replicas   In-sync\n0              2        [2 1 0]    [2 1 0]\n\nConfiguration parameters for topic demo-topic\nName                  Value\ncleanup.policy        delete\nmin.insync.replicas   2\nsegment.bytes         536870912\nretention.ms          86400000\nretention.bytes       1073741824\nOK\n</code></pre> </li> <li> <p>Delete records in a topic (in the command below, we want to delete record on a partition 0 offset 5 and partition 1 from offset 0):</p> <pre><code>$ ibmcloud es topic-delete-records --name demo-topic --partition-offset 1:0;0:5 --force\n</code></pre> </li> <li> <p>Add partitions to an existing topic, by setting the new target number of partition:</p> <pre><code>$ ibmcloud es topic-partition-set --name demo-topic --partitions 30\n</code></pre> </li> <li> <p>Delete a topic:</p> <pre><code>$ ibmcloud es topic-delete demo-topic\nReally delete topic 'demo-topic'? [y/N]&gt; y\nTopic demo-topic deleted successfully\nOK\n</code></pre> </li> <li> <p>List topics:</p> <pre><code>$ ibmcloud es topics\nOK\nNo topics found.\n</code></pre> </li> </ol> <p>For the last list of commands see the CLI Reference manual.</p>"},{"location":"technology/event-streams/es-maas/es-cloud/#getting-started-applications","title":"Getting started applications","text":"<p>From the manage dashboard we can download a getting started application that has two processes: one consumer and one producer, or we can use a second application that we have in this repository</p>"},{"location":"technology/event-streams/es-maas/es-cloud/#using-the-event-streams-on-cloud-getting-started-app","title":"Using the Event Streams on cloud getting started app","text":"<p>To be able to build the code you need to get gradle installed or use the docker image:</p> <pre><code>docker run --rm -u gradle -v \"$PWD\":/home/gradle/project -w /home/gradle/project gradle gradle &lt;gradle-task&gt;\n</code></pre> <p>The instructions are in this documentation and can be summarized as:</p> <ul> <li>Clone the github repository:</li> </ul> <pre><code>git clone https://github.com/ibm-messaging/event-streams-samples.git\n</code></pre> <ul> <li>Build the code:</li> </ul> <p>Using the gradle CLI <pre><code>cd kafka-java-console-sample\ngradle clean &amp;&amp; gradle build\n</code></pre></p> <p>or the gradle docker image</p> <pre><code>docker run --rm -u gradle -v \"$PWD\":/home/gradle/project -w /home/gradle/project gradle gradle build\n</code></pre> <ul> <li>Start consumer</li> </ul> <pre><code>java -jar ./build/libs/kafka-java-console-sample-2.0.jar broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5 -consumer\n</code></pre> <ul> <li>Start producer</li> </ul> <pre><code>java -jar ./build/libs/kafka-java-console-sample-2.0.jar broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 am_rbb9e794mMwhE-KGPYo0hhW3h91e28OhT8IlruFe5 -producer\n</code></pre>"},{"location":"technology/event-streams/es-maas/security/","title":"Event Streams on Cloud Security Control Lab","text":"<p>This documentation aims to be a introductory hands-on lab for the security feature of IBM Event Streams.</p>"},{"location":"technology/event-streams/es-maas/security/#security-considerations","title":"Security considerations","text":"<p>There are a lot of security requirements to address to secure platform and solution, in this section we will list some of them</p> <ul> <li>Secure data access which include data in motion or at rest encryption, connection and isolation via private network. Support to Virtual Private Network and bring your own keys.</li> <li>Control access to Kafka cluster and resources like topics, using role based access control, SAML tokens and TLS certificates. Bring your own keys.</li> <li>Monitor and audit to provide audit log and secured APIs access</li> <li>User onboarding: define application specific service accounts, and admin users for devOps staff. </li> </ul>"},{"location":"technology/event-streams/es-maas/security/#iam-concept-summary","title":"IAM Concept Summary","text":"<p>To undertand the Identity and access management you can read this article.</p> <p>To summarize:</p> <ul> <li>Account represents the billable entity, and can have multiple users.</li> <li>Users are given access to resource groups.</li> <li>_ Identity_ concept consists of user identities, service and app identities, API keys, and resources.</li> <li>Applications and IBM Cloud Services are identified with a service ID.</li> <li>To restrict permissions for using specific services, you can assign specific access policies to the service ID and user ID</li> <li>Resource groups are here to organize any type of resources (services, clusters, VMs...) that are managed by  Identity and Access Management (IAM).</li> <li>Resource groups are not scoped by location.</li> <li>API keys can be use to authenticate user or a service / application.</li> <li> <p>To control access three components are used: access groups, resources and access policies.</p> <ul> <li>Access group are used to organize a set of users and service IDs into a single entity and easily assign permissions via access policies</li> <li>Policies give permission to access account resources. Policies include a subject (user, service, access group), target (resource), and role.</li> <li>Policy can be set to all resources in a resource group</li> <li>There are two types of access roles: platform management and service access.</li> </ul> </li> </ul> <p>Here is the main page for the IAM service on IBM Cloud where an account owner can manage the different components of the security control:</p> <p></p>"},{"location":"technology/event-streams/es-maas/security/#event-streams-access-management-concepts","title":"Event Streams access management concepts","text":"<p>This section is a quick overview of the Managing access to your Event Streams resources article.</p>"},{"location":"technology/event-streams/es-maas/security/#roles","title":"Roles","text":"<p>Users can perform specific tasks when they are assigned to a role for which access policies are defined. The roles defined for Event Streams are Reader, Writer and Manager.</p> <ul> <li>Reader: Users can view Event Streams resources, and applications can only consume records</li> <li>Writer : Users can edit resource, and applications can produce and consume records</li> <li>Manager: Users can do privileged actions.</li> </ul>"},{"location":"technology/event-streams/es-maas/security/#assign-access","title":"Assign access","text":"<p>The product documentation addresses how to assign access in this section.</p> <p>The type of Kafka resources that may be secured are: cluster, topic, group, or transaction id. In this lab we will give topic access to users within a group and application by using service ID.</p>"},{"location":"technology/event-streams/es-maas/security/#pre-requisites","title":"Pre-requisites","text":"<p>This lab requires the following components to work against:</p> <ol> <li>An IBM Cloud account. Get a IBM Cloud Account by using the register link in https://cloud.ibm.com/login Create a new account is free of charge.</li> <li>IBM Cloud CLI (https://cloud.ibm.com/docs/cli?topic=cloud-cli-getting-started)</li> <li>IBM CLoud CLI Event Streams plugin (<code>ibmcloud plugin install event-streams</code>)</li> </ol>"},{"location":"technology/event-streams/es-maas/security/#add-access-group-to-the-account-using-iam","title":"Add access group to the account using IAM","text":"<p>The goal of this step is to create an access group to access Event Streams services as administrator.</p> <ol> <li> <p>From the IBM Cloud main page, go to the Manage &gt; IAM menu on top right of IBM Cloud Dashboard page:</p> <p></p> <p>This should lead you to the main IAM page as illustrated in first figure above.</p> </li> <li> <p>Under the Access Group, you can create a new group of users. This will be an administrator group:</p> <p></p> </li> <li> <p>Then add users to this newly created group, by selecting users from the list, (those users were invited to join the account at some time) and Click on <code>Add to group</code> link.</p> <p></p> </li> <li> <p>Finally, you want now to add Access Policies to control access to Event Streams clusters. For that click Assign access button:</p> <p></p> </li> <li> <p>Then select the type of resource (Event Streams), you want to define the access policy on:</p> <p></p> </li> <li> <p>Then specify that the group can manage all instances of Event Streams service. The condition applies to the service instance with a Manager role:</p> <p></p> </li> <li> <p>Add the policy and assign it to the group. You could stay in the same panel to add more target to the policy.</p> <p></p> </li> <li> <p>The newly created, access policies for administer / manager to any Event Streams services is now listed in the access group:</p> <p></p> </li> </ol> <p>Any user who has a manager role for either 'All' services or 'All' Event Streams services in the same account will also have full access.</p>"},{"location":"technology/event-streams/es-maas/security/#limiting-topic-access-to-group-of-users","title":"Limiting topic access to group of users","text":"<p>The product document illustrates some access control common scenarios in this section. We recommend reading those settings.</p> <p>In this step we are implementing one of the classical scenario: suppose we have a line of business that will create topics by applying a naming convention where topic name starts with a prefix like: <code>bn-lob1-*</code>.  We want users and service ID to  get read / write access to only those topics matching those prefix.</p> <p>To do so you need to:</p> <ol> <li>Add an access group to include member of the line of business: <code>bn-lob1-group</code></li> <li> <p>Define an access policy with the following criterias:</p> <ul> <li>Event streams as resource type</li> <li>All regions</li> <li>Specifying one of the Event Streams resource instance (the one you provisioned in this lab)</li> <li>Select the service instance that hosts the target cluster</li> <li>Specify the resource type to be topic</li> <li>And the resource ID to matches <code>bn-lob1-*</code></li> </ul> <p></p> <ul> <li>Then add a second rule to enforce read access at the cluster level:</li> </ul> <p></p> <ul> <li>Assign the two rules:</li> </ul> <p></p> <ul> <li>The group has the two access policies:</li> </ul> <p></p> </li> </ol>"},{"location":"technology/event-streams/es-maas/security/#authentication-with-api-keys","title":"Authentication with API Keys","text":"<p>To let an application to remotely authenticate itself, Event Streams uses API keys. The goal of this section, is to create API keys so applications, tools, scripts can connect to the newly created IBM Event Streams instance. You will create three keys for the different roles: creation of topics, read access only and read/write access.</p> <ol> <li> <p>In your IBM Event Streams instance service page, click on Service credentials on the left hand side menu:</p> <p></p> </li> <li> <p>Observe, there is no service credentials yet and click on the New credential button on the top right corner:</p> <p></p> </li> <li> <p>Enter a name for your service, choose Manager role for now and click on Add:</p> <p></p> </li> <li> <p>You should now see your new service credential and be able to inspect its details if you click on its dropdown arrow on it left:</p> <p></p> </li> </ol> <p>Using the same process you can add a credential for Writer Role. For the Reader role you will use the CLI in the next section.</p> <p>API Key is used for the <code>sasl.jaas.config</code> conncection properties in Kafka consumer or producer:</p> <pre><code>security.protocol=SASL_SSL\nssl.protocol=TLSv1.2\nssl.enabled.protocols=TLSv1.2\nssl.endpoint.identification.algorithm=HTTPS\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"am....5\";\n</code></pre>"},{"location":"technology/event-streams/es-maas/security/#adding-keys-with-cli","title":"Adding Keys with CLI","text":"<p>You could create the service credentials using the CLI, so you will add Reader role API Keys:</p> <ol> <li> <p>First you can explore the service credentials created previously, using the CLI with <code>$ ibmcloud resource service-key &lt;service_credentials_name&gt;</code>:</p> <pre><code>$ ibmcloud resource service-key demo-serv-cred\nRetrieving service key demo-serv-cred in all resource groups under account bill's Account as A......\n\nName:          demo-serv-cred\nID:            crn:v1:bluemix:public:messagehub:eu-de:a/b636d1d8.....8cfa:b05be932-2....02e687a:resource-key:4ba348d2-...-360e983d99c5\nCreated At:    Tue May 12 10:53:02 UTC 2020\nState:         active\nCredentials:\n            api_key:                  *****\n            apikey:                   *****\n            iam_apikey_description:   Auto-generated for key 4ba348d2-5fcf-4c13-a265-360e983d99c5\n            iam_apikey_name:          demo-serv-cred\n            iam_role_crn:             crn:v1:bluemix:public:iam::::serviceRole:Manager\n            iam_serviceid_crn:        crn:v1:bluemix:public:iam-identity::a/b636d1d83e34d7ae7e904591ac248cfa::serviceid:ServiceId-380e866c-5914-4e01-85c4-d80bd1b8a899\n            instance_id:              b05be932-2a60-4315-951d-a6dd902e687a\n            kafka_admin_url:          https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud\n            kafka_brokers_sasl:       [kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093]\n            kafka_http_url:           https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud\n            password:                 *****\n            user:                     token\n</code></pre> </li> <li> <p>Add a <code>Reader</code> role API key:</p> <pre><code># Get the name of the event streams service:\nibmcloud resource service-instances\nibmcloud resource service-key-create bn-lob1-app-reader Reader --instance-name \"Event Streams-wn\"\n\nOK\nService key crn:v1:bluemix:public:messagehub:eu-de:a/b6...248cfa:b05...e687a:resource-key:7ee00.....15a2 was created.\n\nName:          bn-lob1-reader\nID:            crn:v1:bluemix:public:messagehub:eu-de:a/b636d..fa:b05b..7a:resource-key:7ee0042.....b15a2\nCreated At:    Wed May 13 00:33:49 UTC 2020\nState:         active\nCredentials:\n            api_key:                  xrvMI4PQYmdOcUwPRUJXy6Xlo9UCY9xywNUPiU3jjpKH\n            apikey:                   xrvMI4PQYmdOcUwPRUJXy6Xlo9UCY9xywNUPiU3jjpKH\n            iam_apikey_description:   Auto-generated for key 7ee0042f-572b-46f6-b9cc-912cc63b15a2\n            iam_apikey_name:          bn-lob1-reader\n            iam_role_crn:             crn:v1:bluemix:public:iam::::serviceRole:Reader\n            iam_serviceid_crn:        crn:v1:bluemix:public:iam-identity::a/b636d1....48cfa::serviceid:ServiceId-b4d3....18af1\n            instance_id:              b05....687a\n            kafka_admin_url:          https://mh-tcqsppdpzlrkdmkbgmgl-4c201a...1c6f175-0000.eu-de.containers.appdomain.cloud\n            kafka_brokers_sasl:       [kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c2...175-0000.eu-de.containers.appdomain.cloud:9093....]\n            kafka_http_url:           https://mh-tcqsppdpzlrkdmkbgmgl-4c20...-0000.eu-de.containers.appdomain.cloud\n            password:                 xr......KH\n            user:                     token\n</code></pre> </li> </ol>"},{"location":"technology/event-streams/es-maas/security/#control-application-access","title":"Control application access","text":"<p>The last step of this lab is to add a specific IAM group for applications so you can link any future applications to a group so that producer and consumer codes using the Writer role API key as defined previously can read and write to specific topic as defined by an access policy.</p> <p>We want to give this Writer role API key to the <code>bn-lob1-app</code> group, so applications within this group can create topics and R/W on those topics with the name <code>bn-lob1-*</code>. To do so, we have to add to the Access Group <code>bn-lob1-app</code>, the service ID that maps the Key created above:</p> <ol> <li> <p>In IAM Access groups select the service IDs tab and click on add service id button:</p> <p></p> </li> <li> <p>Search for the service ID name: <code>bn-lob1-writer</code> and</p> <p></p> </li> <li> <p>You should get this result:</p> <p></p> </li> </ol>"},{"location":"technology/event-streams/es-maas/security/#kafka-streams-specifics","title":"Kafka Streams specifics","text":"<p>A lot of Kafka implementations use Kafka Streams API, in this case the applications need to have <code>Manager</code> role on topic resource, and Reader role for cluster and group with a Manager API Key.</p>"},{"location":"technology/event-streams/es-maas/security/#connecting-application-using-api-key","title":"Connecting application using API Key","text":"<p>To connect to Event Streams on cloud, we need to define consumer and producer common configuration as presented in the product documentation</p> <p>Here is an exemple of using reactive messaging in microprofile with the liberty kafka connector:</p> <pre><code>mp.messaging.connector.liberty-kafka.security.protocol=SASL_SSL\nmp.messaging.connector.liberty-kafka.ssl.protocol=TLSv1.2\nmp.messaging.connector.liberty-kafka.sasl.mechanism=PLAIN\nmp.messaging.connector.liberty-kafka.sasl.jaas.config=\"org.apache.kafka.common.security.plain.PlainLoginModule required username=token password=longapikeycomingfromservicecredential;\"\n</code></pre> <p>These properties could in fact be part of a secret in Kubernetes and used by the pod.</p>"},{"location":"technology/event-streams/es-maas/security/#encryption-at-rest","title":"Encryption at rest","text":"<p>Event Streams stores message data at rest and message logs on encrypted disks. Event Streams supports customer-managed encryption with provided key: bring you own key. This feature is available on the Enterprise plan only. The product documentation addresses in detail this capability.</p> <p>In this section, we just want to present some step by step to play with Key Protect, defining root key, and get Wrapped Keys to be used for encryption.</p>"},{"location":"technology/event-streams/es-maas/security/#create-a-key-protect-instance","title":"Create a Key Protect instance","text":"<p>In the <code>Create resource</code> from the IBM Cloud dashboard page, select a <code>Key Protect</code> service, add a name, and select a resource pool:</p> <p></p>"},{"location":"technology/event-streams/es-maas/security/#create-a-root-key-or-upload-yours","title":"Create a root key or upload yours","text":"<p>Using the <code>Manage</code> menu of Key Protect, and <code>Add key</code> button:</p> <p></p> <p>Specify to import your own key or create a new one. Give a name</p> <p></p> <p>Keys are symmetric 256-bit keys, supported by the AES-CBC-PAD algorithm. For added security, keys are generated by FIPS 140-2 Level 3 certified hardware security modules (HSMs) that are located in secure IBM Cloud data centers</p> <p></p> <p>Get the cloud resource name from the newly create key:</p> <p></p>"},{"location":"technology/event-streams/es-maas/security/#get-a-wrapped-key-with-api","title":"Get a Wrapped Key with API:","text":"<pre><code>export GUID=$(ibmcloud resource service-instance eda-KeyProtect --id | grep crn | awk '{print $2}\u2019)\nexport TOKEN=$(ibmcloud iam oauth-tokens | awk '{print $4 }'\n)\ncurl -X POST https://us-south.kms.cloud.ibm.com/api/v2/keys/34f2598f-e6e9-4822-8f51-cd9036e537e8?action=wrap -H 'accept: application/vnd.ibm.kms.key_action+json'\u00a0 \u00a0 -H 'authorization: Bearer $TOKEN' -H 'bluemix-instance: $GUID'\u00a0 -H 'content-type: application/vnd.ibm.kms.key_action+json'\n</code></pre>"},{"location":"technology/event-streams/es-maas/security/#authorize-event-streams-to-access-key-protect","title":"Authorize Event Streams to access Key Protect","text":"<p>In IAM application, go to the <code>Authorizations</code> menu:</p> <p></p> <p>And define event streams as the source service, you can specify a specific instance id or any event streams service. The target is the Key Protect instance. The service access is Reader.</p> <p></p> <p>You should get something like:</p> <p></p> <p>Now Event Streams could use your wrapped key to encrypte data at rest.</p> <p>Warning</p> <p>Temporarily des-authorizing Event Streams to access Key Protect, will block communication to Event Streams instance. Loosing any keys, will mean loosing the data. Restoring access by recreating the authorization between ES and Key Protect, will reopen traffic. When rotating the root key a new rewrapping of DEK is performed and the new key needs to be communicated to Event Streams.</p>"},{"location":"technology/faq/","title":"Kafka Frequently Asked Questions","text":"<p>Updated 03/29/2022</p>"},{"location":"technology/faq/#basic-questions","title":"Basic questions","text":""},{"location":"technology/faq/#what-is-kafka","title":"What is Kafka?","text":"<ul> <li>pub/sub middleware to share data between applications</li> <li>Open source, started in 2011 by Linkedin</li> <li>based on append log to persist immutable records ordered by arrival.</li> <li>support data partitioning, distributed brokers, horizontal scaling, low-latency and high throughput.</li> <li>producer has no knowledge of consumer</li> <li>records stay even after being consumed</li> <li>durability with replication to avoid loosing data for high availability</li> </ul>"},{"location":"technology/faq/#what-are-the-major-components","title":"What are the major components?","text":"<ul> <li>Topic, consumer, producer, brokers, cluster see this note for deep dive</li> <li>Rich API to control the producer semantic, and consumer</li> <li>Consumer groups. See this note for detail</li> <li>Kafka streams API to support data streaming with stateful operations and stream processing topology</li> <li>Kafka connect for source and sink connection to external systems</li> <li>Topic replication with Mirror Maker 2</li> </ul>"},{"location":"technology/faq/#what-are-major-use-cases","title":"What are major use cases?","text":"<ul> <li>Modern data pipeline with buffering to data lake</li> <li>Data hub, to continuously expose business entities to event-driven applications and microservices</li> <li>Real time analytics with aggregate computation, and complex event processing</li> <li>The communication layer for Event-driven, reactive microservice.</li> </ul>"},{"location":"technology/faq/#why-does-kafka-use-zookeeper","title":"Why does Kafka use zookeeper?","text":"<p>Kafka as a distributed system using cluster, it needs to keep cluster states, sharing configuration like topic, assess which node is still alive within the cluster, support registering new node added to the cluster, being able to support dynamic restart. Zookeeper is an orchestrator for distributed system, it maintains Kafka cluster integrity, select broker leader... </p> <p>Zookeeper is also used to manage offset commit, and to the leader selection process.</p> <p>Version 2.8 starts to get rid of Zookeeper so it uses another algorithm to define partition leadership and cluster health via one broker becoming the cluster controller. See this note on KIP 500</p>"},{"location":"technology/faq/#what-is-a-replica","title":"What is a replica?","text":"<p>A lit of nodes responsible to participate into the data replication process for a given partition. </p> <p>It is a critical feature to ensure durability, be able to continue to consume records, or to ensure a certain level of data loss safety is guaranteed when producing records.</p>"},{"location":"technology/faq/#what-are-a-leader-and-follower-in-kafka","title":"What are a leader and follower in Kafka?","text":"<p>Topic has 1 to many partition, which are append logs. Every partition in Kafka has a server that plays the role of leader. When replication is set in a topic, follower brokers will pull data from the leader to ensure replication, up to the specified replication factor.</p> <p>If the leader fails, one of the followers needs to take over as the leader\u2019s role. The leader election process involves zookeeper and assess which follower was the most in-synch with the leader.</p> <p>Leader is the end point for read and write operations on the partition. (Exception is the new feature to read from local follower).</p> <p>To get the list of In-synch Replication for a given topic the following tool can be used:</p> <pre><code>kafka-topics.sh --bootstrap-server :9092 --describe --topic &lt;topicname&gt;\n</code></pre>"},{"location":"technology/faq/#what-is-offset","title":"What is Offset?","text":"<p>A unique identifier of records inside a partition. It is automatically created by the broker, and producer can get it from the broker response.</p> <p>Consumer uses it to commit its read. It means, in case of consumer restarts, it will read from the last committed offset.</p>"},{"location":"technology/faq/#what-is-a-consumer-group","title":"What is a consumer group?","text":"<p>It groups consumers of one to many topics. Each partition is consumed by exactly one consumer within each subscribing consumer group.</p> <p>Consumer group is specified via the <code>group.id</code> consumer's property, and when consumers subscribe to topic(s).</p> <p>There is a protocol to manage consumers within a group so that partition can be reallocated when a consumer lefts the group. The group leader is responsible to do the partition assignment.</p> <p>When using the group.instance.id properties, consumer is treated as a static member, which means there will be no partition rebalance when consumer lefts a group for a short time period. When not set the group coordinator (a broker) will allocate ids to group members, and reallocation will occur. For Kafka Streams application it is recommended to use static membership.</p> <p>Brokers keep offsets until a retention period within which consumer group can lose all its consumers. After that period, offsets are discarded. The consumer group can be deleted manually, or automatically when the last committed offset for that group expires.</p> <p>When the group coordinator receives an OffsetCommitRequest, it appends the request to a special compacted Kafka topic named __consumer_offsets. Ack from the broker is done once all replicas on this hidden topics are successful.</p> <p>The tool <code>kafka-consumer-group.sh</code> helps getting details of consumer group:</p> <pre><code># Inside a Kafka broker container\nbin/kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group order-group --members --verbose\n</code></pre>"},{"location":"technology/faq/#how-to-support-multi-tenancy","title":"How to support multi-tenancy?","text":"<p>Multi-tenant means multiple different groups of application can produce and consumer messages isolated from other. So by constructs, topics and brokers are multi-tenant. Now the control will be at the access control level policy, the use of service account, and naming convention on the topic name. Consumer and producer authenticate themselves using dedicated service account users, with SCRAM user or Mutual TLS user. Each topic can have security policy to control read, write, creation operations.</p>"},{"location":"technology/faq/#how-client-access-kafka-cluster-metadata","title":"How client access Kafka cluster metadata?","text":"<p>Provide a list of Kafka brokers, minimum two, so the client API will get the metadata once connected to one of the broker.</p>"},{"location":"technology/faq/#how-to-get-at-most-once-delivery","title":"How to get at most once delivery?","text":"<p>Set producer acknowledge level (acks) property to 0 or 1.</p>"},{"location":"technology/faq/#how-to-support-exactly-once-delivery","title":"How to support exactly once delivery?","text":"<p>The goal is to address that if a producer sends a message twice the system will send only one message to the consumer, and once the consumer commits the read offset, it will not receive the message again even if it restarts.</p> <p>See the section in the producer implementation considerations note.</p> <p>The consumer needs to always read from its last committed offset.</p> <p>Also it is important to note that the Kafka Stream API supports exactly once semantics with the config: <code>processing.guarantee=exactly_once</code>. Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once.</p> <p>Exactly-once delivery for other destination systems generally requires cooperation with such systems which may be possible by using the offset processing.</p>"},{"location":"technology/faq/#retention-time-for-topic-what-does-it-mean","title":"Retention time for topic what does it mean?","text":"<p>The message sent to a cluster is kept for a max period of time or until a max size is reached. Those topic properties are: <code>retention.ms</code> and <code>retention.bytes</code>. Messages stay in the log even if they are consumed. The oldest messages are marked for deletion or compaction depending of the cleanup policy (delete or compact) set to <code>cleanup.policy</code> topic's parameter.</p> <p>See the Kafka documentation on topic configuration parameters.</p> <p>Here is a command to create a topic with specific retention properties:</p> <pre><code>bin/kafka-configs --zookeeper XX.XX.XX.XX:2181 --entity-type topics --entity-name orders --alter --add-config  retention.ms=55000 --add-config  retention.byte=100000\n</code></pre> <p>But there is also the <code>offsets.retention.minutes</code> property, set at the cluster level to control when the offset information will be deleted. It is defaulted to 1 day, but the max possible value is 7 days. This is to avoid keeping too much information in the broker memory and avoid to miss data when consumers do not run continuously. So consumers need to commit their offset. If the consumer settings define: <code>auto.offset.reset=earliest</code>, the consumer will reprocess all the events each time it restarts, (or skips to the latest if set to <code>latest</code>). When using <code>latest</code>, if the consumers are offline for more than the offsets retention time window, they will lose events.</p>"},{"location":"technology/faq/#what-are-the-topic-characteristics-i-need-to-define-during-requirements","title":"What are the topic characteristics I need to define during requirements?","text":"<p>This is a requirement gathering related question, to understand what need to be done for configuration topic configuration but also consumer and producer configuration, as well as retention strategy.</p> <ul> <li>Number of brokers in the cluster</li> <li>retention time and size</li> <li>Need for HA, set replicas to number of broker or at least the value of 3, with in-synch replica to 2</li> <li>Type of data to transport to assess message size</li> <li>Plan to use schema management to control change to the payload definition</li> <li>volume per day with peak and average</li> <li>Need to do geo replication to other Kafka cluster</li> <li>Network filesystem used on the target Kubernetes cluster and current storage class</li> </ul>"},{"location":"technology/faq/#what-are-the-impacts-of-having-not-enough-resource-for-kafka","title":"What are the impacts of having not enough resource for Kafka?","text":"<p>The table in this Event Streams product documentation illustrates  the resource requirements for a getting started cluster. When resources start to be at stress, then Kafka communication to ZooKeeper  and/or other Kafka brokers can suffer resulting in out-of-sync partitions and container restarts perpetuating the issue. Resource constraints  is one of the first things we consider when diagnosing ES issues.</p>"},{"location":"technology/faq/#security-configuration","title":"Security configuration","text":"<p>On Kubernetes, Kafka can be configured with external and internal URLs. With Strimzi internal URLs are using TLS or Plain authentication, then TLS for encryption. </p> <p>If no authentication property is specified then the listener does not authenticate clients which connect through that listener. The listener will accept all connections without authentication.</p> <ul> <li>Mutual TLS authentication for internal communication looks like:</li> </ul> <pre><code>- name: tls\n    port: 9093\n    type: internal\n    tls: true\n    authentication:\n      type: tls\n</code></pre> <p>To connect any app (producer, consumer) we need a TLS user like:</p> <pre><code>piVersion: kafka.strimzi.io/v1beta2\nkind: KafkaUser\nmetadata:\n  name: tls-user\n  labels:\n    strimzi.io/cluster: vaccine-kafka\nspec:\n  authentication:\n    type: tls\n</code></pre> <p>Then the following configurations need to be done for each app. For example in Quarkus app, we need to specify where to find the client certificate (for each Kafka TLS user a secret is created with the certificate (ca.crt) and a user password)</p> <pre><code>oc describe secret tls-user\nData\n====\nca.crt:         1164 bytes\nuser.crt:       1009 bytes\nuser.key:       1704 bytes\nuser.p12:       2374 bytes\nuser.password:  12 bytes\n</code></pre> <p>For Java client we need the following security settings, to specify from which secret to get the keystore password and certificate. The certificate will be mounted to <code>/deployments/certs/user</code>. </p> <pre><code>%prod.kafka.security.protocol=SSL\n%prod.kafka.ssl.keystore.location=/deployments/certs/user/user.p12\n%prod.kafka.ssl.keystore.type=PKCS12\nquarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.from-secret=${KAFKA_USER:tls-user}\nquarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.with-key=user.password\nquarkus.openshift.mounts.user-cert.path=/deployments/certs/user\nquarkus.openshift.secret-volumes.user-cert.secret-name=${KAFKA_USER:tls-user}\n# To validate server side certificate we will mount it too with the following declaration\nquarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\nquarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key=ca.password\nquarkus.openshift.mounts.kafka-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.kafka-cert.secret-name=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\n</code></pre> <p>For the server side certificate, it will be in a truststore, which is mounted to  <code>/deployments/certs/server</code> and from a secret (this secret is created at the cluster level).</p> <p>Also because we also use TLS for encryption we need:</p> <pre><code>%prod.kafka.ssl.protocol=TLSv1.2\n</code></pre> <p>Mutual TLS authentication is always used for the communication between Kafka brokers and ZooKeeper pods. For mutual, or two-way, authentication, both the server and the client present certificates.</p> <ul> <li> <p>SCRAM: (Salted Challenge Response Authentication Mechanism) is an authentication protocol that can establish mutual authentication using passwords. Strimzi can configure Kafka to use SASL (Simple Authentication and Security Layer) SCRAM-SHA-512 to provide authentication on both unencrypted and encrypted client connections.</p> <ul> <li>The listener declaration:</li> </ul> <pre><code>- name: external\nport: 9094\ntype: route\ntls: true\nauthentication:\n  type: scram-sha-512\n</code></pre> <ul> <li>Need a scram-user:</li> </ul> <pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaUser\nmetadata:\nname: scram-user\nlabels:\n    strimzi.io/cluster: vaccine-kafka\nspec:\nauthentication:\n    type: scram-sha-512\n</code></pre> </li> </ul> <p>Then the app properties need to have:</p> <pre><code>security.protocol=SASL_SSL\n%prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\n%prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key=ca.password\n%prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.from-secret=${KAFKA_USER:scram-user}\n%prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.with-key=password\n%prod.quarkus.openshift.mounts.kafka-cert.path=/deployments/certs/server\n%prod.quarkus.openshift.secret-volumes.kafka-cert.secret-name=${KAFKA_CA_CERT_NAME:kafka-cluster-ca-cert}\n</code></pre>"},{"location":"technology/faq/#verify-consumer-connection","title":"Verify consumer connection","text":"<p>Here is an example of TLS authentication for Event streams</p> <pre><code>ConsumerConfig values: \n    bootstrap.servers = [eda-dev-kafka-bootstrap.eventstreams.svc:9093]\n    check.crcs = true\n    client.dns.lookup = default\n    client.id = cold-chain-agent-c2c11228-d876-4db2-a16a-ea7826e358d2-StreamThread-1-restore-consumer\n    client.rack = \n    connections.max.idle.ms = 540000\n    default.api.timeout.ms = 60000\n    enable.auto.commit = false\n    exclude.internal.topics = true\n    fetch.max.bytes = 52428800\n    fetch.max.wait.ms = 500\n    fetch.min.bytes = 1\n    group.id = null\n    group.instance.id = null\n    heartbeat.interval.ms = 3000\n    interceptor.classes = []\n    internal.leave.group.on.close = false\n    isolation.level = read_uncommitted\n    key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n    sasl.client.callback.handler.class = null\n    sasl.jaas.config = null\n    sasl.kerberos.kinit.cmd = /usr/bin/kinit\n    sasl.kerberos.min.time.before.relogin = 60000\n    sasl.kerberos.service.name = null\n    sasl.kerberos.ticket.renew.jitter = 0.05\n    sasl.kerberos.ticket.renew.window.factor = 0.8\n    sasl.login.callback.handler.class = null\n    sasl.login.class = null\n    sasl.login.refresh.buffer.seconds = 300\n    sasl.login.refresh.min.period.seconds = 60\n    sasl.login.refresh.window.factor = 0.8\n    sasl.login.refresh.window.jitter = 0.05\n    sasl.mechanism = GSSAPI\n    security.protocol = SSL\n    security.providers = null\n    send.buffer.bytes = 131072\n    session.timeout.ms = 10000\n    ssl.cipher.suites = null\n    ssl.enabled.protocols = [TLSv1.2]\n    ssl.endpoint.identification.algorithm = https\n    ssl.key.password = null\n    ssl.keymanager.algorithm = SunX509\n    ssl.keystore.location = /deployments/certs/user/user.p12\n    ssl.keystore.password = [hidden]\n    ssl.keystore.type = PKCS12\n    ssl.protocol = TLSv1.2\n    ssl.provider = null\n    ssl.secure.random.implementation = null\n    ssl.trustmanager.algorithm = PKIX\n    ssl.truststore.location = /deployments/certs/server/ca.p12\n    ssl.truststore.password = [hidden]\n    ssl.truststore.type = PKCS12\n    value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n</code></pre>"},{"location":"technology/faq/#what-security-support-in-kafka","title":"What Security support in Kafka","text":"<ul> <li>Encrypt data in transit between producer and Kafka brokers</li> <li>Client authentication</li> <li>Client authorization</li> </ul>"},{"location":"technology/faq/#how-to-protect-data-at-rest","title":"How to protect data at rest?","text":"<ul> <li>Use encrypted file system for each brokers</li> <li>Encrypt data at the producer level, using some API, and then decode at the consumer level. The data in the appeld log will be encrypted.</li> </ul>"},{"location":"technology/faq/#more-advanced-concepts","title":"More advanced concepts","text":""},{"location":"technology/faq/#what-is-range-partition-assignment-strategy","title":"What is range partition assignment strategy?","text":"<p>There are multiple partition assignment strategy for a consumer, part of a consumer group , to get its partition to fetch data from. Members of the consumer group subscribe to the topics they are interested in and forward their subscriptions to a Kafka broker serving as the group coordinator. The coordinator selects one member to perform the group assignment and propagates the subscriptions of all members to it. Then assign(Cluster, GroupSubscription) is called to perform the assignment and the results are forwarded back to each respective members.</p> <p>Range assignor works on a per-topic basis: it lays out the available partitions in numeric order and the consumers in lexicographic order, and assign partition to each consumer so partition with the same id will be in the same consumer: topic-1-part-0 and topic-2-part-0 will be processed by consumer-0</p>"},{"location":"technology/faq/#what-is-sticky-assignor","title":"What is sticky assignor?","text":"<p>The CooperativeStickyAssignor helps supporting incremental cooperative rebalancing to the clients' group protocol, which allows consumers to keep all of their assigned partitions during a rebalance and at the end revoke only those which must be migrated to another consumer for overall cluster balance.</p> <p>The goal is to reduce unnecessary downtime due to unnecessary partition migration, by leveraging the sticky assignor which link consumer to partition id. See KIP 429 for details. </p>"},{"location":"technology/faq/#how-to-get-an-homogeneous-distribution-of-message-to-partitions","title":"How to get an homogeneous distribution of message to partitions?","text":"<p>Design the message key and hash coding for even distributed. Or implement a customer partitioner by implementing the Partitioner interface. </p>"},{"location":"technology/faq/#how-to-ensure-efficient-join-between-two-topics","title":"How to ensure efficient join between two topics?","text":"<p>Need to use co-partitioning, which means having the same key in both topic, the same number of partitions and the same producer partitioner, which most likely should be the default one that uses the following formula: partition = hash(key) % numPartitions.</p>"},{"location":"technology/faq/#what-is-transaction-in-kafka","title":"What is transaction in Kafka?","text":"<p>Producer can use transaction begin, commit and rollback API while publishing events to a multi partition topic. This is done by setting a unique transactionId as part of its configuration (with idempotence and min  inflight record set to 1).  Either all messages are successfully written or none of them are.</p> <p>There are some producer exception to consider to abort the transaction: any KafkaException for sure, but also OutOfSequenceTx which may happen when the PID is greater than the last one seen by the producer.</p> <p>See explanations here.</p> <p>And the KIP 98</p>"},{"location":"technology/faq/#what-is-the-high-watermark","title":"What is the high watermark?","text":"<p>The high watermark offset is the offset of the last message that was successfully copied to all of the log\u2019s replicas. A consumer can only read up to the high watermark offset to prevent reading un-replicated messages.</p>"},{"location":"technology/faq/#what-should-we-do-for-queue-full-exception-or-timeout-exception-on-producer","title":"What should we do for queue full exception or timeout exception on producer?","text":"<p>The brokers are running behind, so we need to add more brokers and redistribute partitions.</p>"},{"location":"technology/faq/#how-to-send-large-messages","title":"How to send large messages?","text":"<p>We can set some properties at the broker, topic, consumer and producer level:</p> <ul> <li>Broker: consider the message.max.bytes and replica.fetch.max.bytes</li> <li>Consumer: max.partition.fetch.bytes. Records are fetched in batches by the consumer, so this properties gives the max amount of data per partition the server will return. Default 1 Megabyte</li> </ul>"},{"location":"technology/faq/#how-to-maximize-throughput","title":"How to maximize throughput?","text":"<p>For producer if you want to maximize throughput over low latency, set batch.size and linger.ms to higher value. Linger delay producer, it will wait for up to the given delay to allow other records to be sent so that the sends can be batched together.</p>"},{"location":"technology/faq/#why-kafka-stream-applications-may-impact-cluster-performance","title":"Why Kafka Stream applications may impact cluster performance?","text":"<ul> <li>They may use internal hidden topics to persist their states for Ktable and GlobalKTable.</li> <li>Process input and output topics</li> </ul>"},{"location":"technology/faq/#how-message-schema-version-is-propagated","title":"How message schema version is propagated?","text":"<p>The record includes a byte with the version number from the schema registry.</p>"},{"location":"technology/faq/#consumers-do-not-see-message-in-topic-what-happens","title":"Consumers do not see message in topic, what happens?","text":"<p>The brokers may have an issue on this partition. If a broker, part of the ISR list fails, then new leader election may delay the broker commit from a producer.</p> <p>The consumer has a communication issue, or fails, so the consumer group rebalance is underway.</p>"},{"location":"technology/faq/#how-compression-schema-used-is-known-by-the-consumer","title":"How compression schema used is known by the consumer?","text":"<p>The record header includes such metadata. So it is possible to have different schema per record.</p>"},{"location":"technology/faq/#what-does-out-of-synch-partition-mean-and-when-it-occurs","title":"What does out-of-synch partition mean and when it occurs?","text":"<p>With partition leader and replication to the followers, the number of in-synch replicas is at least the number of expected replicas. For example for a replicas = 3 the in-synch is set to 2, and it represents the minimum number of replicas that must acknowledge a write for the write to be considered successful. The record is considered \u201ccommitted\u201d when all ISRs for a partition wrote to their log. Only committed records are readable from consumer.</p> <p>So out-of-synch will happen if the followers are not able to send their acknowledge to the replica leader as quickly as expected.</p>"},{"location":"technology/faq/#run-kafka-test-container-with-topologytestdriver","title":"Run Kafka Test Container with TopologyTestDriver","text":"<p>Topology Test Driver is used without kafka, so there is no real need to use test container. </p>"},{"location":"technology/faq/#how-to-remove-personal-identifying-information","title":"How to remove personal identifying information?","text":"<p>From the source connector, it is possible to add processing class to process the records before publishing them to Kafka topic, so that any Kafka Streams apps will not see PII.</p>"},{"location":"technology/faq/#how-to-handle-variable-workload-with-kafka-connector-source-connector","title":"How to handle variable workload with Kafka Connector source connector?","text":"<p>Increase and decrease the number of Kafka connect workers based upon current application load.</p>"},{"location":"technology/faq/#derived-products-related-questions","title":"Derived products related questions","text":""},{"location":"technology/faq/#competitors-to-kafka","title":"Competitors to Kafka","text":"<ul> <li>NATS</li> <li>Redpanda a Modern streaming platform for mission critical workloads, and is compatible with Kafka API. It is a cluster of brokers without any zookeepers. It also leverage the SSD technology to improve I/O operations.</li> <li> <p>AWS Kinesis</p> <ul> <li>Cloud service, managed by AWS staff, paid as you go, proportional to the shard (like partition) used.</li> <li>24h to 7 days persistence</li> <li>Number of shards are adaptable with throughput.</li> <li>Uses the concept of Kinesis data streams, which uses shards: data records are composed of a sequence number, a partition key and a data blob.</li> <li>restrictions on message size (1 MB) and consumption rate of messages (5 reads /s, &lt; 2MB per shard, 1000 write /s)</li> <li>Server side encryption using master key managed by AWS KMS</li> </ul> </li> <li> <p>GCP Pub/sub</p> </li> <li>Solace</li> <li> <p>Active MQ:</p> <ul> <li>Java based messaging server to be the JMS reference implementation, so it supports transactional messaging. </li> <li>various messaging protocols including AMQP, STOMP, and MQTT</li> <li>It maintains the delivery state of every message resulting in lower throughput.</li> <li>Can apply JMS message selector to consumer specific message</li> <li>Point to point or pub/sub, but servers push messages to consumer/subscribers</li> <li>Performance of both queue and topic degrades as the number of consumers rises</li> </ul> </li> <li> <p>Rabbit MQ:</p> <ul> <li>Support queues, with messages removed once consumed</li> <li>Add the concept of Exchange to route message to queues</li> <li>Limited throughput, but can send large message</li> <li>Support JMS, AMQP protocols, and participation to transaction</li> <li>Smart broker / dumb consumer model that focuses on consistently delivering messages to consumers.</li> </ul> </li> </ul>"},{"location":"technology/faq/#differences-between-amq-streams-and-confluent","title":"Differences between AMQ Streams and Confluent","text":"<p>AMQ Streams and Confluent are based on the open source Kafka, but Confluent as the main contributer to Kafka, is adding proprietary features to make the product more marketable,  so we will not do a pure features comparison a generic features comparison:</p> Feature Confluent AMQ Streams Kafka open source Aligned within a month to the Kafka release Within 2 months after Kafka release Kafka API Same Same k8s / OpenShift deployment Helm \"operator\" Real Kubernetes Operator based on open source Strimzi Kafka Connectors Connectors hub to reference any connectors on the market, with some certified for Confluent. Open source connectors supported. Apache Camel offers a set of connectors not directly supported by Red Hat but useful in a BYO connectors. Fuse and Debezium can be used. Schema registry Proprietary API and schema Solution may leverage open source Apicur.io schema registry which is compatible with Confluent API. Cruise control for auto cluster balancing Adds on Available via Operator Mirroring between clusters Replicator tool Mirror Maker 2 deployable and managed by Strimzi operator Multi region cluster Supported Supported Role Based access control Supported Supported with explicit user manifest, integrated with Red Hat SSO and OPA. ksql Open sourced licensed by Confluent Customer can use open source version of kSQL but meed to verify licensing for cloud deployment. SQL processing on Kafka Records may also being done with Apache Flink Kafka Streams Supported from Kafka Open Source Supported from Kafka Open Source. Also moving CEP and Streams processing to an external tool makes a lot more sense. Apache Flink should be considered. Not directly supported by Red Hat Storage NFS and tiered storage Block storage with replication to s3 buckets for long persisence using Kafka connector. S3 Connector is not supported by Red Hat. As a managed service Proprietary solution Same with: IBM Event Streams and Red Hat AMQ streams as a service Integration with IBM mainframe Not strategic - Weak Strong with IBM connector and deployment on Z and P Admin User Interface Control center Operator in OpenShift and 3nd party open-source user interface like Kafdrop, Kowl, work with AMQ Streams but without direct support from Red Hat <p>As Kafka adoption is a strategic investment, it is important to grow the competency and skill set to manage kafka clusters.  Zookeeper was an important element to complexify the cluster management, as 2.8 it is removed, so it should be simpler to manage cluster.</p> <p>With customers cann influence the product roadmap, but it will kill the open source approach if only Confluent committers prioritize the feature requets.  It is important to keep competitions and multi committers.</p>"},{"location":"technology/faq/#event-streams-resource-requirements","title":"Event streams resource requirements","text":"<p>See the detailed tables in the product documentation.</p>"},{"location":"technology/faq/#differences-between-akka-and-kafka","title":"Differences between Akka and Kafka?","text":"<p>Akka is a open source toolkit for Scala or Java to simplify multithreading programming and makes application more reactive by adopting an asynchronous mechanism to access to io: database or HTTP request. To support asynchronous communication between 'actors', it uses messaging, internal to the JVM.  Kafka is part of the architecture, while Akka is an implementation choice for one of the component of the business application deployed inside the architecture.</p> <p>vert.x is another open source implementation of such internal messaging mechanism but supporting more language:  Java, Groovy, Ruby, JavaScript, Ceylon, Scala, and Kotlin.</p>"},{"location":"technology/faq/#is-is-possible-to-stream-video-to-kafka","title":"Is is possible to stream video to kafka?","text":"<p>Yes it is possible, but need to do that with care and real justification. If the goal is to classify streamed images, then it is possible to do so, and need to assess if it will be important to be streams versus video at rest.</p> <p>The following article, from Neeraj Krishna illustrates a python implementation to send video frame every 3 images, do image classification using  ResNet50 model trained on ImageNet, embbeded in python consumer. The results are saved in mongodb with the metadata needed to query after processing.</p>"},{"location":"technology/faq/#other-faqs","title":"Other FAQs","text":"<ul> <li> <p>IBM Event streams on Cloud FAQ </p> </li> <li> <p>FAQ from Confluent</p> </li> </ul>"},{"location":"technology/flink/","title":"Apache Flink Technology Summary","text":"<p>Warning</p> <p>Updated 08/20/2022- Work in progress</p>"},{"location":"technology/flink/#why-flink","title":"Why Flink?","text":"<p>In classical IT architecture, we can see two types of data processing: transactional and analytics.  With 'monolytics' application, the database system serves multiple applications which sometimes access the same database  instances and tables. This approach cause problems to support evolution and scaling.  Microservice architecture addresses part of those problems by isolating data storage per service. </p> <p>To get insight from the data, the traditional approach is to develop data warehouse and ETL jobs to copy and transform data  from the transactional systems to the warehouse. ETL process extracts data from a transactional database, transforms data  into a common representation that might include validation, value normalization, encoding, deduplication, and schema  transformation, and finally loads the new record into the target analytical database. They are batches and run periodically.</p> <p>From the data warehouse, the analysts build queries, metrics, and dashboards / reports to address a specific business question.  Massive storage is needed, which uses different protocol such as: NFS, S3, HDFS...</p> <p>Today, there is a new way to think about data by seeing they are created as continuous streams of events, which can be processed  in real time, and serve as the foundation for stateful stream processing application: the analytics move to the real data stream.</p> <p>We can define three classes of applications implemented with stateful stream processing:</p> <ol> <li>Event-driven applications: to adopt the reactive manifesto for scaling, resilience, responsive application, leveraging messaging as communication system.</li> <li>Data pipeline applications: replace ETL with low latency stream processing.</li> <li>Data analytics applications: immediatly act on the data and query live updated reports. </li> </ol> <p>For more real industry use cases content see the Flink Forward web site.</p>"},{"location":"technology/flink/#the-what","title":"The What","text":"<p>Apache Flink (2016) is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink supports batch (data set )and graph (data stream) processing. It is very good at:</p> <ul> <li>Very low latency processing event time semantics to get consistent and accurate results even in case of out of order events</li> <li>Exactly once state consistency </li> <li>Millisecond latencies while processing millions of events per second</li> <li>Expressive and easy-to-use APIs: map, reduce, join, window, split, and connect.</li> <li>SQL support to implement user friendly streaming queries</li> <li>Fault tolerance, and high availability: supports worker and master failover, eliminating any single point of failure</li> <li>A lot of connectors to integrate with Kafka, Cassandra, Elastic Search, JDBC, S3...</li> <li>Support container and deployment on Kubernetes</li> <li>Support updating the application code and migrate jobs to different Flink clusters without losing the state of the application</li> <li>Also support batch processing</li> </ul> <p>The figure below illustrates those different models combined with Zepellin as a multi purpose notebook to develop data analytic projects on top of Spark, Python or Flink.</p> <p></p>"},{"location":"technology/flink/#flink-architecture","title":"Flink architecture","text":"<p>Flink consists of a Job Manager and n Task Managers. </p> <p>The JobManager controls the execution of a single application. It receives an application for execution and builds a Task Execution Graph from the defined Job Graph. It manages job submission and the job lifecycle then allocates work to Task Managers The Resource Manager manages Task Slots and leverages underlying orchestrator, like Kubernetes or Yarn. A Task slot is the unit of work executed on CPU. The Task Managers execute the actual stream processing logic. There are multiple task managers running in a cluster. The number of slots limits the number of tasks a TaskManager can execute. After it has been started, a TaskManager registers its slots to the ResourceManager</p> <p></p> <p>The Disparcher exposes API to submit applications for execution. It hosts the user interface too.</p> <p>Only one Job Manager is active at a given point of time, and there may be <code>n</code> Task Managers.</p> <p>There are different deployment models: </p> <ul> <li>Deploy on executing cluster, this is the session mode. Use session cluster to run multiple jobs: we need a JobManager container. </li> <li>Per job mode, spin up a cluster per job submission. More k8s oriented. This provides better resource isolation. </li> <li>Application mode creates a cluster per app with the main() function executed on the JobManager. It can include multiple jobs but they run inside the app. It allows for saving the required CPU cycles, but also save the bandwidth required for downloading the dependencies locally.</li> </ul> <p>Flink can run on any common resource manager like Hadoop Yarn, Mesos, or Kubernetes. For development purpose, we can use docker images to deploy a Session or Job cluster.</p>"},{"location":"technology/flink/#batch-processing","title":"Batch processing","text":"<p>Process all the data in one job with bounded dataset. It is used when we need all the data for assessing trend, develop AI model, and with a focus on throughput instead of latency.</p> <p>Hadoop was designed to do batch processing. Flink has capability to replace Hadoop map reduce processing.</p>"},{"location":"technology/flink/#high-availability","title":"High Availability","text":"<p>With Task managers running in parallel, if one fails the number of available slots drops by the JobManager asks the Resource Manager to get new processing slots. The application's restart strategy determines how often the JobManager restarts the application and how long it waits between restarts.</p> <p>Flink uses Zookeeper to manage multiple JobManagers and select the leader to control the execution of the streaming application. Application's tasks checkpoints and other states are saved in a remote storage, but metadata are saved in Zookeeper. When a JobManager fails, all tasks that belong to its application are automatically cancelled. A new JobManager that takes over the work by getting information of the storage from Zookeeper, and then restarts the process with the JobManager.</p>"},{"location":"technology/flink/#stream-processing-concepts","title":"Stream processing concepts","text":"<p>In Flink, applications are composed of streaming dataflows that may be transformed by user-defined operators. These dataflows form directed graphs that start with one or more sources, and end in one or more sinks. The data flows between operations.  The figure below, from product documentation, summarizes the simple APIs used to develop a data stream processing flow:</p> <p></p> <p>src: apache Flink product doc</p> <p>Stream processing includes a set of functions to transform data, to produce a new output stream. Intermediate steps compute rolling aggregations like min, max, mean, or collect and buffer records in time window to compute metrics on finite set of events.  To properly define window operator semantics, we need to determine both how events are assigned to buckets and how often the window produces a result. Flink's streaming model is based on windowing and checkpointing, it uses controlled cyclic dependency graph  as its execution engine.</p> <p>The following figure is showing integration of stream processing runtime with an append log system, like Kafka, with internal local state persistence and continuous checkpoint to remote storage as HA support:</p> <p></p> <p>As part of the checkpointing process, Flink saves the 'offset read commit' information of the append log, so in case of a failure, Flink recovers a stateful streaming application by restoring its state from a previous checkpoint and resetting the read position on the append log.</p> <p>The evolution of microservice is to become more event-driven, which are stateful streaming applications that ingest event streams and process the events with application-specific business logic. This logic can be done in flow defined in Flink and executed in the clustered runtime.</p> <p></p> <p>A lot of predefined connectors exist to connect to specific source and sink. Transform operators can be chained. Dataflow can consume from Kafka, Kinesis, Queue, and any data sources. A typical high level view of Flink app is presented in figure below:</p> <p></p> <p>src: apache Flink product doc</p> <p>Programs in Flink are inherently parallel and distributed. During execution, a stream has one or more stream partitions, and each operator has one or more operator subtasks.</p> <p></p> <p>src: apache Flink site</p> <p>A Flink application, can be stateful, run in parallel on a distributed cluster. The various parallel instances of a given operator will execute independently, in separate threads, and in general will be running on different machines. State is always accessed local, which helps Flink applications achieve high throughput and low-latency. You can choose to keep state on the JVM heap, or if it is too large, saves it in efficiently organized on-disk data structures.</p> <p></p> <p>This is the Job Manager component which parallelizes the job and distributes slices of the Data Stream flow, you defined, to the Task Managers for execution. Each parallel slice of your job will be executed in a task slot.</p> <p></p> <p>Once Flink is started (for example with the docker image), Flink Dashboard http://localhost:8081/#/overview presents the execution reporting of those components:</p> <p></p> <p>The execution is from one of the training examples, the number of task slot was set to 4, and one job is running.</p> <p>Spark is not a true real time processing while Flink is. Flink and Spark support batch processing too. </p>"},{"location":"technology/flink/#stateless","title":"Stateless","text":"<p>Some applications support data loss and expect fast recovery times in case of failure and  always consuming the latest incoming data.  Alerting applications where only low latency alerts are useful, or application where only the last data received is relevant. </p> <p>When checkpointing is turned off Flink offers no inherent guarantees in case of failures. This means that you can  either have data loss or duplicate messages combined always with a loss of application state.</p>"},{"location":"technology/flink/#statefulness","title":"Statefulness","text":"<p>When using aggregates or windows operators, states need to be kept. For fault tolerant Flink uses checkpoints and savepoints.  Checkpoints represent a snapshot of where the input data stream is with each operator's state. A streaming dataflow can be resumed from a checkpoint while maintaining consistency (exactly-once processing semantics) by restoring the state of the operators and by replaying the records from the point of the checkpoint.</p> <p>In case of failure of a parallel execution, Flink stops the stream flow, then restarts operators from the last checkpoints. When doing the reallocation of data partition for processing, states are reallocated too.  States are saved on distributed file systems. When coupled with Kafka as data source, the committed read offset will be part of the checkpoint data.</p> <p>Flink uses the concept of <code>Checkpoint Barriers</code>, which represents a separation of records, so records received since the last snapshot are part of the future snapshot. Barrier can be seen as a mark, a tag in the data stream that close a snapshot. </p> <p></p> <p>In Kafka, it will be the last committed read offset. The barrier flows with the stream so can be distributed. Once a sink operator (the end of a streaming DAG) has received the <code>barrier n</code> from all of its input streams, it acknowledges that <code>snapshot n</code> to the checkpoint coordinator.  After all sinks have acknowledged a snapshot, it is considered completed. Once <code>snapshot n</code> has been completed, the job will never ask the source for records before such snapshot.</p> <p>State snapshots are save in a state backend (in memory, HDFS, RockDB). </p> <p>KeyedStream is a key-value store. Key match the key in the stream, state update does not need transaction.</p> <p>For DataSet (Batch processing) there is no checkpoint, so in case of failure the stream is replayed. When addressing exactly once processing it is very important to consider the following:</p> <ol> <li>the read from the source</li> <li>apply the processing logic like window aggregation</li> <li>generate the results to a sink</li> </ol> <p>1 and 2 can be done exactly once, using Flink source connector and checkpointing but generating one unique result to a sink is more complex and  is dependant of the target technology. </p> <p></p> <p>After reading records from Kafka, do the processing and generate results, in case of failure Flink will reload the record from the read offset and may generate duplicate in the Sink. </p> <p></p> <p>As duplicates will occur, we always need to assess idempotent support from downstream applications. A lot of distributed key-value storages support consistent result event after retries.</p> <p>To support end-to-end exactly one delivery we need to have a sink that supports transaction and two-phase commit. In case of failure we need to rollback the output generated. It is important to note  transactional output impacts latency.</p> <p>Flink takes checkpoints periodically, like every 10 seconds, which leads to the minimum latency we can expect at the sink level.</p> <p>For Kafka Sink connector, as kafka producer, we need to set the <code>transactionId</code>, and the delivery type:</p> <pre><code>new KafkaSinkBuilder&lt;String&gt;()\n    .setBootstrapServers(bootstrapURL)\n    .setDeliverGuarantee(DeliveryGuarantee.EXACTLY_ONCE)\n    .setTransactionalIdPrefix(\"store-sol\")\n</code></pre> <p>With transaction ID, a sequence number is sent by the kafka producer API to the broker, and so the partition leader will be able to remove duplicate retries.</p> <p></p> <p>When the checkpointing period is set, we need to also configure <code>transaction.max.timeout.ms</code> of the Kafka broker and <code>transaction.timeout.ms</code> for the producer (sink connector) to a higher timeout than the checkpointing interval plus the max expected Flink downtime. If not the Kafka broker will consider the connection has fail and will remove its state management.</p>"},{"location":"technology/flink/#state-management","title":"State management","text":"<ul> <li>All data maintained by a task and used to compute the results of a function belong to the state of the task.</li> <li>While processing the data, the task can read and update its state and compute its result based on its input data and state.</li> <li>State management includes address very large states, and no state is lost in case of failures.</li> <li>Each operator needs to register its state.</li> <li>Operator State is scoped to an operator task: all records processed by the same parallel task have access to the same state</li> <li>Keyed state is maintained and accessed with respect to a key defined in the records of an operator\u2019s input stream. Flink maintains one state instance per key value and Flink partitions all records with the same key to the operator task that maintains the state for this key. The key-value map is sharded across all parallel tasks:</li> </ul> <ul> <li>Each task maintains its state locally to improve latency. For small state, the state backends will use JVM heap, but for larger state RocksDB is used. A state backend takes care of checkpointing the state of a task to a remote and persistent storage.</li> <li>With stateful distributed processing, scaling stateful operators, enforces state repartitioning and assigning to more or fewer parallel tasks. Keys are organized in key-groups, and key groups are assigned to tasks. Operators with operator list state are scaled by redistributing the list entries. Operators with operator union list state are scaled by broadcasting the full list of state entries to each task.</li> </ul> <p>Flink uses Checkpointing to periodically store the state of the various stream processing operators on durable storage. </p> <p></p> <p>When recovering from a failure, the stream processing job can resume from the latest checkpoint. </p> <p></p> <p>Checkpointing is coordinated by the Job Manager, it knows the location of the latest completed checkpoint which will get important later on. This checkpointing and recovery mechanism can provide exactly-once consistency for application state, given that all operators checkpoint and restore all of their states and that all input streams are reset to the position up to which they were consumed when the checkpoint was taken. This will work perfectly with Kafka, but not with sockets or queues where messages are lost once consumed. Therefore exactly-once state consistency can be ensured only if all input streams are from resettable data sources.</p> <p>During the recovery and depending on the sink operators of an application, some result records might be emitted multiple times to downstream systems.</p>"},{"location":"technology/flink/#windowing","title":"Windowing","text":"<p>Windows are buckets within a Stream and can be defined with times, or count of elements.</p> <ul> <li>Tumbling window assign events into nonoverlapping buckets of fixed size. When the window border is passed, all the events are sent to an evaluation function for processing. Count-based tumbling windows define how many events are collected before triggering evaluation. Time based timbling window define time interval of n seconds. Amount of the data vary in a window. <code>.keyBy(...).window(TumblingProcessingTimeWindows.of(Time.seconds(2)))</code></li> </ul> <p></p> <ul> <li>Sliding window: same but windows can overlap. An event might belong to multiple buckets. So there is a <code>window sliding time</code> parameter: <code>.keyBy(...).window(SlidingProcessingTimeWindows.of(Time.seconds(2), Time.seconds(1)))</code></li> </ul> <p></p> <ul> <li>Session window: Starts when the data stream processes records and stop when there is inactivity, so the timer set this threshold: <code>.keyBy(...).window(ProcessingTimeSessionWindows.withGap(Time.seconds(5)))</code>. The operator creates one window for each data element received.</li> </ul> <p></p> <ul> <li> <p>Global window: one window per key and never close. The processing is done with Trigger:</p> <pre><code>.keyBy(0)\n.window(GlobalWindows.create())\n.trigger(CountTrigger.of(5))\n</code></pre> </li> </ul> <p>KeyStream can help to run in parallel, each window will have the same key.</p> <p>Time is central to the stream processing, and the time is a parameter of the flow / environment and can take different meanings:</p> <ul> <li><code>ProcessingTime</code> = system time of the machine executing the task: best performance and low latency</li> <li><code>EventTime</code> = the time at the source level, embedded in the record. Deliver consistent and deterministic results regardless of order </li> <li><code>IngestionTime</code> = time when getting into Flink. </li> </ul> <p>See example TumblingWindowOnSale.java and to test it, do the following:</p> <pre><code># Start the SaleDataServer that starts a server on socket 9181 and will read the avg.txt file and send each line to the socket\njava -cp target/my-flink-1.0.0-SNAPSHOT.jar jbcodeforce.sale.SaleDataServer\n# inside the job manager container start with \n`flink run -d -c jbcodeforce.windows.TumblingWindowOnSale /home/my-flink/target/my-flink-1.0.0-SNAPSHOT.jar`.\n# The job creates the data/profitPerMonthWindowed.txt file with accumulated sale and number of record in a 2 seconds tumbling time window\n(June,Bat,Category5,154,6)\n(August,PC,Category5,74,2)\n(July,Television,Category1,50,1)\n(June,Tablet,Category2,142,5)\n(July,Steamer,Category5,123,6)\n...\n</code></pre>"},{"location":"technology/flink/#trigger","title":"Trigger","text":"<p>Trigger determines when a window is ready to be processed. All windows have default trigger. For example tumbling window has a 2s trigger. Global window has explicit trigger. We can implement our own triggers by implementing the Trigger interface with different methods to implement: onElement(..), onEventTime(...), onProcessingTime(...)</p> <p>Default triggers:</p> <ul> <li>EventTimeTrigger: fires based upon progress of event time</li> <li>ProcessingTimeTrigger: fires based upon progress of processing time</li> <li>CountTrigger: fires when # of element in a window &gt; parameter</li> <li>PurgingTrigger</li> </ul>"},{"location":"technology/flink/#eviction","title":"Eviction","text":"<p>Evictor is used to remove elements from a window after the trigger fires and before or after the window function is applied. The logic to remove is app specific.</p> <p>The predefined evictors: CountEvictor, DeltaEvictor and TimeEvictor.</p>"},{"location":"technology/flink/#watermark","title":"Watermark","text":"<p>Watermark is the mechanism to keep how the event time has progressed: with windowing operator, event time stamp is used, but windows are defined on elapse time, for example, 10 minutes, so watermark helps to track te point of time where no more delayed events will arrive.  The Flink API expects a WatermarkStrategy that contains both a TimestampAssigner and WatermarkGenerator. A TimestampAssigner is a simple function that extracts a field from an event. A number of common strategies are available out of the box as static methods on WatermarkStrategy, so reference to the documentation and examples.</p> <p>Watermark is crucial for out of order events, and when dealing with multi sources. Kafka topic partitions can be a challenge without watermark. With IoT device and network latency, it is possible to get an event with an earlier timestamp, while the operator has already processed such event timestamp from other sources.</p> <p>It is possible to configure to accept late events, with the <code>allowed lateness</code> time by which element can be late before being dropped. Flink keeps a state of Window until the allowed lateness time expires.</p>"},{"location":"technology/flink/#resources","title":"Resources","text":"<ul> <li>Product documentation. </li> <li>Official training</li> <li>Base docker image is: https://hub.docker.com/_/flink</li> <li>Flink docker setup and the docker-compose files in this repo.</li> <li>FAQ</li> <li>Cloudera flink stateful tutorial: very good example for inventory transaction and queries on item considered as stream</li> <li>Building real-time dashboard applications with Apache Flink, Elasticsearch, and Kibana</li> <li>Udemy Apache Flink a real time hands-on: do not recommend this one !.</li> </ul>"},{"location":"technology/kafka-connect/","title":"Kafka Connect","text":"<p>Kafka connect is an open source component  for easily integrate external systems with Kafka. It works with any Kafka product like IBM Event Streams, Strimzi, AMQ Streams, Confluent.  It uses the concepts of source and sink connectors to ingest or deliver data to / from  Kafka topics.</p> <p></p> <p>The general concepts are detailed in the IBM Event streams product documentation, and Robin Moffatt's video. Here is a quick summary:</p> <ul> <li>Connector represents a logical job to move data from / to kafka  to / from external systems. A lot of existing connectors, Apache Camel Kafka connectors can be reused, or you can implement your own.</li> <li>Workers are JVMs running the connectors. For production deployment workers run in cluster or \"distributed mode\", and leverage the Kafka consumer group management protocol to scale tasks horizontally.</li> <li>Tasks: each worker coordinates a set of tasks to copy data. In distributed mode, task states are saved in Kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline.</li> <li>REST API to configure the connectors and monitors the tasks.</li> </ul> <p>The following figure illustrates a classical 'distributed' deployment of a Kafka Connect cluster.  Workers are the running processes to execute connectors and tasks. Each Connector is responsible for defining and updating a set of Tasks that actually copy the data. Tasks are threads in a JVM.  For fault tolerance and offset management, Kafka Connect uses Kafka topics (suffix name as <code>-offsets, -config, -status</code>) to persist its states. When a connector is first submitted to the cluster, the workers rebalance the full set of connectors  in the cluster with their tasks so that each worker has approximately the same amount of work.\u00a0</p> <p></p> <ul> <li>Connector and tasks are not guaranteed to run on the same instance in the cluster,  especially if you have multiple tasks and multiple instances in your kafka connect cluster.</li> <li>The connector may be configured to add <code>Converters</code> (code used to translate data between Connect and the system sending or receiving data),  and <code>Transforms</code>: simple logic to alter each message produced by or sent to a connector.</li> </ul> <p>Connector keeps state into three topics, which may be created when the connectors start are:</p> <ul> <li>connect-configs: This topic stores the connector and task configurations.</li> <li>connect-offsets: This topic stores offsets for Kafka Connect.</li> <li>connect-status: This topic stores status updates of connectors and tasks.</li> </ul>"},{"location":"technology/kafka-connect/#characteristics","title":"Characteristics","text":"<ul> <li>Copy vast quantity of data from source to kafka: work at the datasource level. So when the source is a database, it uses JDBC API for example.</li> <li>Support streaming and batch.</li> <li>Scale from standalone, mono connector approach to start small, to run in parallel on distributed cluster.</li> <li>Copy data, externalizing transformation in other framework.</li> <li>Kafka Connect defines three models: data model, worker model and connector model. Worker model allows Kafka Connect to scale the application. Kafka Connect cluster can serve multiple applications and so may be organized as a service.</li> </ul>"},{"location":"technology/kafka-connect/#connector-cluster-configuration","title":"Connector cluster configuration","text":"<p>The following configurations are important to review:</p> <ul> <li><code>group.id</code>: one per connect cluster. It is ised by source connectors only.</li> <li><code>heartbeat.interval.ms</code>: The expected time between heartbeats to the group coordinator when using Kafka\u2019s group management facilities.</li> </ul>"},{"location":"technology/kafka-connect/#fault-tolerance","title":"Fault tolerance","text":"<p>When a worker fails: </p> <p></p> <p>Tasks allocated in the failed worker are reallocated to existing workers, and the task's state, read offsets, source record mapping to offset are reloaded from the different topics.</p> <p></p> <p>Both figure above are illustrating a MongoDB sink connector.</p>"},{"location":"technology/kafka-connect/#mq-source-connector","title":"MQ Source connector","text":"<p>The source code is in this repo and uses JMS as protocol to integrate with MQ. When the connector encounters a message that it cannot process, it stops rather than throwing the message away.  The MQ source connector does not currently make much use of message keys. It is possible to use CorrelationID as a key by defining MQ source <code>mq.record.builder.key.header</code> property:</p> <pre><code>    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.kafka.connect.converters.ByteArrayConverter\n    mq.record.builder: com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\n    mq.connection.mode: client\n    mq.message.body.jms: true\n    mq.record.builder.key.header: JMSCorrelationID\n</code></pre> <p>The record builder helps to transform the input message to a kafka record, using or not a schema.</p> <p>Always keep the coherence between body.jms, record builder and data converter. </p> <p>The MQ source task starts a unique JMS Reader that will read n messages from the queue. The `poll() function  returns the list of MQ source records, and will commit to JMS if the number of message read match the batch size or if there is no more records. Once the Kafka Producer gets acknowledge that records are received by Brokers then use callback on the source task to commit MQ transaction for example. </p> <p>Any producer configuration can be modified in the source connector configuration:</p> <pre><code>producer.override.acks: 1\n</code></pre>"},{"location":"technology/kafka-connect/#installation","title":"Installation","text":"<p>The  Kafka connect framework fits well into a kubernetes deployment.  In 2021 we have different options for that deployment: the Strimzi Kafka connect operator,  IBM Event Streams Connector, Red Hat AMQ Streams (2021.Q3) connector or one of the Confluent connector.</p>"},{"location":"technology/kafka-connect/#ibm-event-streams-cloud-pak-for-integration","title":"IBM Event Streams Cloud Pak for Integration","text":"<p>If you are using IBM Event Streams 2021.x on Cloud Pak for Integration, the connectors setup is part of the user admin console toolbox:</p> <p></p> <p>Deploying connectors against an IBM Event Streams cluster, you need to have a Kafka user with Manager role, to be able to create topic, produce and consume messages for all topics.</p> <p>As an extendable framework, Kafka Connect, can have new connector plugins. To deploy new connector, you need to use the kafka docker  image which needs to be updated with the connector jars and redeployed to kubernetes cluster  or to other environment. With IBM Event Streams on Openshift, the toolbox includes a  kafka connect environment packaging, that defines a Dockerfile and configuration files  to build your own image with the connectors jar files you need. The configuration files  defines the properties to connect to Event Streams kafka brokers using API keys and SASL.</p> <p>The following public IBM messaging github account includes  supported, open sourced, connectors (search for <code>connector</code>).</p> <p>Here is the list of supported connectors for IBM Event Streams.</p> <p>Event Stream Kafka connector use custom resource definition defined by Strimzi. So configuration for Strimzi works for Event Streams.</p> <p>Normally you define one Kafka connect cluster, with a custom docker image which has all the necessary jars file for any connector you want to use. Then you configure each connector so they can start processing events or producing events. A Kafka connect cluster is identified with a group.id and then it saves its states in topics. The example below are for the configuration in cluster, also named distributed.</p> <pre><code>  config:\n    group.id: connect-cluster\n    offset.storage.topic: connect-cluster-offsets\n    config.storage.topic: connect-cluster-configs\n    status.storage.topic: connect-cluster-status\n</code></pre> <p>The real-time inventory gitops repository includes a MQ Source connector to push message to Kafka. It uses ArgoCD to maintain states of Kafka Cluster, topics, users, and Kafka connector.</p> <p>Once the connector pods are running we need to start the connector tasks. </p>"},{"location":"technology/kafka-connect/#strimzi","title":"Strimzi","text":"<p>KafkaConnector resources allow you to create and manage connector instances for Kafka Connect in a Kubernetes-native way. To manage connectors, you can use the Kafka Connect REST API, or use KafkaConnector custom resources. In case of GitOps methodology we will define connector cluster and connector instance as yamls. Connector configuration is passed to Kafka Connect as part of an HTTP request and stored within Kafka itself.</p>"},{"location":"technology/kafka-connect/#further-readings","title":"Further Readings","text":"<ul> <li>Apache Kafka connect documentation</li> <li>Confluent Connector Documentation</li> <li>IBM Event Streams Connectors </li> <li>List of supported connectors by Event Streams</li> <li>MongoDB Connector for Apache Kafka</li> </ul>"},{"location":"technology/kafka-consumers/","title":"Kafka Consumers","text":"<p>Info</p> <p>Updated 05/05/2022</p>"},{"location":"technology/kafka-consumers/#understanding-kafka-consumers","title":"Understanding Kafka Consumers","text":"<p>This chapter includes some technology summary and best practices about Kafka consumer. It may be useful for beginner or seasoned developers who  want a refresh after some time far away from Kafka...</p>"},{"location":"technology/kafka-consumers/#consumer-group","title":"Consumer group","text":"<p>Consumers belong to consumer groups. </p> <p></p> <p>You specify the group name as part of the consumer connection parameters using the <code>group.id</code> configuration:</p> <pre><code>  properties.put(ConsumerConfig.GROUP_ID_CONFIG,  groupid);\n</code></pre> <p>Consumer groups are grouping consumers to cooperate to consume messages from one or more topics. Consumers can run in separate hosts and separate processes. </p> <p>The figure below represents 2 consumer apps belonging to the same consumer group. Consumer 1 is getting data from 2 partitions, while consumer 2 is getting from one partition. </p> <p>When a consumer is unique in a group, it will get data from all partitions. There is always at least one consumer per partition.</p> <p>One broker is responsible to be the consumer group coordinator which is responsible for assigning partitions to the consumers in the group. </p> <p>The first consumer to join the group will be the group leader. It will get the list of consumers and it is responsible for assigning a subset  of partitions to each consumer.</p> <p>Membership in a consumer group is maintained dynamically. Consumers send hearbeats to the group coordinator broker (see configuration like  heartbeat.interval.ms) and <code>session.timeout.ms</code>.  Partition assignement is done by different strategies from range, round robin, sticky and cooperative sticky (See partition assignement strategy). </p> <p>When a consumer fails, the partitions assigned to it will be reassigned to an other consumer in the same group. When a new consumer joins  the group, partitions will be moved from existing consumers to the new one. Group rebalancing is also used when new partitions are added to one  of the subscribed topics. The group will automatically detect the new partitions through periodic metadata refreshes and assign them to members  of the group. During a rebalance, depending of the strategy, consumers may not consume messages (Need Kafka 2.4+ to get cooperative balancing feature). </p> <p>Kafka automatically detects failed consumers so that it can reassign partitions to working consumers. </p> <p>The consumer can take time to process records, so to avoid the consumer group controler removing consumer taking too long, it is possible to set the max.poll.interval.ms consumer property. If <code>poll()</code> is not called before expiration of this timeout, then the consumer is considered failed and the group will rebalance in order to reassign the partitions to another member.  The second mechanism is the heartbeat consumers send to the group cordinator to show they are alive. The session.timeout.ms specifies the max value to consider before removing a non responding consumer. </p> <p>Implementing a Topic consumer is using the kafka KafkaConsumer class  which the API documentation is a must read. It is interesting to note that:</p> <ul> <li>To support the same semantic of a queue processing like other integration messaging systems, you need to have all the consumers assigned to a single consumer group,  so that each record delivery would be balanced over the group like with a queue.</li> <li>To support pub/sub like other messaging systems, each consumer would have its own consumer group, and subscribes to all the records published  to the topic.</li> <li>With <code>client.rack</code> setting a consumer can consume from a local replica, which will have better latency when using a stretched cluster or multiple availability zones.</li> </ul> <p>For a single thread consumer, the implementation code follow the following pattern:</p> <ul> <li>prepare the consumer properties</li> <li>create an instance of KafkaConsumer to subscribe to at least one topic</li> <li>loop on polling events: the consumer ensures its liveness with the broker via the poll API. It will get <code>n records</code> per poll.</li> <li>process the ConsumerRecords and commit the offset by code or by using the autocommit attribute of the consumer</li> </ul> <p>As long as the consumer continues to call poll(), it will stay in the group and continues to receive messages from the partitions it was assigned to.  When the consumer does not send heartbeats for a duration of <code>session.timeout.ms</code>, then it is considered unresponsive and its partitions will be reassigned.</p> <p>Examples of Java consumers can be found in the order management microservice project under the order-command-ms folder.</p> <p>We are proposing a deep dive study on this manual offset commit in this consumer code. Example of Javascript implementation is in this repository/folder</p> <p>But the complexity comes from the offset management and multithreading needs. So the following important considerations need to be addressed while implementing a consumer.</p>"},{"location":"technology/kafka-consumers/#assess-number-of-consumers-needed","title":"Assess number of consumers needed","text":"<p>The KafkaConsumer is not thread safe so it is recommended to run it in a unique thread. If really, needed you can implement a multi-threads solution,  but as each thread will open a TCP connection to the Kafka brokers, be sure to close the connection to avoid memory leak.  The alternate is to start n processes (JVM process) with a mono thread.</p> <p>If you need multiple consumers running in parallel to scale horizontally, you have to define multiple partitions while configuring the topic and use  fine-grained control over offset persistence.  The consumer-per-partition pattern maximizes throughput. When consumers run in parallel and you use multiple threads per consumer you need to be sure  the total number of threads across all instances do not exceed the total number of partitions in the topic.</p> <p>Also, a consumer can subscribe to multiple topics. The brokers are doing rebalancing of the assignment of topic-partition to a consumer  that belong to a group. </p>"},{"location":"technology/kafka-consumers/#offset-management","title":"Offset management","text":"<p>Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to  commit the offsets they have received, to present a recovery point in case of failure. To commit offset (via API or automatically) the consumer  sends a message to kafka broker to the special topic named <code>__consumer_offsets</code> to keep the committed offset for each partition. (When there is a committed offset, the auto.offset.reset property is not used)</p> <p>Consumers do a read commit for the last processed record: </p> <p></p> <p>When a consumer starts, it receives a partition to consume, and it starts at its group's committed offset or the latest or earliest offset  as specified in the auto.offset.reset property.</p> <p>If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. </p> <p></p> <p>This means that the message will be processed again by the next consumer, in that group, to be assigned the partition.</p> <p>In the case where consumers are set to auto commit, it means the offset if committed at the <code>poll()</code> function and if the service  crashed while processing of this record as: </p> <p></p> <p>then the record (partition 0 - offset 4) will never be processed. But it is not lost.</p> <p>As shown in the figure below, in case of consumer failure, it is possible to get duplicates. When the last message processed by the consumer,  before crashing, is younger than the last committed offset, the consumer will get this record again. This case may happen when using a time based commit strategy.</p> <p> Source: Kafka definitive guide book from Todd Palino, Gwen Shapira</p> <p>In the opposite, if the last committed offset is after the last processed messages and there were multiple messages returned in the poll,  then those messages may be lost (in term of consumer processing not lost in kafka). This will happen with autocommit set up at the time  of the read operation, and the last offset of the poll is the committed offset. See the enable.auto.commit property.</p> <p></p> <p>Limiting to poll one message at the time, will help to avoid this problem, but will impact throughput.</p> <p>It is possible to commit by calling API so developer can control when to commit the read. For manual commit, we can use one of the two approaches:</p> <ul> <li>offsets\u2014synchronous commit: send the offset number for the records read using <code>consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1)))</code> method </li> <li>asynchronous</li> </ul> <p>As soon as you are coding manual commit, it is strongly recommended to implement the ConsumerRebalanceListener interface to be able to do state  modifications when the topic is rebalanced.</p> <p>Assess if it is acceptable to loose messages from topic.  If so, when a consumer restarts, it will start consuming the topic from the latest  committed offset within the partition allocated to itself.</p> <p>As storing a message to an external system and storing the offsets are two separate operations, and in case of failure between them,  it is possible to have stale offsets, which will introduce duplicate messages when consumers restart to process from last known committed offset.  In this case, consumer's idempotence is needed to support updating the same row in the table, or use the event timestamp as update timestamp in  the database record or use other clever solution.</p> <p>As presented in the producer coding practice, using transaction to support \"exactly-once\", also means the consumers should read committed data only.  This can be achieved by setting the <code>isolation.level=read_committed</code> in the consumer's configuration. The last offset will be the first message  in the partition belonging to an open not yet committed transaction. This offset is known as the 'Last Stable Offset'(LSO).</p>"},{"location":"technology/kafka-consumers/#producer-transaction","title":"Producer transaction","text":"<p>When consuming from a Kafka topic and producing to another topic, like in Kafka Streams programming approach, we can use the producer's transaction  feature to send the committed offset message and the new records in the second topic in the same transaction.  This can be seen as a  <code>consume-transform-produce</code> loop pattern so that every input event is processed exactly once. </p> <p>An example of such pattern in done in the order management microservice - command part.</p>"},{"location":"technology/kafka-consumers/#consumer-lag","title":"Consumer lag","text":"<p>The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset.</p> <p>If the lag starts to grow, it means the consumer is not able to keep up with the producer's pace.</p> <p>The risk, is that slow consumer may fall behind, and when partition management may remove old log segments, leading the consumer to jump forward to continnue on the next log segment. Consumer may have lost messages.</p> <p></p> <p>You can use the kafka-consumer-groups tool to see and manage the consumer lag.</p> <p>You can use the kafka-consumer-groups tool to see the consumer lag, or use the Event Streams User Interface:</p> <p></p> <p>The group can be extended to see how each consumer, within the group, performs on a multi partitions topic:</p> <p></p>"},{"location":"technology/kafka-consumers/#reset-a-group","title":"Reset a group","text":"<p>Sometime it is needed to reprocess the messages. The easiest way is to change the groupid of the consumers to get an implicit offsets reset, but it is also possible to reset for some topic to the earliest offset:</p> <pre><code>kafka-consumer-groups \\\n                    --bootstrap-server kafkahost:9092 \\\n                    --group ordercmd-command-consumer-grp \\\n                    --reset-offsets \\\n                    --all-topics \\\n                    --to-earliest \\\n                    --execute\n</code></pre>"},{"location":"technology/kafka-consumers/#kafka-useful-consumer-apis","title":"Kafka useful Consumer APIs","text":"<ul> <li>KafkaConsumer a topic consumer which support:</li> <li>transparently handles brokers failure</li> <li>transparently adapt to partition migration within the cluster</li> <li>support grouping for load balancing among consumers</li> <li>maintains TCP connections to the necessary brokers to fetch data</li> <li>subscribe to multiple topics and being part of consumer groups</li> <li>each partition is assigned to exactly one consumer in the group</li> <li>if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group</li> <li>ConsumerRecords holds the list ConsumerRecord per partition for a particular topic.</li> <li>ConsumerRecord A key/value pair to be received from Kafka. This also consists of a topic name and a partition number from which the record is being received, an offset that points to the record in a Kafka partition, and a timestamp</li> </ul>"},{"location":"technology/kafka-consumers/#repositories-with-consumer-code","title":"Repositories with consumer code","text":"<ul> <li>Within the Reefer ontainer shipment solution we have a order events consumer: order event agent</li> <li>Quarkus app with Kafka streams</li> <li>Nodejs kafka consumers and producers</li> <li>A lot of python consumer codes in the integration tests, with or without Avro schema</li> </ul>"},{"location":"technology/kafka-consumers/#references","title":"References","text":"<ul> <li>IBM Event Streams - Consuming messages</li> <li>KafkaConsumer class</li> </ul>"},{"location":"technology/kafka-mirrormaker/","title":"Kafka Mirror Maker 2","text":"<p>This section introduces Mirror Maker 2.0, the new replication feature of Kafka 2.4, and how it can be used, along with best practices, for data replication between two Kafka clusters.  Mirror Maker 2.0 was defined as part of the Kafka Improvement Process - KIP 382 and can be used for disaster recovery (active / passive) or for more complex topology with 3 data centers to support always on.</p>"},{"location":"technology/kafka-mirrormaker/#overview","title":"Overview","text":"<p>We recommend to start by reading the IBM Event Streams product documentation on geo-replication to understand the main concepts. Some core principles we use in this article:</p> <ul> <li>We have two data centers in different region and each region has multiple availability zones</li> <li>OpenShift clusters are defined in both region and spread among the three data center. For a better view of the golden topology for OpenShift see this diagram  with master, worker nodes deployment.</li> <li>At least two Event Streams / Kafka clusters are defined, one as <code>source</code> in the active region and one <code>target</code> for disaster recovery or passive region.</li> <li>Source cluster has producer and consumer applications deployed in the same OpenShift Cluster or deployed on VMs and accessing the Kafka brokers via network load balancer.</li> <li>Producer, consumer or streaming applications deployed within OpenShift use the <code>bootstrap URL</code> to kafka broker via internal service definition. Something like <code>es-prod-kafka-bootstrap.ibm-eventstreams.svc</code>. With such configuration their setting will be the same on the target cluster.</li> <li>The target cluster has the mirror maker cluster which is based on the Kafka connect framework.</li> </ul> <p>The following diagram illustrates those principles:</p> <p></p> <p>When zooming into what need to be replicated, we can see source topics from the blue cluster to target topics on the green cluster.  This configuration is for disaster recovery, with a active - passive model, where only the left side has active applications producing and consuming records from Kafka Topics.</p> <p></p> <p>As the mirroring is over longer internet distance, then expect some latency in the data mirroring.</p> <p>We can extend this deployment by using Mirror Maker 2 to replicate data over multiple clusters with a more active - active deployment which the following diagram illustrates the concepts for an \"always-on\" deployment:</p> <p></p> <p>This model can also being used between cloud providers.</p> <p>In active - active mode the clusters get data injected in local cluster and replicated data injected from remote cluster.  The topic names are prefixed with the original cluster name. In the figure below, the cluster on the right has green local producers and consumers,  topics are replicated to the left, the blue cluster. Same for blue topic from the left to the right.</p> <p></p> <p>Consumers on both sides are getting data from the 'order' topics (local and replicated) to get a complete view of all the orders created on both sides. </p> <p>The following diagram zooms into a classical Web based solution design where mobile or web apps are going to a web tier to serve single page application, static content, and APIs.</p> <p></p> <p>Then a set of microservices implement the business logic, some of those services are event-driven, so they produce and consumer events from topics. When active-active replication is in place it, means the same topology is deployed in another data center and data from the same topic (business entity) arrive to the replicated topic. The service can save the record in its own database and cache. (The service Tier is not detailed with the expected replicas, neither the application load balancer displays routes to other data center)</p> <p>If there is a failure on one of the side of the data replication, the data are transparently available. A read model query will return the good result on both side.</p> <p>In replication, data in topic, topic states and metadata are replicated.</p> <p>IBM Event Streams release 10.0 is supporting Mirror Maker 2 as part of the geo-replication feature.</p>"},{"location":"technology/kafka-mirrormaker/#mirror-maker-2-components","title":"Mirror Maker 2 components","text":"<p>Mirror maker 2.0 is the solution to replicate data in topics from one Kafka cluster to another. It uses the Kafka Connect framework to simplify configuration, parallel execution and horizontal scaling.</p> <p>The figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect.</p> <p></p> <p>MirrorMaker 2 uses the cluster name or identifier as prefix for topic, and uses the concept of source topic and target topic. It runs in standalone mode, which can be used for development and test purpose, or in distributed mode (cluster) for production deployment. With distribution mode, MirrorMaker 2.0 creates the following topics on the cluster it is connected to (See later the property <code>connectCluster</code>):</p> <ul> <li>...-configs.source.internal: This topic is used to store the connector and task configuration.</li> <li>...-offsets.source.internal: This topic is used to store offsets for Kafka Connect.</li> <li>...-status.source.internal: This topic is used to store status updates of connectors and tasks.</li> <li>source.heartbeats: to check that the remote cluster is available and the clusters are connected</li> <li>**source.checkpoints.internal*: MirrorCheckpointConnector tracks and maps offsets for specified consumer groups using an offset sync topic and checkpoint topic.</li> </ul> <p>A typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition.  Here is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and  SASL authentication protocol.  The IBM Event Streams instance runs on the Cloud.</p> <pre><code>clusters=source, target\nsource.bootstrap.servers=${KAFKA_SOURCE_BROKERS}\ntarget.bootstrap.servers=${KAFKA_TARGET_BROKERS}\ntarget.security.protocol=SASL_SSL\ntarget.ssl.protocol=TLSv1.2\ntarget.ssl.endpoint.identification.algorithm=https\ntarget.sasl.mechanism=PLAIN\ntarget.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY};\n# enable and configure individual replication flows\nsource-&gt;target.enabled=true\nsource-&gt;target.topics=products\ntasks.max=10\n</code></pre> <ul> <li>Topics to be replicated are configured via a whitelist that may include regular expression for pattern matching on the topic name. So if you use naming convention for your topic, you could do fine grained selection of the replicated topic.  It is possible to specify topics you do not want to replicate via the blacklist property.</li> <li>White listed topics are set with the <code>source-&gt;target.topics</code> attribute of the replication flow and uses Java regular expression syntax.</li> <li>The default blacklisted topics are Kafka internal topic:</li> </ul> <pre><code>blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas]\n</code></pre> <p>We can also define the blacklist with the properties: <code>topics.blacklist</code>. Comma-separated lists and Java Regular Expressions are supported.</p> <p>Internally, <code>MirrorSourceConnector</code> and <code>MirrorCheckpointConnector</code> will create multiple Kafka tasks (up to the value of <code>tasks.max</code> property), and <code>MirrorHeartbeatConnector</code> creates an additional task. <code>MirrorSourceConnector</code> will have one task per topic-partition combination to replicate, while <code>MirrorCheckpointConnector</code> will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the <code>assign()</code> API, so there is no consumer group used while fetching data from source topics. There is no call to <code>commit()</code> either; rebalancing occurs only when there is a new topic created that matches the whitelist pattern.</p> <p>Mirror Maker 2 can run on VM, bare metal or within containers deployed on kubernetes cluster.</p>"},{"location":"technology/kafka-mirrormaker/#why-replicating","title":"Why replicating?","text":"<p>The classical needs for replication between clusters can be listed as:</p> <ul> <li>Disaster recovery when one secondary cluster is passive while the producer and consumers are on the active cluster in the primary data center: The following article goes over those principals.</li> <li>Active-active cluster mirroring for inter services communication: consumers and producers are on both sides and consume or produce to their local cluster.</li> <li>Moving data to a read only cluster as a front door to data lake, or to do cross data centers aggregation on the different event streams: Fan-in to get holistic data view.</li> <li>GDPR compliance to isolate data in country and geography</li> <li>Hybrid cloud operations to share data between on-premise cluster and managed service clusters.</li> </ul>"},{"location":"technology/kafka-mirrormaker/#deployment-examples","title":"Deployment examples","text":"<p>We encourage you to go over our Mirror maker 2 labs which addresses different replication scenarios. The <code>Connect</code> column defines where the Mirror Maker 2 runs.</p> Scenario Source Target Connect Lab Getting Started Lab 1 Event Streams on Cloud Local Kafka Local on localhost Kafka Mirror Maker 2 - Lab 1 Lab 2 Using Mirror Maker 2 from Event Streams on premise to Event stream on cloud On OCP OCP Kafka Mirror Maker 2 - Lab 2"},{"location":"technology/kafka-mirrormaker/#replication-considerations","title":"Replication considerations","text":""},{"location":"technology/kafka-mirrormaker/#topic-metadata-replication","title":"Topic metadata replication","text":"<p>It is possible to disable the topic metadata replication. We do not encourage to do so. Per design topic can be added dynamically, specially when developing with Kafka Streams where intermediate topics are created by the stream topology semantic, and topic configuration can be altered to increase the number of partitions. Changes to the source topic are dynamically propagated to the target avoiding maintenance nightmare. By synchronizing configuration properties, the need for rebalancing is reduced.</p> <p>When doing manual configuration, even if the initial topic configuration is duplicated, any dynamic changes to the topic properties are not going  to be automatically propagated and the administrator needs to change the target topic. If the throughput on  the source topic has increased and the number of partition was increased to support the load, then the target cluster will not have the same  downstream capability which may lead to overloading (disk space or memory capacity). With a GitOps approach, this risk is mitigated as topic definition in the GitOps repository could be propagated to the target and source cluster mostly at the same time. </p> <p>Also if the consumer of a partition is expecting to process the events in order within the partition, then changing the number of partitions  between source and target will make the ordering not valid any more.</p> <p>If the replication factor are set differently between the two clusters then the availability guaranty of the replicated data may be impacted  and bad settings with broker failure will lead to data lost.</p> <p>Finally, it is important to consider that changes to topic configuration triggers a consumer rebalance which stalls the mirroring process and creates  a backlog in the pipeline and increases the end to end latency observed by the downstream application.</p>"},{"location":"technology/kafka-mirrormaker/#naming-convention","title":"Naming convention","text":"<p>Mirror maker 2 sets the prefix for the name of the replicated topic with the name of the source cluster. This is an important and simple solution to avoid  infinite loop when doing bi-directional mirroring. At the consumer side the <code>subscribe()</code> function supports regular expression for topic name. So a code like:</p> <pre><code>kafkaConsumer.subscribe(\"^.*accounts\")\n</code></pre> <p>will listen to all the topics in the cluster having cluster name prefixed topics and the local <code>accounts</code> topic.  This could be useful when we want to aggregate data from different data centers / clusters.</p>"},{"location":"technology/kafka-mirrormaker/#offset-management","title":"Offset management","text":"<p>Mirror maker 2 tracks offset per consumer group. There are two topics created on the target cluster to manage the offset mapping between the source and target clusters  and the checkpoints of the last committed offset in the source topic/partitions/consumer group. When a producer sends its records, it gets the offsets  in the partition the records were created.</p> <p>In the diagram below we have a source topic A/partition 1 with the last write offset done by a producer to be  5, and the last read committed offset by the consumer assigned to partition 1 being 3.  The last replicated offset 3 is mapped as downstream offset 12 in the target partition. Offset numbers do not match between replicated partitions. So if the blue consumer needs to reconnect to the green target cluster it will read from the last committed offset which is 12 in this environment.  This information is saved in the <code>checkpoint</code> topic.</p> <p></p> <p>Offset synch are emitted at the beginning of the replication and when there is a situation which leads that the numbering sequencing diverges.  For example, the normal behavior is to increase the offset by one 2,3,4,5,6,7, which is mapped to 12,13,14,15,16,... on target cluster.  If the write operation for offset 20 at the source is a 17 on the target then MM 2 emits a new offset synch records to the <code>offset-synch</code> topic.</p> <p>The <code>checkpoint</code> and <code>offset_synch</code> topics enable replication to be fully restored from the correct offset position on failover.  On the following diagram, once the cluster source is down, the consumers on the target cluster are restarted, and they will start from the last committed  offset of the source, which was offset 3 that is in fact offset 12 on target replicated topic. No record skipped.</p> <p></p>"},{"location":"technology/kafka-mirrormaker/#record-duplication","title":"Record duplication","text":"<p>Exactly-once delivery is difficult to achieve in distributed system. In the case of Kafka, producer, brokers, and consumers are working together to ensure  only one message is processed end to end. With coding practice and configuration settings, within a unique cluster, Kafka can guarantee exactly once processing.  No duplicated records between producer and broker, and committed reads, on consumer side, are not reprocessed in case of consumer restarts.</p> <p>But for cross cluster replications, the semantic is based on at least once approach. Duplicates can happen when the mirror maker source task stops  before committing its offset to the source topic. A restart will load records from the last committed offset which can generate duplicates.  The following diagram illustrate this case, record offset 26 on target topic is a duplicate of record 25.</p> <p></p> <p>Also Mirror Maker 2 is a generic topic consumer, it will not participate to the \"read-committed\" process, if the topic includes duplicate messages  it will propagate them to the target.</p> <p>In the future MM2 will be able to support exactly once by using the <code>checkpoint</code> topic on the target cluster to keep the state of the committed offset  from the consumer side, and write with an atomic transaction between the target topic and the checkpoint topic, and commit the source read offset as part  of the same transaction.</p>"},{"location":"technology/kafka-mirrormaker/#mm2-topology","title":"MM2 topology","text":"<p>In this section we want to address horizontal scalability and how to organize the MirrorMaker 2 topology for multi-tenancy. The simplest approach is  to use one Mirror Maker instance per family of topics: the classification of family of topic can be anything, from line of business, to team, to application.  Suppose an application is using 1000 topic - partitions, for data replication it may make sense to have one MM2 instance for this application.  The configuration will define the groupId to match the application name for example.</p> <p>The following diagram illustrates this kind of topology by using regular expression on the topic white list selection, there are three Mirror Maker 2  instances mirroring the different topics with name starting with <code>topic-name-A*, topic-name-B*, topic-name-C*,</code> respectively.</p> <p></p> <p>Each connect instance is a JVM workers that replicate the topic/partitions and has different group.id.</p> <p>For Bi-directional replication for the same topic name, Mirror Maker 2 will use the cluster name as prefix. With MM2, we do not need to have 2 MM2 clusters  but only one and bidirectional definitions. The following example is showing the configuration for a MM2 bidirectional settings, with <code>accounts</code> topic to be  replicated on both cluster:</p> <pre><code>apiVersion: kafka.strimzi.io/v1alpha1\nkind: KafkaMirrorMaker2\n...\n mirrors:\n  - sourceCluster: \"event-streams-wdc\"\n    targetCluster: \"kafka-on-premise\"\n    ...\n    topicsPattern: \"accounts,orders\"\n  - sourceCluster: \"kafka-on-premise\"\n    targetCluster: \"event-streams-wdc\"\n    ...\n    topicsPattern: \"accounts,customers\"\n</code></pre>"},{"location":"technology/kafka-mirrormaker/#consumer-coding","title":"Consumer coding","text":"<p>We recommend to review the producer implementation best practices and the consumer considerations.</p>"},{"location":"technology/kafka-mirrormaker/#capacity-planning","title":"Capacity planning","text":"<p>For platform sizing, the main metric to assess, is the number of partitions to replicate. The number of partitions and number of brokers are somehow connected as  getting a high number of partitions involves increasing the number of brokers. For Mirror Maker 2, as it is based on Kafka connect, there is a unique cluster  and each partition mirroring is supported by a task within the JVM so the first constraint is the memory allocated to the container and the heap size.</p> <p>To address capacity planning, we need to review some characteristic of the Kafka Connect framework: For each topic/partition there will be a task running.  We can see in the trace that tasks are mapped to threads inside the JVM. So the parallelism will be bound by the number of CPUs the JVM runs on.  The parameters <code>max.tasks</code> specifies the max parallel processing we can have per JVM. So for each Topic we need to assess the number of partitions to be replicated.  Each task is using the consumer API and is part of the same consumer group, the partition within a group are balanced by an internal controller.  With Kafka connect any changes to the topic topology triggers a partition rebalancing. In MM2 each consumer / task is assigned a partition by the controller.  So the rebalancing is done internally. Still adding a broker node into the cluster will generate rebalancing.</p> <p>The task processing is stateless, consume - produce wait for acknowledge,  commit offset. In this case, the CPU and network performance are key.  For platform tuning activity, we need to monitor operating system performance metrics. If the CPU becomes the bottleneck, we can allocate more CPU or start  to scale horizontally by adding more Mirror Maker 2 instances. If the network at the server level is the bottleneck, then adding more servers will help.  Kafka will automatically balance the load among all the tasks running on all the machines. The size of the message impacts also the throughput as with small  message the throughput is CPU bounded. With 100 bytes messages or more we can observe network saturation.</p> <p>The parameters to consider for sizing are the following:</p> Parameter Description Impact Number of topic/ partition Each task processes one partition For pure parallel processing max.tasks is set around the number of CPU Record size Size of the message in each partition in average Memory usage and Throughput: the # of records/s decrease when size increase, while MB/s throughput increases in logarithmic Expected input throughput The producer writing to the source topic throughput Be sure the consumers inside MM2 absorb the demand Network latency This is where positioning MM2 close to the target cluster may help improve latency"},{"location":"technology/kafka-mirrormaker/#version-migration","title":"Version migration","text":"<p>Once the Mirror Maker cluster is up and running, it may be needed to update the underlying code when a new product version is released. Based on Kafka Connect distributed mode multiple workers JVM coordinate the topic / partition repartition among themselves.  If a worker process dies, the cluster is rebalanced to distribute the work fairly over the remaining workers. If a new worker starts work, a rebalance ensures it takes over some work from the existing workers.</p> <p>Using the REST API it is possible to stop and restart a connector. As of now the recommendation is to start a new MirrorMaker instance with the new version and the same groupId as the existing workers you want to migrate. Then stop the existing version. As each MirrorMaker workers are part of the same group, the internal worker controller will coordinate with the other workers the  'consumer' task to partition assignment.</p> <p>We have presented a similar approach in this section, where we tested that each instance of MirrorMaker 2 could assume the replication.  First we will stop the Node 1 instance, upgrade it to the latest version, then start it again.  Then we\u2019ll repeat the same procedure on Node 2.  We\u2019ll continue to watch the Consumer VM window to note that replication should not stop at any point.</p> <p></p> <p>We\u2019ve now upgraded to Kafka 2.5 including the latest MirrorMaker 2.  Meanwhile, replication was uninterrupted due to the second instance of MirrorMaker 2:</p> <p></p> <p>Now we\u2019ll restart the Node 1 instance of MirrorMaker 2, stop the Node 2 instance, we can still see replication occurring on the upgraded Node 1 instance of MirrorMaker 2.</p> <p></p> <p>We upgrade Node 2\u2019s instance of MirrorMaker 2 exactly as on Node 1, and start it again, and once again, replication is still going.</p> <p></p> <p>When using Strimzi, if the update applies to the MM2 Custom Resource Definition, just reapplying the CRD should be enough.</p> <p>Be sure to verify the product documentation as new version may enforce to have new topics. It was the case when Kafka connect added the config topic in a recent version.</p>"},{"location":"technology/kafka-mirrormaker/#resources","title":"Resources","text":"<ul> <li>IBM Event Streams product documentation</li> <li>IBM Event Streams managed service - Disaster recovery example scenario</li> <li>Strimzi configuration for Mirror Maker 2</li> <li>Getting started with Mirror Maker 2 - Tech Academy</li> <li>Using MirrorMaker2 from Dale Lane</li> </ul>"},{"location":"technology/kafka-monitoring/","title":"Monitoring Kafka with Prometheus and Grafana","text":"<p>Info</p> <p>Updated 09/08/2022</p>"},{"location":"technology/kafka-monitoring/#overview","title":"Overview","text":"<p>A comprehensive Kafka monitoring plan should collect metrics from the following components:</p> <ul> <li>Kafka Broker(s)</li> <li>ZooKeeper metrics as Kafka relies on it to maintain its state</li> <li>Producer(s) / Consumer(s), in general sense, which includes Kafka Connector cluster</li> </ul> <p>Kafka Broker, Zookeeper and Java clients (producer/consumer) expose metrics via JMX (Java Management Extensions)  and can be configured to report stats back to Prometheus using the JMX exporter  maintained by Prometheus.  There is also a number of exporters maintained by the community to explore.  Some of them can be used in addition to the JMX export. To monitor Kafka, for example, the JMX exporter  is often used to provide broker level metrics, while community exporters claim to provide more accurate cluster level  metrics (e.g. Kafka exporter, Kafka Zookeeper Exporter by CloudFlare, and others).  Alternatively, you can consider writing your own custom exporter.</p>"},{"location":"technology/kafka-monitoring/#what-to-monitor","title":"What to monitor","text":"<p>A long list of metrics is made available by Kafka (here) and Zookeeper (here).  The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or Kafka/Prometheus server;  this will allow browsing all metrics with JMX. But you are still left to figure out which ones you want to actively monitor and the ones  that you want to be actively alerted.</p> <p>An simple way to get started would be to start with the Grafana\u2019s sample dashboards for the Prometheus exporters you chose to use and then modify  them as you learn more about the available metrics and/or your environment. The Monitoring Kafka metrics article by DataDog  and How to monitor Kafka by Server Density provides  guidance on key Kafka and Prometheus metrics,  reasoning to why you should care about them and suggestions on thresholds to trigger alerts.  In the next section, we will demonstrate exactly that; we will start with sample dashboards and make few modifications to exemplify  how to configure key Kafka metrics to display in the dashboard.</p> <p>Here are a set of helpful links for Event Streams monitoring:</p> <ul> <li>Monitoring deployment health using User Interface, or OpenShift CLI</li> <li>Cluster health: Event Streams has a preconfigured monitoring dashboard, but other tools could be used as it exports a set of metrics via standard like JMX.</li> </ul> <p></p>"},{"location":"technology/kafka-monitoring/#prometheus-server-and-scrape-jobs","title":"Prometheus Server and scrape jobs","text":"<p>Prometheus uses a configuration file in YAML format to define the scraping jobs and their instances. You can also use the configuration file to define recording rules and alerting rules:</p> <ul> <li> <p>Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. This is especially useful for dashboards, which need to query the same expression repeatedly every time they refresh.</p> </li> <li> <p>Alerting rules allow you to define alert conditions based on Prometheus expression language expressions and to send notifications about firing alerts to an external service. Alerting rules in Prometheus servers send alerts to an Alertmanager. The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, PagerDuty and others.</p> </li> </ul> <p>Below, we will go through the steps to stand-up a local Prometheus server as a Docker container and to modify the configuration file to scrape Kafka metrics:</p> <ul> <li>Create/run a docker container using Prometheus official image from DockerHub</li> </ul> <pre><code>docker run -d -p 9090:9090 prom/prometheus\n</code></pre> <ul> <li>Obtain the IP address of the Kafka container</li> </ul> <pre><code>docker inspect kafka_c | grep IPAddress\n</code></pre> <ul> <li>Edit the prometheus.yml to add Kafka as a target</li> </ul> <pre><code>docker exec -it prometheus_c \\sh\nvi /etc/prometheus/prometheus.yml\n</code></pre> <ul> <li>Locate the scrape_configs section in the properties file and add the lines below to define the Kafka job, where the IP should be the IP of the kafka container</li> </ul> <pre><code>- job_name: 'kafka'\n  static_configs:\n  - targets: ['172.17.0.4:7071']\n</code></pre> <ul> <li>Reload the configuration file</li> </ul> <pre><code>ps -ef\nkill -HUP &lt;prometheus PID&gt;\n</code></pre> <ul> <li>You can now verify that Kafka is listed as a target job in Prometheus. On a Browser, open the http://localhost:9090/targets URL.</li> </ul> <p></p>"},{"location":"technology/kafka-monitoring/#grafana-server-and-dashboards","title":"Grafana Server and dashboards","text":"<p>We will use Grafana for visualization of the metrics scraped by Prometheus for that, we will need to:</p> <ul> <li>Stand-up a local Grafana server as a Docker container</li> <li>Configure Prometheus as a data source in Grafana</li> <li>Import sample dashboards provided by Grafana and/or community</li> <li>Modify the sample dashboards as we see fit</li> </ul> <p>Let\u2019s get started:</p> <ul> <li>Create a docker container using Prometheus official image from DockerHub</li> </ul> <pre><code>docker run -d --name=grafana_c -p 3000:3000 grafana/grafana\n</code></pre> <ul> <li> <p>On a Browser, open the http://localhost:3000 URL.</p> </li> <li> <p>Login as admin/admin. You will be prompted to change the password.</p> </li> <li> <p>Once logged in, Grafana provides visual guidance on what the next steps are: a) Add data sources b) Create first dashboard and others</p> </li> </ul> <p></p> <ul> <li> <p>Configure Prometheus as a data source:</p> </li> <li> <p>Enter a Name for the data source (e.g. Prometheus)</p> </li> <li>Select Prometheus as Type</li> <li>Enter http://localhost:9090 for HTTP URL</li> <li>In our simple server configuration, select Browser for HTTP Access </li> <li> <p>Click Save and Test to validate configuration</p> <p></p> </li> <li> <p>Back to Home, click Dashboards -&gt; Manage to import sample dashboards</p> </li> <li> <p>Click the +Import button and paste this URL https://grafana.com/dashboards/721</p> </li> <li>Make sure to select Prometheus as the data source.</li> </ul> <p>NOTE: You can also explore other sample dashboard options at https://grafana.com/dashboards. For instance, there is a Kubernetes Kafka resource metrics sample dashboard that you could use instead as the starting point when configuring Kafka monitoring on ICP.</p> <p></p> <p></p> <p>The six graphs displayed in the dashboard are configured as follows:</p> <p>NOTE: You might want to go back to your Kafka Docker container and push messages into the topics you have created above to see changes to the graph. Or, if you have already pushed messages, you can change the Quick Range from last 5 minutes to something else (e.g. last 6 hours) on the top right hand corner of the dashboard.</p> Graph Formula Format As CPU Usage rate(process_cpu_seconds_total{job=\"kafka\"}[1m]) Time Series JVM Memory Used sum without(area)(jvm_memory_bytes_used{job=\"kafka\"}) Time Series Time spent in GC sum without(gc)(rate(jvm_gc_collection_seconds_sum{job=\"kafka\"}[5m])) Time Series Messages In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_messagesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes Out per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesout_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series <p>Prometheus provides a functional expression language that lets the user select and aggregate time series data in real time. Before proceeding review the information on these pages to gain basic understanding of:</p> <ul> <li>Prometheus Expression language - http://docs.grafana.org/features/datasources/prometheus/</li> <li>Grafana Query Editor - http://docs.grafana.org/features/datasources/prometheus/</li> </ul> <p>As you make modifications to the dashboard it is also important to understand the data returned by the scrape jobs in the first place. For two of the metrics above, this is what the Kafka JMX exporter returns. You can go to https://localhost:7071/metrics to inspect others returned in /metrics endpoint response:</p> <ul> <li>Messages in Per Topic</li> </ul> <p></p> <ul> <li>Time spent in GC</li> </ul> <p></p>"},{"location":"technology/kafka-overview/","title":"Kafka Overview","text":"<p>In this article we are summarizing what Apache Kafka is and grouping some references, notes and tips we gathered working with Kafka while producing the different assets for this Event Driven Architecture references. This content does not replace the excellent introduction every developer using Kafka should read.</p>"},{"location":"technology/kafka-overview/#introduction","title":"Introduction","text":"<p>Kafka is a distributed real time event streaming platform with the following key capabilities:</p> <ul> <li>Publish and subscribe streams of records. Data are stored so consuming applications can pull the information they need, and keep track of what they have seen so far.</li> <li>It can handle hundreds of read and write operations per second from many producers and consumers.</li> <li>Atomic broadcast, send a record once, every subscriber gets it once.</li> <li>Store streams of data records on disk and replicate them within the distributed cluster for fault-tolerance. Persist data for a given time period before delete.</li> <li>Can grow elastically and transparently with no downtime.</li> <li>Built on top of the ZooKeeper synchronization service to keep topic, partitions and metadata highly available.</li> </ul>"},{"location":"technology/kafka-overview/#kafka-components","title":"Kafka Components","text":"<p>The diagram below presents Kafka's key components:</p> <p></p>"},{"location":"technology/kafka-overview/#brokers","title":"Brokers","text":"<ul> <li>Kafka runs as a cluster of broker servers that can, in theory, span multiple data centers. Each brokers manages data replication, topic/partition management, offset management. To cover multiple data centers within the same cluster, the network latency between data centers needs to be very low, at the 15ms or less, as there is a lot of communication between kafka brokers and between kafka brokers and zookeeper servers.</li> <li>The Kafka cluster stores streams of records in topics. Topic is referenced by producer to send data to, and subscribed by consumers to get data. Data in topic is persisted to file systems for a retention time period (Defined at the topic level). The file system can be network based.</li> </ul> <p>In the figure above, the Kafka brokers are allocated on three servers, with data within the topic are replicated two times. In production, it is recommended to use at least five nodes to authorize planned failure and un-planned failure, and when doing replicas, use a replica factor at least equals to three.</p>"},{"location":"technology/kafka-overview/#zookeeper","title":"Zookeeper","text":"<p>Zookeeper is used to persist the component and platform states and it runs in cluster to ensure high availability. One zookeeper server is the leader and other are used in backup.</p> <ul> <li>Kafka does not keep state regarding consumers and producers.</li> <li>Depends on kafka version, offsets are maintained in Zookeeper or in Kafka: newer versions use an internal Kafka topic called __consumer_offsets. In any case consumers can read next message (or from a specific offset) correctly even during broker server outrages.</li> <li>Access Controls are saved in Zookeeper</li> </ul> <p>As of Kafka 2.8+ Zookeeper is becoming optional.</p>"},{"location":"technology/kafka-overview/#topics","title":"Topics","text":"<p>Topics represent end points to publish and consume records.</p> <ul> <li>Each record consists of a key, a value (the data payload as byte array), a timestamp and some metadata.</li> <li>Producers publish data records to topic and consumers subscribe to topics. When a record is produced without specifying a partition, a partition will be chosen using a hash of the key. If the record did not provide a timestamp, the producer will stamp the record with its current time (creation time or log append time). Producers hold a pool of buffers to keep records not yet transmitted to the server.</li> <li>Kafka store log data in its <code>log.dir</code> and topic maps to subdirectories in this log directory.</li> <li>Kafka uses topics with a pub/sub combined with queue model: it uses the concept of consumer group to divide the processing over a collection of consumer processes, running in parallel, and messages can be broadcasted to multiple groups.</li> <li>Consumer performs asynchronous pull to the connected brokers via the subscription to a topic.</li> </ul> <p>The figure below illustrates one topic having multiple partitions, replicated within the broker cluster:</p> <p></p>"},{"location":"technology/kafka-overview/#partitions","title":"Partitions","text":"<p>Partitions are basically used to parallelize the event processing when a single server would not be able to process all events, using the broker clustering. So to manage increase in the load of messages, Kafka uses partitions.</p> <p></p> <ul> <li>Each broker may have zero or more partitions per topic. When creating topic we specify the number of partition to use.</li> <li>Kafka tolerates up to N-1 server failures without losing any messages. N is the replication factor for a given partition.</li> <li>Each partition is a time ordered immutable sequence of records, that are persisted for a long time period. It is a log. Topic is a labelled log.</li> <li>Consumers see messages in the order they are stored in the log.</li> <li>Each partition is replicated across a configurable number of servers for fault tolerance. The number of partition will depend on characteristics like the number of consumers, the traffic pattern, etc... You can have 2000 partitions per broker.</li> <li>Each partitioned message has a unique sequence id called offset (\"abcde, ab, a ...\" in the figure above are offsets). Those offset ids are defined when events arrived at the broker level, and are local to the partition. They are immutable.</li> <li>When a consumer reads a topic, it actually reads data from all the partitions. As a consumer reads data from a partition, it advances its offset. To read an event the consumer needs to use the topic name, the partition number and the last offset to read from.</li> <li>Brokers keep offset information in an hidden topic.</li> <li>Partitions guarantee that data with the same keys will be sent to the same consumer and in order.</li> <li>Partitions are saved to disk as append log. The older records are deleted after a given time period or if the size of log goes over a limit. It is possible to compact the log. The log compaction means, the last known value for each message key is kept. Compacted Topics are used in Streams processing for stateful operator to keep aggregate or grouping by key. You can read more about log compaction from the kafka doc.</li> </ul>"},{"location":"technology/kafka-overview/#replication","title":"Replication","text":"<p>Each partition can be replicated across a number of servers. The replication factor is captured by the number of brokers to be used for replication. To ensure high availability it should be set to at least a value of three. Partitions have one leader and zero or more followers.</p> <p></p> <p>The leader manages all the read and write requests for the partition. The followers replicate the leader content. We are addressing data replication in the high availability section below.</p>"},{"location":"technology/kafka-overview/#consumer-group","title":"Consumer group","text":"<p>This is the way to group consumers so the processing of event is parallelized.  The number of consumers in a group is the same as the number of partition defined in a topic.  We are detailing consumer group implementation in this note.</p>"},{"location":"technology/kafka-producers/","title":"Kafka Producers","text":""},{"location":"technology/kafka-producers/#understanding-kafka-producers","title":"Understanding Kafka Producers","text":"<p>A producer is a thread safe kafka client API that publishes records to the  cluster. It uses buffers, thread pool, and serializers to send data.  They are stateless: the consumers is responsible to manage the offsets of  the message they read. When the producer connects via the initial bootstrap connection,  it gets the metadata about the topic - partition and the leader broker to connect to.  The assignment of messages to partition is done following different algorithms:  round-robin if there is no key specified, using the hash code of the key, or custom defined.</p> <p>We recommend reading IBM Event streams producer guidelines to understand  how producers work with its configuration parameters.</p>"},{"location":"technology/kafka-producers/#design-considerations","title":"Design considerations","text":"<p>When developing a record producer you need to assess the followings:</p> <ul> <li>What is the event payload to send? Is is a root aggregate, as defined in domain driven design, with value objects?  Does it need to be kept in sequence to be used as event sourcing? or order does not matter? Remember that when order is important, messages need to go to the same topic. When multiple partitions are used, the messages with the same key will go to the same partition to guaranty the order. See related discussions from Martin Kleppmann on confluent web site. Also to be exhaustive, it is possible to get a producer doing retries that could generate duplicate records as acknowledges may take time to come: within a batch of n records, if the producer did not get all the n acknowledges on time, it may resend the batch. This is where 'idempotence' becomes important (see later section).</li> <li>Is there a strong requirement to manage the schema definition? If using one topic to manage all events about a business entity, then be sure to support a flexible avro schema.</li> <li>What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with <code>buffer.memory</code> property. (See producer configuration API)</li> <li>Can the producer batches events together to send them in batch over one send operation? By design kafka producers batch events.</li> <li>Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 or even better 5, brokers within the cluster to maintain quorum in case of one failure. The client API is implemented to support reconnection.</li> <li> <p>When deploying kafka on Kubernetes, it is important to proxy the broker URLs with a proxy server outside of kubernetes. The HAProxy needs to scale, and as the kafka traffic may be important, it may make sense to have a dedicated HAProxy for clients to brokers traffic.</p> </li> <li> <p>Assess exactly once delivery requirement. Look at idempotent producer: retries will not introduce duplicate records (see section below).</p> </li> <li>Partitions help to scale the consumer processing of messages, but it also helps the producer to be more efficient as it can send message in parallel to different partition.</li> <li>Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that <code>LogAppendTime</code> is considered to be processing time, and <code>CreateTime</code> is considered to be event time.</li> </ul>"},{"location":"technology/kafka-producers/#typical-producer-code-structure","title":"Typical producer code structure","text":"<p>The producer code, using java or python API, does the following steps:</p> <ul> <li>define producer properties</li> <li>create a producer instance</li> <li>Connect to the bootstrap URL, get a broker leader</li> <li>send event records and get resulting metadata.</li> </ul> <p>Producers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgements.</p> <p>Here is an example of producer code from the quick start.</p>"},{"location":"technology/kafka-producers/#kafka-useful-producer-apis","title":"Kafka useful Producer APIs","text":"<p>Here is a list of common API to use in your producer and consumer code.</p> <ul> <li>KafkaProducer A Kafka client that publishes records to the Kafka cluster.  The send method is asynchronous. A producer is thread safe so we can have per topic to interface.</li> <li>ProducerRecord to be published to a topic</li> <li>RecordMetadata metadata for a record that has been acknowledged by the server.</li> </ul>"},{"location":"technology/kafka-producers/#properties-to-consider","title":"Properties to consider","text":"<p>The following properties are helpful to tune at each topic and producer and will vary depending on the requirements:  </p> Properties Description BOOTSTRAP_SERVERS_CONFIG A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer. ACKS_CONFIG specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. RETRIES_CONFIG specifies the number of times to attempt to resend a batch of events. ENABLE_IDEMPOTENCE_CONFIG Set to true, the number of retries will be maximized, and the acks will be set to <code>All</code>. TRANSACTION_ID A unique identifier for a producer. In case of multiple producer instances, a same ID will mean a second producers can commit the transaction. Epoch number, linked to the process ID, avoid having two producers doing this commit. If no transaction ID is specified, the transaction will be valid within a single session."},{"location":"technology/kafka-producers/#advanced-producer-guidances","title":"Advanced producer guidances","text":""},{"location":"technology/kafka-producers/#how-to-support-exactly-once-delivery","title":"How to support exactly once delivery","text":"<p>Knowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events.</p> <p>Producer can set acknowledge level to control the delivery semantic to ensure not loosing data. The following semantic is supported:</p> <ul> <li>At least once: means the producer set  <code>ACKS=1</code> and get an acknowledgement message when the message sent, has been written to at least one time in the cluster (assume replicas = 3).  If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message.</li> <li>At most semantic: means the producer will not do retry in case of no acknowledge received. It may create log and compensation, but the message may be lost.</li> <li>Exactly once means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event.</li> </ul> <p>With <code>acks = 1</code> it is possible to lose messages, as illustrated in the following diagram, where new messages were not replicated yet, and ack was already sent back to the producer. Losing messages will depend if a replica is taking the leader position or not, or if the failed broker goes back online before replicas election but has no fsynch to the disk before the crash. </p> <p></p> <p>To avoid that we need to have <code>ack=-1</code>, three replicas and in-sync-replica=2. Replicated messages are acknowledged, when broker fails, a new leader is selected with the replicated messagesm it becomes the partition leader and others start to replicate from him. Producer reconnect to the partition leader.</p> <p></p> <p>Here is an example of cluster configuration with default set for all topic</p> <pre><code>spec:\n    strimziOverrides:\n        kafka:\n            config:\n                default.replication.factor: 3\n                min.insync.replicas: 2\n</code></pre> <p>Or at the topic level:</p> <pre><code>kind: KafkaTopic\nmetadata:\n  name: rt-store.inventory\n  namespace: rt-inventory-dev\n  labels:\n    eventstreams.ibm.com/cluster: dev\nspec:\n  partitions: 1\n  replicas: 3\n  config:\n    min.insync.replicas: 1\n</code></pre> <p>At the best case scenario, with a replica factor set to 3, a broker responding on time to the producer, and with a consumer committing its offset and reading from the last committed offset it is possible to get only one message end to end.</p> <p></p> <p>Sometime the brokers will not send acknowledge in expected time, and the producer may decide to send the records again, generating duplicate...</p> <p></p> <p>To avoid duplicate message at the broker level, when acknowledge is set to ALL, the producer can also set idempotence flag: ENABLE_IDEMPOTENCE_CONFIG = true. With the idempotence property, the record sent, has a sequence number and a producer id, so that the broker keeps the last sequence number per producer and per partition. If a message is received with a lower sequence number, it means a producer is doing some retries on record already processed, so the broker will drop it, to avoid having duplicate records per partition. If the id is greater than current id known by the broker, the broker will create an OutOfSequence exception, which may be fatal as records may have been lost.</p> <p></p> <p>The sequence number is persisted in a log so even in case of broker leader failure, the new leader will have a good view of the states of the system.</p> <p>The replication mechanism guarantees that, when a message is written to the leader replica, it will be replicated to all available replicas. As soon as you want to get acknowledge of all replicates, it is obvious to set idempotence to true. It does not impact performance.</p> <p>To add to this discussion, as topic may have multiple partitions, idempotent producers do not provide guarantees for writes across multiple Topic-Partition. For that Kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a transacitional protocol with coordinator and control message. Here is an example of such configuration that can be done in a producer constructor method:</p> <pre><code>producerProps.put(\"enable.idempotence\", \"true\");\nproducerProps.put(\"transactional.id\", \"prod-1\");\nkafkaProducer.initTransactions()\n</code></pre> <p><code>initTransactions()</code> registers the producer with the broker as one that can use transaction, identifying it by its <code>transactional.id</code> and a sequence number, or epoch. Epoch is used to avoid an old producer to commit a transaction while a new producer instance was created for that and continues its work.</p> <p>Kafka streams with consume-process-produce loop requires transaction and exactly once. Even commiting its read offset is part of the transaction. So Producer API has a sendOffsetsToTransaction method.</p> <p>See the KIP 98 for details.</p> <p>In case of multiple partitions, the broker will store a list of all updated partitions for a given transaction.</p> <p>To support transaction a transaction coordinator keeps its states into an internal topic (TransactionLog). Control messages are added to the main topic but never exposed to the 'user', so that consumers have the knowledge if a transaction is committed or not. </p> <p>See the code in order command microservice.</p> <p>The consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Consumer waits to read transactional messages until the associated transaction has been committed. Here is an example of consumer code and configuration</p> <pre><code>consumerProps.put(\"enable.auto.commit\", \"false\");\nconsumerProps.put(\"isolation.level\", \"read_committed\");\n</code></pre> <p>With <code>read_committed</code>, no message that was written to the input topic in the same transaction will be read by this consumer until message replicas are all written.</p> <p>In consume-process-produce loop, producer commits its offset with code, and specifies the last offset to read.</p> <pre><code>offsetsToCommit.put(partition, new OffsetAndMetadata(offset + 1))\nproducer.sendOffsetsToTransaction(offsetsToCommit, \"consumer-group-id\");\n</code></pre> <p>The producer then commits the transaction.</p> <pre><code>try {\n    kafkaProducer.beginTransaction();\n    ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(ApplicationConfig.ORDER_COMMAND_TOPIC, key, value);\n    Future&lt;RecordMetadata&gt; send = kafkaProducer.send(record, callBackFunction);\n\n    kafkaProducer.commitTransaction();\n} catch (KafkaException e){\n    kafkaProducer.abortTransaction();\n}\n</code></pre> <p>There is an interesting article from the Baeldung team about exactly once processing in kafka with code example which we have re-used to implement the order processing in our Reefer Container Shipment reference application and explained here</p>"},{"location":"technology/kafka-producers/#code-examples","title":"Code Examples","text":"<ul> <li>Order management with CQRS in Java</li> <li>EDA quickstart Quarkus Producer API</li> <li>Springboot with kafka template</li> <li>Event driven microservice template</li> </ul>"},{"location":"technology/kafka-producers/#more-readings","title":"More readings","text":"<ul> <li>Creating advanced kafka producer in java - Cloudurable</li> <li>Confluent blog: Exactly-once Semantics are Possible: Here\u2019s How Kafka Does it</li> </ul>"},{"location":"technology/kafka-streams/","title":"Kafka Streams","text":"<p>Info</p> <p>Updated 09/06/2022 </p> <p>Kafka Streams is client API to build microservices with input and output data are in Kafka. It is based on programming a graph of processing nodes to support the business logic developer wants to apply on the event streams. </p> <p>We recommend reading this excellent introduction Kafka stream made simple from Jay Kreps from Confluent to get a good understanding of why Kafka stream was created.</p>"},{"location":"technology/kafka-streams/#concepts","title":"Concepts","text":"<p>The business logic is implemented via topology that represents a graph of processing nodes.  Each node within the graph, processes events from the parent node. </p> <p>To summarize, Kafka Streams has the following capabilities:</p> <ul> <li>Kafka Streams applications are built on top of producer and consumer APIs and are leveraging Kafka capabilities to do data parallelism processing, support distributed coordination of partition to task assignment, and being fault tolerant.</li> <li>Streams processing is helpful for handling out-of-order data, reprocessing input as code changes, and performing stateful computations, like real time analytics. It uses producer / consumer APIs, stateful storage and consumer groups. It treats both past and future data the same way.</li> <li>Kafka Streams is an embedded library to integrate in your Java application. No need for separate processing cluster. As deployable container it can scale horizontally easily within Kubernetes platform. It does not run in Kafka cluster.</li> <li>Topology consumes continuous real time flows of records and publishes new flows to one or more topics.</li> <li>A stream (represented by the KStream API) is a durable, partitioned sequence of immutable events. When a new event is added a stream, it's appended to the partition that its key belongs to.</li> <li>It can scale vertically, by increasing the number of threads for each Kafka Streams application on a single machine, and horizontally by adding additional machines or pods in kubernetes.  Each deployed instance use the same value for the <code>application.id</code> kafka stream property.</li> </ul> <p></p> <pre><code>The assignment of stream partitions to stream tasks never changes, so task is the unit of parallelism. Task executes the topology, and is buffering records coming from the attached partitions.\n</code></pre> <ul> <li>KTable is a durable, partitioned collection that models change over time. It's the mutable counterpart of KStreams. It represents what is true at the current moment. Each data record is considered a contextual update. Tables are saved in state store backed up with kafka topic and are queryables. Any operation on the table such as querying, inserting, or updating a row is carried out behind the scenes by a corresponding operation on the table\u2019s state store.</li> </ul> <p>These state stores are being\u00a0materialized on local disk\u00a0inside your application instances</p> <p></p> <ul> <li>It supports exactly-once processing semantics to guarantee that each record is processed once and only once even when there is a failure.</li> <li>Stream APIs transform, aggregate and enrich data, per record with milli second latency, from one topic to another one.</li> <li>Supports stateful and windowing operations by processing one record at a time.</li> <li>An application's processor topology is scaled by breaking it into multiple tasks.</li> <li>Tasks can then instantiate their own processor topology based on the assigned partitions.</li> </ul> <p></p>"},{"location":"technology/kafka-streams/#fault-tolerance","title":"Fault tolerance","text":"<p>As KTables are persisted on state store, they are materialized on local to broker disk, as change log streams:</p> <p></p> <p>In the case of a stream processing task fails, it can rebuild its internal, in memory state store from the kafka topic / change log. Once done it can reconsume messages. The system is fault tolerant.</p>"},{"location":"technology/kafka-streams/#scaling","title":"Scaling","text":"<p>When topics have multiple partitions, each kafka streams task consumes a unique partition. </p> <p></p> <p>If for any reasons, we need to scale by adding new instances of the application, so in term of kubernetes, adding more pods, then the system will rebalance the stream tasks allocation to new instances created.</p> <p></p> <p>We can start as many threads of the application as there are input Kafka topic partitions.</p> <p>Another good example to illustrate threading, task and machine scaling is documented in this on Confluent article.</p>"},{"location":"technology/kafka-streams/#code-structure","title":"Code structure","text":"<p>In general the code for processing event does the following:</p> <ul> <li>Set a properties object to specify which brokers to connect to and what kind of key and value des/serialization mechanisms to use.</li> <li>Define a stream client: if you want to get the stream of records use KStream, if you want a changelog with the last value of a given key use KTable (For example, using KTable to keep a user profile data by userid key).</li> <li>Create a topology of input source and sink target and the set of actions to perform in between.</li> <li>Start the stream client to consume records.</li> </ul> <p>Programming with KStream and Ktable is not easy at first, as there are a lot of concepts for data manipulations, serialization and operations chaining.</p> <p>A stateful operator uses the streaming Domain Specific Language, with constructs for aggregation, join and time window operations. Stateful transformations require a state store associated with the stream processor.</p> <p>We recommend at this stage to do our exercise 1 from the Tech Academy tutorial to develop a first simple topology and test it without any Kafka cluster using the <code>Topology test driver</code>.</p> <p>The following code extract, is part of the Apache Kafka Word count example and is used to illustrate the programming model used: </p> <pre><code>// Streams processing are created from a builder.\nfinal StreamsBuilder builder = new StreamsBuilder();\n// pattern to extract word\nfinal Pattern pattern = Pattern.compile(\"\\\\W+\");\n// source is a kafka topic, and materialized as a KStream\nKStream&lt;String, String&gt; textLines = builder.stream(source);\n// implement the logic to count words\nKTable&lt;String, Long&gt; wordCounts = textLines\n    .flatMapValues(textLine -&gt; Arrays.asList(pattern.split(textLine.toLowerCase())))\n    .print(Printed.toSysOut())\n    .groupBy((key, word) -&gt; word)\n    .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as(\"counts-store\"));\n// sink is another kafka topic. Produce for each word the number of occurrence in the given doc\nwordCounts.toStream().to(sink, Produced.with(Serdes.String(), Serdes.Long()));\n\nKafkaStreams streams = new KafkaStreams(builder.build(), props);\nstreams.start();\n</code></pre> <ul> <li>KStream represents KeyValue records coming as event stream from the input topic.</li> <li><code>flatMapValues()</code> transforms the value of each record in \"this\" stream into zero or more values with the same key in a new KStream (in memory). So here the text line is split into words. The parameter is a ValueMapper which applies transformation on values but keeps the key. Another important transformation is the KeyValueMapper.</li> <li><code>groupBy()</code> Group the records of this KStream on a new key that is selected using the provided KeyValueMapper. So here it creates new KStream with the extracted word as key.</li> <li><code>count()</code> counts the number of records in this stream by the grouped key. <code>Materialized</code> is an class to define a \"store\" to persist state and data. So here the state store is \"counts-store\". As store is a in-memory table, but it could also be persisted in external database. Could be the Facebook's RocksDB key value persistence or a log-compacted topic in Kafka.</li> <li>Produced defines how to provide the optional parameter types when producing to new topics.</li> <li>KTable is an abstraction of a changelog stream from a primary-keyed table.</li> </ul> <p>Important: map, flatMapValues and mapValues ... functions don\u2019t modify the object or value presented as a parameter.</p>"},{"location":"technology/kafka-streams/#available-tutorials","title":"Available tutorials","text":"<p>We found the following tutorial helpful to grow your competency on Kafka Streams:</p> <ul> <li>Word count Kafka Stream example from product documentation</li> <li>Use Quarkus and Kafka Streams to use groupBy, join with another Stream</li> <li>Quarkus and Kafka Streams guides</li> <li>Build an inventory aggregator with Quarkus, with kstreams, ktable and interactive queries, Mutiny, all deployable on OpenShift with quarkus kubernetes plugin.</li> </ul>"},{"location":"technology/kafka-streams/#interactive-queries","title":"Interactive queries","text":"<p>State store can be queried, and this is supported by the interactive queries. Result can be from the local store, if the key is in the local store, or a remote one. The metadata of the key to task allocation is maintained and shared between tasks. </p> <p>As a Kafka stream app runs on multiple instances, the entire state of the app is distributed among the instances. </p> <p>The Stream topology will transform the stream to table with one of the groupBy or aggregate operation:</p> <pre><code>items\n.groupByKey(ItemStream.buildGroupDefinition())\n.aggregate(\n    () -&gt;  new Inventory(),\n    (k , newItem, existingInventory) \n        -&gt; existingInventory.updateStockQuantity(k,newItem), \n        InventoryAggregate.materializeAsInventoryStore()); \n</code></pre> <p>which then it can be materialized as queryable key - value store.</p> <pre><code>// class InventoryAggregate\n/**\n * Create a key value store named INVENTORY_STORE_NAME to persist store inventory\n */\npublic static Materialized&lt;String, Inventory, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materializeAsInventoryStore() {\n    return Materialized.&lt;String, Inventory, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as(INVENTORY_STORE_NAME)\n            .withKeySerde(Serdes.String()).withValueSerde(inventorySerde);\n}\n</code></pre> <p>Each store is local to the instance it was created in:</p> <p></p> <p>The storeID is the key used in the KTable. Once the Kafka Stream is started and store created (loop to get it ready) then it is easy to access it:</p> <pre><code>@Inject\nKafkaStreams streams;\n\nprivate ReadOnlyKeyValueStore&lt;String, Inventory&gt; getInventoryStockStore() {\n        while (true) {\n            try {\n\n                StoreQueryParameters&lt;ReadOnlyKeyValueStore&lt;String,Inventory&gt;&gt; parameters = StoreQueryParameters.fromNameAndType(InventoryAggregate.INVENTORY_STORE_NAME,QueryableStoreTypes.keyValueStore());\n                return streams.store(parameters);\n        ...\n\n// access one element of the store\nInventory result = getInventoryStockStore().get(storeID);\n</code></pre> <p>To get access to remote store, we need to expose each store via an API. The easiest one is a REST api, but it could be any RPC protocol. Each instance is uniquely identified via the <code>application.server</code> property. When deploying on kubernetes it could be the pod IP address accessible via the $POD_IP environment variable.  </p> <p>Below is a quarkus declaration:</p> <pre><code>hostname=${POD_IP:localhost}\nquarkus.kafka-streams.application-server=${hostname}:8080\n</code></pre> <p>Now the design decision is to return the URL of the remote instance to the client doing the query call or do the call internally to the instnace reached to always returning a result.</p> <p>The knowledge of other application instance is done by sharing metadata. The following code example illustrates the access to metadata for store via the kafka stream context and then build a pipeline medata to share information about host, port and partition allocation.</p> <pre><code>streams.allMetadataForStore(ItemStream.ITEMS_STORE_NAME)\n                .stream()\n                .map(m -&gt; new PipelineMetadata(\n                        m.hostInfo().host() + \":\" + m.hostInfo().port(),\n                        m.topicPartitions()\n                                .stream()\n                                .map(TopicPartition::toString)\n                                .collect(Collectors.toSet())))\n                .collect(Collectors.toList());\n</code></pre>"},{"location":"technology/kafka-streams/#design-considerations","title":"Design considerations","text":"<ul> <li>Partitions are assigned to a StreamTask, and each StreamTask has its own state store. So it is important to use key and kafka will assign records with same key to same partition so lookup inside state store will work.</li> <li>Avoid external database lookup as part of the stream: As kafka can handle million of records per second, so a lookup to an external  database to do a join between a primary key that is in the event and a table in the database to do a data enrichment, for example,  is a bad practice. The approach will be to use Ktable, with state store and perform a join in memory.</li> <li>Reference data can be loaded inside a Ktable for event stream enrichment.</li> <li> <p>Table and streams joins: we recommend reading this deep dive article on joining between streams and joining stream with table. The important points from this article:</p> <ul> <li>kstream - kstream joins are windowed to control the size of data to keep in memory to search for the matching records.</li> </ul> </li> <li></li> </ul>"},{"location":"technology/kafka-streams/#faust-a-python-library-to-do-kafka-streaming","title":"Faust: a python library to do kafka streaming","text":"<p>Faust is a python library to support stream processing. It does not have its own DSL as Kafka streams in Java has, but just python functions.</p> <p>It uses rocksdb to support tables.</p> <p>For the installation, in your python environment do a <code>pipenv run pip install faust</code>, or <code>pip install faust</code>. Then use faust as a CLI. So to start an agent as worker use:</p> <pre><code>faust -A nameofthepythoncode -l info\n</code></pre> <p>Multiple instances of a Faust worker can be started independently to distribute stream processing across machines and CPU cores.</p>"},{"location":"technology/kafka-streams/#further-readings","title":"Further readings","text":"<ul> <li>The API and product documentation.</li> <li>Kafka Streams  concepts from Confluent</li> <li>Deep dive explanation for the differences between KStream and KTable from Michael Noll</li> <li>Our set of samples to getting started in coding kafka streams </li> <li>Distributed, Real-time Joins and Aggregations using Kafka Stream, from Michael Noll at Confluent</li> <li>Confluent Kafka Streams documentation</li> <li>Kafka Streams architecture article from Confluent.</li> <li>Andy Bryant's article on kafka stream work allocation and sub-topologies</li> </ul>"},{"location":"technology/mq/","title":"IBM MQ in the context of EDA","text":"<p>Warning</p> <p>Updated 7/13/2022- Work in progress</p> <p>IBM MQ is the enterprise solution to exchange message over queues.  As it supports loosely coupling communication between applications, via asynchronous protocol, and message exchange, it has to be part of any modern digital, responsive solutions, and so it makes sense to write about it in the context of EDA. </p> <p>This note is to summarize, for architects, the technology as it fits into EDA and gives pointers to important documentations, articles, and code repositories for using MQ.</p> <p>We already addressed the difference between event and messaging systems, and we can affirm that real production plaform needs to include both. </p> <p>This site includes a lot of content around Kafka as the backbone to support EDA, but EDA is not just Kafka. I will prefer to mention that EDA is about modern asynchronous microservice based solution, that need to exchange messages.  Messages can be sent to MQ or Kafka or both.  IBM MQ delivers different features than Kafka and it is important to assess the fit for purpose. </p> <p>MQ queue managers are the main component to define queues and where applications connect to.  They can be organized in network to deliver messages between applications and locations.  Queue Managers can be organized in cluster to increase high availability and scaling.</p> <p></p>"},{"location":"technology/mq/#concepts-to-keep-in-mind","title":"Concepts to keep in mind","text":"<p>We encourage to read the article from Richard Coppen's: 'IBM MQ fundamentals'.</p> <ul> <li>Queues are addressable locations to deliver messages to and store them reliably until they need to be consumed.  We can have many queues and topics on one queue manager</li> <li>Queue managers are the MQ servers that host the queues. They can be interconnected via MQ network.</li> <li>Channels are the way queue managers communicate with each other and with the applications.</li> <li>MQ networks are loose collections of interconnected queue managers, all working together to deliver messages between applications and locations.</li> <li>MQ clusters are tight couplings of queue managers, enabling higher levels of scaling and availability</li> <li>Point to point for a single consumer. Senders produce messages to a queue, and receivers asynchronously consume messages from that queue. With multiple receivers, each message is only consumed by one receiver, distributing the workload across them all.</li> <li>Publish/subscribe is supported via topic and subscription, and MQ sends copies of the message to those subscribing applications</li> </ul>"},{"location":"technology/mq/#major-mq-benefits-in-eda","title":"Major MQ benefits in EDA","text":"<ul> <li>MQ provides assured delivery of data: No data loss and no duplication, strong support of exactly once.</li> <li>MQ is horizontally scalable: As the workload for a single queue manager increases, it is easy to add more queue managers to share tasks and distribute the messages across them. </li> <li>Highly available (See section below) with scalable architecture with different topologies</li> <li>Integrate well with Mainframe to propagate transaction to the eventual consistent world of cloud native distributed applications. Writing to database and MQ queue is part of the same transaction, which simplifies the injection into event backbone like Kafka, via Kafka MQ connector.</li> <li>Containerized to run on modern kubernetes platform.</li> </ul>"},{"location":"technology/mq/#decentralized-architecture","title":"Decentralized architecture","text":"<p>The figure below illustrates the different ways to organize the MQ brokers according to the applications' needs.</p> <p></p> <ul> <li>On the top row, applications have decoupled queue managers, with independent availability / scalability. The ownership  is decentralized, as each application owner also owns the broker configuration and deployment.  Such cloud native application may adopt the Command Query Responsability Seggregation pattern and use queues to propagage  information between the microservices. The deployment of both broker and microservices follows the same CI/CD pipeline,  with a <code>kustomize</code>, for example, to describe the broker configuration.  See the CQRS with MQ implementation, we did for the Reefer manager service in the vaccine solution.  </li> <li>A central MQ broker can still be part of the architecture to support legacy applications integrations and federated queues. </li> </ul> <p>This type of deployment supports heterogenous operational procedures across technologies. </p>"},{"location":"technology/mq/#high-availability","title":"High availability","text":"<p>As introduced in multiple blogs and product documentation, we use the high availability definition of a capability of a system to be operational for a greater proportion of time than other available apps. In term of measurement, we are talking about 99.999% which is less than 5 minutes a year (99.99% is 1 hour/yr). </p> <p>Important to always revisit the requirements, in term of availability measurement for any application modernization projects. </p> <p>For a messaging system it is important to consider three characteristics:</p> <ul> <li>Redundancy so applications can connect in case of broker failure. Applications locally bound to a queue manager will limit availability. Recommended to use remote MQ client connection, and considering <code>automatic client reconnection</code>.</li> <li>Message routing: always deliver message even with failures. </li> <li>Message availability: not loosing messages and always readable.</li> </ul> <p>With IBM MQ on multiplatforms, a message is stored on exactly one queue manager. To achieve high message availability, you need to be able to recover a queue manager as quickly as possible. You can achieve service availability by having multiple instances of queue manager for client applications to use, for example by using an IBM MQ uniform cluster.</p> <p>A set of MQ topology can be defined to support HA:</p> <p></p> <ol> <li>Single resilient queue manager: MQ broker runs in a VM or a single container, and if it stops the VM or pod scheduler will restart it. This is using the platform resynch capability combined with HA storage. IP Address is kept between the instances. The queue content is saved to a storage supporting HA. In the case of container, new restarted pod will connect to existing storage, and the IP gateway routes traffic to the active instance via service and app selector.</li> <li>Multi-instance queue manager: active - standby topology - Failover is triggered on failure of the active instance. IP Address is also kept. When using k8s, the stand-by broker is on a separate node, ready to be activated. The pods use persistence volumes with ReadWriteMany settings. </li> <li>Replicated data queue manager: this is an extension of the previous pattern where data saved locally is replicated to other sites.</li> </ol> <p>The deployed MQ broker is defined in k8s as a <code>StatefulSet</code> which may not restart automatically in case of node failure. So there is a time to fail over, which is not the case with the full replication mechanism of Kafka. A service will provide consistent network identity to the MQ broker.</p> <p>On kubernetes, MQ relies on the availability of the data on the persistent volumes. The availability of the storage providing the persistent volumes, defines IBM MQ availability.</p> <p>For multi-instance deployment, the shared file system must support write through to disk on flush operation, to keep transaction integrity (ensure writes have been safely committed before acknowledging the transaction), must support exclusive access to files so the queue managers write access is synchronized. Also it needs to support releasing locks in case of failure.</p> <p>See product documentation for testing message integrity on file systems.</p>"},{"location":"technology/mq/#active-active-with-uniform-cluster","title":"Active-active with uniform cluster","text":"<p>With Uniform Cluster and  Client Channel Definition Tables it will be possible to achieve high availability with at least three brokers and multiple application instances accessing brokers group via the CCDT. The queue managers are configured almost identically, and application interacts with the group.</p> <p></p> <p>You can have as many application instances as there are queue managers in the cluster. </p> <p>Here are the main benefits of Uniform Cluster:</p> <ul> <li>A directory of all clustering resources, discoverable by any member in a cluster</li> <li>Automatic channel creation and connectivity</li> <li>Horizontal scaling across multiple matching queues, using message workload balancing</li> <li>Dynamic message routing, based on availability</li> </ul> <p>The brokers are communicating their states between each others, and connection rebalancing can be done behind the scene without application knowledge. In case of a Queue manager failure, the connections are rebalanced to the active ones. Those applications do not need strong ordering. </p> <p>With the same approach, we can add new Queue manager See this video from David Ware abour active - active with Uniform cluster to see how this rebalancing works between queue managers as part of a Uniform queue manager.</p> <p>See the 'MQ Uniform Cluster' related repository.</p>"},{"location":"technology/mq/#native-ha","title":"Native HA","text":"<p>Native HA queue managers involve an active and two replica Kubernetes <code>Pods</code>, which run as part of a  Kubernetes <code>StatefulSet</code> with exactly three replicas each with their own set of Kubernetes Persistent Volumes.</p> <p>Native HA provides built in replication of messages and state across multiple sets of storage, removing the  dependency of replication and locking from the file system.</p> <p>Each replica writes to its own recovery log, acknowledges the data, and then updates its own queue data from the replicated recovery log.</p> <p></p> <p>A Kubernetes Service is used to route TCP/IP client connections to the current active instance. </p> <p>Set the availability in the queueManager configuration.</p> <pre><code>  queueManager:\n    availability:\n      type: NativeHA\n</code></pre>"},{"location":"technology/mq/#disaster-recovery","title":"Disaster recovery","text":"<p>For always on deployment we need three data center and active-active on the three data center. So how is it supported with MQ?</p>"},{"location":"technology/mq/#installation-with-cloud-pak-for-integration","title":"Installation with Cloud Pak for Integration","text":"<p>Starting with release 2020.2, MQ can be installed via Kubernetes Operator on Openshift platform. From the operator catalog search for MQ. See the product documentation installation guide for up to date details.</p> <p>You can verify your installation with the following CLI, and get the IBM catalogs accessible:</p> <pre><code>oc project openshift-marketplace\noc get CatalogSource\nNAME                   DISPLAY                TYPE      PUBLISHER     AGE\ncertified-operators    Certified Operators    grpc      Red Hat       42d\ncommunity-operators    Community Operators    grpc      Red Hat       42d\nibm-operator-catalog   ibm-operator-catalog   grpc      IBM Content   39d\nopencloud-operators    IBMCS Operators        grpc      IBM           39d\nredhat-marketplace     Red Hat Marketplace    grpc      Red Hat       42d\nredhat-operators       Red Hat Operators      grpc      Red Hat       42d\n</code></pre> <p>Once everything is set up, create an operator. The IBM MQ operator can be installed scoped to a single namespace or to monitor <code>All namespaces</code>.  </p> <p></p> <p>Verify your environment fits the deployment. Prepare your Red Hat OpenShift Container Platform for MQ Then once the operator is installed (it could take up to a minute), go to the operator page and create a MQ Manager instance. For example be sure to have defined an ibm-entitlement-key in the project you are planning to use to deploy MQ manager.</p> <p></p> <p>Then update the Yaml file for name, license and persistence.</p> <p></p> <p>As an alternate, define a QueueManager manifest yaml file as:</p> <p>```yaml apiVersion: mq.ibm.com/v1beta1 kind: QueueManager metadata:   name: eda-mq-lab spec:   version: 9.2.5.0-r3   license:     accept: true     license:      use: NonProduction   web:     enabled: true   queueManager:     name: \"EDAQMGR1\"     storage:       queueManager:         type: ephemeral   template:     pod:       containers:        - name: qmgr          env:          - name: MQSNOAUT            value: \"yes\"  <pre><code>Then create the QueueManager resource with `oc or kubectl cli`: \n\n```shell\noc apply -f mq-manager.yaml \noc get queuemanager\n# Get the UI route \noc describe queuemanager eda-mq-lab\n</code></pre> <p>You should get the console from this URL: https://eda-mq-lab-ibm-mq-web-....containers.appdomain.cloud/ibmmq/console/#/</p> <p></p> <p>To access to the <code>mqsc</code> CLI and run configuration remote connect via <code>oc exec -it &lt;podname&gt; bash</code>.</p> <p>We need route to access MQ from outside of OpenShift. The connection uses TLS1.2. </p> <p>you can customize the environment using config map:</p> <pre><code>  queueManager:\n    name: QM1\n    mqsc:\n      - configMap:\n          name: mq-mqsc-config\n          items:\n            -  example.mqsc\n# ...\n  template:\n    pod:\n      containers:\n        - name: qmgr\n          env:\n            - name: MQSNOAUT\n              value: 'yes' \n          envFrom:\n          - configMapRef:\n              name: mq-config\n</code></pre> <p>And</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mq-config\ndata:\n  LICENSE: accept\n  MQ_APP_PASSWORD: passw0rd\n  MQ_ENABLE_METRICS: \"true\"\n  MQ_QMGR_NAME: QM1\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mq-mqsc-config\ndata:\n  example.mqsc: |\n    DEFINE QLOCAL('ITEMS') REPLACE\n    DEFINE CHANNEL('DEV.ADMIN.SVRCONN') CHLTYPE(SVRCONN) REPLACE\n    DEFINE QLOCAL('DEV.DEAD.LETTER.QUEUE') REPLACE\n    ALTER QMGR DEADQ('DEV.DEAD.LETTER.QUEUE')\n    DEFINE CHANNEL(DEV.APP.SVRCONN) CHLTYPE(SVRCONN) \n    ALTER QMGR CHLAUTH (DISABLED)\n    REFRESH SECURITY TYPE(CONNAUTH)\n</code></pre>"},{"location":"technology/mq/#running-mq-in-docker","title":"Running MQ in docker","text":"<p>The following recent article from Richard J. Coppen presents such deployment, and can be summarized as:</p> <pre><code># Use Docker to create a volume:\ndocker volume create qm1data\n# Start queue manager: QM1\ndocker run --env LICENSE=accept --env MQ_QMGR_NAME=QM1 --volume qm1data:/mnt/mqm --name mq --rm --publish 1414:1414 --publish 9443:9443 --detach --env MQ_APP_PASSWORD=passw0rd ibmcom/mq:latest\n# The queue manager\u2019s listener listens on port 1414 for incoming connections and port 9443 is used by MQ console\n</code></pre> <p>One queue is created DEV.QUEUE.1 and a channel: DEV.APP.SRVCONN. </p> <p>Then <code>docker exec</code> on the docker container and use the <code>mqsc</code> CLI.</p> <p>The ibm-messaging/mq-container github repository describes properties and different configurations.</p> <p>You can also run it via docker compose. We have different flavor in the real time inventory gitops repository under <code>local-demo</code> folder. Here is an example of such compose file:</p> <pre><code>version: '3.7'\nservices:\n  ibmmq:\n    image: ibmcom/mq\n    ports:\n        - '1414:1414'\n        - '9443:9443'\n        - '9157:9157'\n    volumes:\n        - qm1data:/mnt/mqm\n    stdin_open: true\n    tty: true\n    restart: always\n    environment:\n        LICENSE: accept\n        MQ_QMGR_NAME: QM1\n        MQ_APP_PASSWORD: passw0rd\n        MQ_ENABLE_METRICS: \"true\"\n</code></pre>"},{"location":"technology/mq/#getting-access-to-the-mq-console","title":"Getting access to the MQ Console","text":"<p>The MQ Console is a web browser based interface for interacting with MQ objects. It comes pre-configured inside the  developer version of MQ in a container. On localhost deployment the URL is  https://localhost:9443/ibmmq/console/ (user admin) while on OpenShift it depends of the Route created.</p> <p>See this article for a very good overview for using the console.</p> <p>From the console we can define access and configuration:</p> <ul> <li>A new channel called MQ.QUICKSTART.SVRCONN. This new channel will be configured with NO MCA user. An MCA user is the identity that is used for all communication on that channel. As we are setting no MCA user this means that the identity used to connect to the queue manager will be used for MQ authorization.</li> <li>A channel authority record set to block no-one. We will do this so that any authority records you add in the following security step will be automatically configured to allow access the queue manager and resources below.</li> </ul> <p>When an application connects to a queue manager it will present an identity. That identity needs two permissions; one to connect to the queue manager and one to put/get messages from the queue.</p>"},{"location":"technology/mq/#some-useful-cli","title":"Some useful CLI","text":"<p>Those commands can be run inside the docker container: <code>oc exec -ti mq1-cp4i-ibm-mq-0 -n cp4i-mq1 bash</code> <pre><code># Display MQ version\ndspmqver\n# Display your running queue managers \ndspmq\n</code></pre></p> <p>To access to log errors</p> <pre><code>oc rsh &lt;to-mq-broker-pod&gt;\n# use you QM manager name instead of BIGGSQMGR\ncd /var/mqm/qmgrs/BIGGSQMGR/errors\ncat AMQERR01.LOG\n</code></pre>"},{"location":"technology/mq/#connecting-your-application","title":"Connecting your application","text":"<p>JMS should be your first choice to integrate a Java application to MQ. The mq-dev-patterns includes JMS code samples for inspiration.  See also this IBM developer tutorial  and our code example from the store simulator used in different MQ to Kafka labs.</p> <p>The article seems to have issue in links and syntax, below is the updated steps:</p> <pre><code># get the file\nmkdir -p com/ibm/mq/samples/jms &amp;&amp; cd com/ibm/mq/samples/jms\ncurl -o JmsPuGet.java https://raw.githubusercontent.com/ibm-messaging/mq-dev-samples/master/gettingStarted/jms/com/ibm/mq/samples/jms/JmsPutGet.java\ncd ../../../../..\n# Modify the connection setting in the code\n# Compile\njavac -cp com.ibm.mq.allclient-9.2.1.0.jar:javax.jms-api-2.0.1.jar com/ibm/mq/samples/jms/JmsPutGet.java\n# Run it\njava -cp com.ibm.mq.allclient-9.2.1.0.jar:javax.jms-api-2.0.1.jar:. com.ibm.mq.samples.jms.JmsPutGet\n</code></pre> <p>To connect to MQ server, you need to get the hostname for the Queue manager, the port number, the channel and the queue name to access.</p> <p>For Quarkus and Maven use the following dependencies:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;javax.jms&lt;/groupId&gt;\n  &lt;artifactId&gt;javax.jms-api&lt;/artifactId&gt;\n  &lt;version&gt;2.0.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n  &lt;groupId&gt;com.ibm.mq&lt;/groupId&gt;\n  &lt;artifactId&gt;com.ibm.mq.allclient&lt;/artifactId&gt;\n  &lt;version&gt;9.2.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>For automatic client reconnection, use the JMS connection factory configuration.</p>"},{"location":"technology/mq/#client-channel-definition-tables","title":"Client Channel  Definition Tables","text":"<p>CCDT provides encapsulation and abstraction of connection information for applications, hiding the MQ architecture and configuration from the application. CCDT defines which real queue managers the application will connect to. Which could be a single queue manager or a group of queue managers.</p> <pre><code>  \"channel\": [\n    {\n      \"name\": \"STORE.CHANNEL\",\n      \"clientConnection\": {\n        \"connection\": [\n          {\n            \"host\": \"mq1-cp4i-ibm-mq-qm-cp4i-mq1........com\",\n            \"port\": \"443\"\n          }\n        ],\n        \"queueManager\": \"BIGGSQMGR\"\n      },\n      \"type\": \"clientConnection\"\n    }\n  ]\n}\n</code></pre> Important readings <ul> <li>MQ family page</li> <li>MQ 9.3 product documentation</li> <li>Article for developer from Richard Coppen's: 'IBM MQ fundamentals'</li> <li>MQ on Container</li> <li>Learning path: IBM MQ Developer</li> <li>Developer cheat sheet</li> </ul>"},{"location":"technology/mq/#code-repositories","title":"Code repositories","text":"<ul> <li>Store simulator - JMS producer to MQ</li> <li>AMQP and reactive messaging</li> </ul>"},{"location":"technology/security/","title":"Kafka Security Overview","text":"<p>Updated 04/13/2022</p> <p>Review this video for a refresh on SSL and TLS certificates and keep in mind what the speaker quotes:</p> <ul> <li>Any message encrypted with Bob's public key can only be decrypted with Bob's private key</li> <li>Anyone with access to Alice's public key can verify that a message could only have been created by someone with access to Alice's private key.</li> </ul> <p></p> <p>For a deeper dive into security administration see this confluent article and Kafka's product documentation.</p> <p>We also strongly recommend reading Rick Osowski's blogs Part 1 and Part 2 on Kafka security configuration.</p>"},{"location":"technology/security/#understand-the-kafka-cluster-listeners","title":"Understand the Kafka cluster listeners","text":"<p>You can secure your IBM Event Streams resources by managing the access each user and application has to each resource.</p> <p>An Event Streams cluster can be configured to expose up to 2 internal and 1 external Kafka listeners. These listeners provide the mechanism for Kafka client applications to communicate with the Kafka brokers and these can be configured as secured listeners (which is the default for the <code>tls</code> and <code>external</code> Kafka listener you will see below).</p> <p>Each Kafka listener providing a connection to Event Streams can also be configured to authenticate connections with either Mutual TLS or SCRAM SHA 512 authentication mechanisms.  Additionally, the Event Streams cluster can be configured to authorize operations sent via an authenticated listener using access control list defined at the user level.</p> <p>The following figure presents a decision tree and the actions to consider for configuring cluster and applications.</p> <p></p> <p>In Event Streams, the following yaml snippet from an IBM Event Streams instance definition defines the following Kafka listeners\"</p> <ul> <li>One internal non secured kafka listener on port <code>9092</code> called <code>plain</code></li> <li>One internal secured (TLS encrypted) Kafka listener on port <code>9093</code> called <code>tls</code>, which also enforces authentication throughout TLS, and </li> <li> <p>One external secured (TLS encrypted) Kafka listener on port <code>9094</code> called <code>external</code>, which also enforces authentication throughout SCRAM credentials, that is exposed through a route.</p> <p><pre><code>listeners:\n  - name: plain\n    port: 9092\n    type: internal\n    tls: false\n  - name: tls\n    port: 9093\n    type: internal\n    tls: true\n    authentication:\n        type: tls\n  - name: external\n    type: route\n    port: 9094\n    tls: true \n    authentication:\n      type: scram-sha-512\n</code></pre> (*) <code>tls: true</code> enforces traffic encryption. Default is true for Kafka listeners on ports <code>9093</code> and <code>9094</code></p> <p>(**) <code>type: internal</code> specifies that a Kafka listener is internal. Kafka listenes on ports <code>9092</code> and <code>9093</code> default to internal.</p> </li> </ul>"},{"location":"technology/security/#to-connect-to-kafka-using-kafka-api","title":"To connect to kafka using Kafka API","text":"<p>The most important and essential property to connect to a Kafka broker is the <code>bootstrap.servers</code> property. This property tells Kafka clients what URL to use to talk to kafka cluster. <code>bootstrap.server</code> defines what Kafka listener your application will use to connect to Kafka. And based on that Kafka listener, you may need to provide your application with extra configuration.</p> <p>At the very minimum, you will need to set the security.protocol property that will tell whether you are connecting to a secured Kafka listener or not. As a result, the values for <code>security.protocol</code> are:</p> <ul> <li><code>PLAINTEXT</code> - using PLAINTEXT transport layer &amp; no authentication - default value.</li> <li><code>SSL</code> - using SSL transport layer &amp; certificate-based authentication or no authentication.</li> <li><code>SASL_PLAINTEXT</code> - using PLAINTEXT transport layer &amp; SASL-based authentication.</li> <li><code>SASL_SSL</code> - using SSL transport layer &amp; SASL-based authentication.</li> </ul> <p>Based on the above, the security protocol you will use to connect to the different Kafka listeners that IBM Event Streams deploys are:</p> <ul> <li><code>PLAINTEXT</code> when connecting to the non secured internal <code>plain</code> Kafka listener on port <code>9092</code></li> <li><code>SSL</code> when connecting to the secured (TLS encrypted) internal <code>tls</code> Kafka listener on port <code>9093</code> that also enforces authentication through TLS certificates</li> <li><code>SASL_SSL</code> when connecting to the secured (TLS encrypted) external <code>external</code> Kafka listener on port <code>9094</code> that also enforces authentication through SCRAM credentials.</li> </ul>"},{"location":"technology/security/#non-secured-listener","title":"Non-secured listener","text":"<p>You would only need to specify that there is no security in place for your application to connect to a non-secured kafka listener:</p> <pre><code>security.protocol=PLAINTEXT\n</code></pre>"},{"location":"technology/security/#secured-listener","title":"Secured listener","text":"<p>In order for your application to be able to connect to Kafka through the internal secured (TLS encrypted) Kafka listener, you need to set the appropriate value for <code>security.protocol</code> as seen above plus provide the Certificate Authority of the Kafka cluster (its public key).</p> <p>Depending on the technology of your application, you will need to provide the Certificate Authority of the Kafka cluster for the TLS encryption either as a <code>PKCS12</code> certificate for a Java client or as a <code>PEM</code> certificate for anything else. <code>PKCS12</code> certificates (or truststores) come in the form of a <code>.p12</code> file and are secured with a password. You can inspect a <code>PKCS12</code> certificate with:</p> <pre><code>openssl pkcs12 -info -nodes -in truststore.p12\n</code></pre> <p>and providing the truststore password.</p> <p>An example of the output would be:</p> <pre><code>MAC Iteration 100000\nMAC verified OK\nPKCS7 Encrypted data: Certificate bag\nBag Attributes\n    friendlyName: ca.crt\n    2.16.840.1.113894.746875.1.1: &lt;Unsupported tag 6&gt;\nsubject=/O=io.strimzi/CN=cluster-ca v0\nissuer=/O=io.strimzi/CN=cluster-ca v0\n-----BEGIN CERTIFICATE-----\nMIIDOzCCAiOgAwIBAgIUe0BjKXdgPF+AMpMXvPREf5XCZi8wDQYJKoZIhvcNAQEL\nBQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2\nMDAeFw0yMjAzMDgxMjU1MjJaFw0yMzAzMDgxMjU1MjJaMC0xEzARBgNVBAoMCmlv\nLnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggEiMA0GCSqGSIb3DQEB\nAQUAA4IBDwAwggEKAoIBAQDLKGs6BfZVM1gWqZhOzbB/iqhVktBhTXC4u4V7d+kx\nOF4JJDPcbhZbpajn7ADABDJtE38cc6qzflJqUWlcqjIhdl7FUUSso/z9/FduSF0j\ndM9LUjwzII3TMq3vnqYxjbwb2u0NTtgT3n6Qi8ST/9qmlCOFJfzUvXErYx00IZ2c\nBj3PG6OoZbJjb3RgkQi+2CxGL95G3xd6v/5ZmHt2YRe5MxMN7pU0z1LHOR0zZvGk\nH2B2d+4S8dSX6lA84XKENFbtiZiglcMEdyu9Uy5DOfznw9eXzysal6UOzEu0mInD\n25gdtPJVKgrAbSMPI4eKmA9JjP8gYwhorj6r/ra0hcj7AgMBAAGjUzBRMB0GA1Ud\nDgQWBBT9IfWWejk2NXK8RkreFe2atXhMBDAfBgNVHSMEGDAWgBT9IfWWejk2NXK8\nRkreFe2atXhMBDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAa\n/aq2+Mb4g7/GLLOo+4nY7LZ2Zl7K37elymxHhafpKdhZYHEAIgj+Gda3OJytHMwq\n3KqDyBJW4IptT631Z70EsMHM+J9ok/KupAbNMilCfevrzVAzXnFhSm3OCVIelLag\njxlzb9da45/0ZwpTg93x2r3s8GhpLKTSUJEHL2ywsY65VZ5JSbyz9TIaYmnlnoL0\nJsuP73iJlg2Nmsd7zVXekqXo/r5I/sraNet2nqc9YyLs6+pzhsfq0oTDT2nA1nZk\nDl9DpLZo0fVoJF73k2z2mBk8gCjGqZk289octuOCr+MwXcGN6JTR2Iux05TBI6uf\n924CQFYsZS2kdhl5GgqQ\n-----END CERTIFICATE-----\n</code></pre> <p>On the other hand, <code>PEM</code> certificates come in the form of a <code>.pem</code> file and are not password protected.</p> <p>You can inspect them using <code>cat</code>. </p> <p>The output should be the same certificate as the one provided within the <code>PKCS12</code> certificate:</p> <pre><code>cat es-cert.pem \n-----BEGIN CERTIFICATE-----\nMIIDOzCCAiOgAwIBAgIUe0BjKXdgPF+AMpMXvPREf5XCZi8wDQYJKoZIhvcNAQEL\nBQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2\nMDAeFw0yMjAzMDgxMjU1MjJaFw0yMzAzMDgxMjU1MjJaMC0xEzARBgNVBAoMCmlv\nLnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggEiMA0GCSqGSIb3DQEB\nAQUAA4IBDwAwggEKAoIBAQDLKGs6BfZVM1gWqZhOzbB/iqhVktBhTXC4u4V7d+kx\nOF4JJDPcbhZbpajn7ADABDJtE38cc6qzflJqUWlcqjIhdl7FUUSso/z9/FduSF0j\ndM9LUjwzII3TMq3vnqYxjbwb2u0NTtgT3n6Qi8ST/9qmlCOFJfzUvXErYx00IZ2c\nBj3PG6OoZbJjb3RgkQi+2CxGL95G3xd6v/5ZmHt2YRe5MxMN7pU0z1LHOR0zZvGk\nH2B2d+4S8dSX6lA84XKENFbtiZiglcMEdyu9Uy5DOfznw9eXzysal6UOzEu0mInD\n25gdtPJVKgrAbSMPI4eKmA9JjP8gYwhorj6r/ra0hcj7AgMBAAGjUzBRMB0GA1Ud\nDgQWBBT9IfWWejk2NXK8RkreFe2atXhMBDAfBgNVHSMEGDAWgBT9IfWWejk2NXK8\nRkreFe2atXhMBDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAa\n/aq2+Mb4g7/GLLOo+4nY7LZ2Zl7K37elymxHhafpKdhZYHEAIgj+Gda3OJytHMwq\n3KqDyBJW4IptT631Z70EsMHM+J9ok/KupAbNMilCfevrzVAzXnFhSm3OCVIelLag\njxlzb9da45/0ZwpTg93x2r3s8GhpLKTSUJEHL2ywsY65VZ5JSbyz9TIaYmnlnoL0\nJsuP73iJlg2Nmsd7zVXekqXo/r5I/sraNet2nqc9YyLs6+pzhsfq0oTDT2nA1nZk\nDl9DpLZo0fVoJF73k2z2mBk8gCjGqZk289octuOCr+MwXcGN6JTR2Iux05TBI6uf\n924CQFYsZS2kdhl5GgqQ\n-----END CERTIFICATE-----\n</code></pre> <p>You can find in the Event Streams with CP4i section how to obtain the Certificate Authority of your IBM Event Streams instance.</p> <p>Once you have the Certificate Authority of your Kafka cluster, you will provide its location and password in your properties file through the <code>ssl.truststore.location</code> and <code>ssl.truststore.password</code> properties.</p> <pre><code>security.protocol=SSL or SASL_SSL\nssl.protocol=TLSv1.2\n\nssl.truststore.password=&lt;truststore.p12-password&gt;\nssl.truststore.location=truststore.p12\nssl.truststore.type=PKCS12\n</code></pre> <p>where <code>security.protocol</code> will vary between <code>SSL</code> or <code>SASL_SSL</code> based on the authentication as you will see next.</p>"},{"location":"technology/security/#authentication","title":"Authentication","text":"<p>You have seen above that your Kafka listeners can require authentication to any application or client wanting to connect to the Kafka cluster through them. It was also said that authentication could be either of type SASL-based, through SCRAM (modern Salted Challenge Response Authentication Mechanism) credentials, or certificate-based (TLS). Either way, IBM Event Streams will handle authentication through <code>KafkaUser</code> objects. </p> <p>These objects that represent Kafka users of your IBM Event Streams instance will have their authentication (and authorization through ACLs) credentials or TLS certificates associated to them stored in a secret. In order to find out how to create these <code>KafkaUsers</code>, which will vary depending on the authentication method, check out this section.</p>"},{"location":"technology/security/#scram","title":"Scram","text":"<p>If you have created a <code>KafkaUser</code> to be used with a Kafka listener that requires SCRAM authentication, you will be able to retrieve its SCRAM credentials either from the IBM Event Streams UI at creation time or later on from the secret these are stored to:</p> <pre><code>oc extract secret/&lt;KAFKA_USER&gt; -n &lt;NAMESPACE&gt; --keys=sasl.jaas.config --to=-\n</code></pre> <p>where</p> <ul> <li><code>&lt;KAFKA_USER&gt;</code> is the name of the <code>KafkaUser</code> object you created.</li> <li><code>&lt;NAMESPACE&gt;</code> is the namespace where IBM Event Streams is deployed on.</li> </ul> <p>Example:</p> <pre><code>oc extract secret/test-app -n tools --keys=sasl.jaas.config --to=-\n# sasl.jaas.config\norg.apache.kafka.common.security.scram.ScramLoginModule required username=\"test-app\" password=\"VgWpkjAkvxH0\";\n</code></pre> <p>You can see above your SCRAM username and password.</p>"},{"location":"technology/security/#tls","title":"TLS","text":"<p>If you have created a <code>KafkaUser</code> to be used with a Kafka listener that requires TLS authentication,  you will be able to retrieve its TLS certificates either from the IBM Event Streams UI at creation time in a zip folder or  later on from the secret these are stored to.</p> <p>First, describe the secret to see what certificates are stored in it:</p> <pre><code>$ oc describe secret test-app-tls -n tools\nName:         test-app-tls\nNamespace:    tools\nLabels:       app.kubernetes.io/instance=test-app-tls\n              app.kubernetes.io/managed-by=strimzi-user-operator\n              app.kubernetes.io/name=strimzi-user-operator\n              app.kubernetes.io/part-of=eventstreams-test-app-tls\n              eventstreams.ibm.com/cluster=es-inst\n              eventstreams.ibm.com/kind=KafkaUser\nAnnotations:  &lt;none&gt;\n\nType:  Opaque\n\nData\n====\nuser.key:       1704 bytes\nuser.p12:       2384 bytes\nuser.password:  12 bytes\nca.crt:         1180 bytes\nuser.crt:       1025 bytes\n</code></pre> <p>You can see that the secret will store the following:</p> <ul> <li><code>user.key</code> and <code>user.crt</code> - the client certificate key-pair.</li> <li><code>user.p12</code> - trustore that contains the <code>user.key</code> and <code>user.crt</code>.</li> <li><code>user.password</code> - contains the <code>user.p12</code> truststore password.</li> <li><code>ca.crt</code> - CA used to sign the client certificate key-pair.</li> </ul> <p>Then, you can extract the appropriate certificate based on whether your application or Kafka client is Java based or not. In the case of a Java based application or Kafka client, extract the <code>user.p12</code> and <code>user.password</code>:</p> <pre><code>oc extract secret/&lt;KAFKA_USER&gt; -n &lt;NAMESPACE&gt; --keys=user.p12\noc extract secret/&lt;KAFKA_USER&gt; -n &lt;NAMESPACE&gt; --keys=user.password\n</code></pre> <p>where</p> <ul> <li><code>&lt;KAFKA_USER&gt;</code> is the name of the <code>KafkaUser</code> object you created.</li> <li><code>&lt;NAMESPACE&gt;</code> is the namespace where IBM Event Streams is deployed on.</li> </ul>"},{"location":"technology/security/#properties-config","title":"Properties config","text":"<p>Now that you know how to get the authentication credentials or certificates for a proper authentication of your application or Kafka client you need to configure the appropriate properties for that:</p> <ul> <li>If your Kafka listener authentication method is SCRAM:</li> </ul> <pre><code>security.protocol=SASL_SSL\n\nsasl.mechanism=SCRAM-SHA-512\nsasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\"&lt;USERNAME&gt;\" password\\=\"&lt;PASSWORD&gt;\";\n</code></pre> <ul> <li>If your Kafka listener authentication method is TLS:</li> </ul> <pre><code>security.protocol=SSL\n\nssl.keystore.location=&lt;location_to_your_user.p12&gt;\nssl.keystore.password=&lt;user.p12-password&gt;\nssl.keystore.type=PKCS12\n</code></pre>"},{"location":"technology/security/#recapitulation","title":"Recapitulation","text":"<p>Let's have a full look at how the Kafka communication properties, for a Java application or client, would look like for IBM Event Streams  on RedHat OpenShift with the defaults. If you look at the IBM Event Streams instance deployment sample definitions  in this GitHub repository,  that is mentioned in the IBM Event Streams official documentation here,  you will see that the defaults defined for the Kafka listeners for all of them (except from the <code>light-insecure.yaml</code> sample) are:</p> <pre><code>listeners:\n  external:\n    type: route\n    authentication:\n      type: scram-sha-512\n  tls:\n    authentication:\n      type: tls\n</code></pre> <p>This translates to Strimzi (the open source project IBM Event Streams is based on) in:</p> <pre><code>listeners:\n  - name: tls\n    port: 9093\n    type: internal\n    tls: true\n    authentication:\n        type: tls\n  - name: external\n    type: route\n    port: 9094\n    tls: true \n    authentication:\n      type: scram-sha-512\n</code></pre> <p>Let's also add the <code>plain</code> non-secure Kafka listener to the picture so that all cases are covered in this recap section.</p> <pre><code>listeners:\n  plain: {}\n  external:\n    type: route\n    authentication:\n      type: scram-sha-512\n  tls:\n    authentication:\n      type: tls\n</code></pre> <p>As a result, the IBM Event Streams instance deployed will count with:</p> <ul> <li>One internal non secured kafka listener on port <code>9092</code> called <code>plain</code></li> <li>One internal secured (TLS encrypted) Kafka listener on port <code>9093</code> called <code>tls</code>, which also enforces authentication throughout TLS, and </li> <li>One external secured (TLS encrypted) Kafka listener on port <code>9094</code> called <code>external</code>, which also enforces authentication throughout SCRAM credentials, that is exposed through a route.</li> </ul>"},{"location":"technology/security/#plain","title":"Plain","text":"<p>The Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the non secured kafka listener on port <code>9092</code> called <code>plain</code> will be as follows:</p> <pre><code># Internal plain listener\n# =======================\nsecurity.protocol=PLAINTEXT\nbootstrap.servers=&lt;ES_NAME&gt;-kafka-bootstrap.&lt;NAMESPACE&gt;.svc\\:9092\n</code></pre> <p>where</p> <ul> <li><code>&lt;ES_NAME&gt;</code> is the name of the IBM Event Streams instance deployed you are trying to connect to.</li> <li><code>&lt;NAMESPACE&gt;</code> is the namespace the IBM Event Streams instance you are trying to connect to is deployed in.</li> </ul>"},{"location":"technology/security/#internal-tls","title":"Internal tls","text":"<p>The Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the internal s ecured (TLS encrypted) Kafka listener on port <code>9093</code> called <code>tls</code>, which also enforces authentication throughout mTLS will be as follows:</p> <pre><code># Internal tls listener\n# =====================\nbootstrap.servers=&lt;&lt;ES_NAME&gt;-kafka-bootstrap.&lt;NAMESPACE&gt;.svc\\:9093\n\nsecurity.protocol=SSL\nssl.protocol=TLSv1.2\n\n## mTLS Authentication for the client.\nssl.keystore.location=&lt;user.p12-location&gt;\nssl.keystore.password=&lt;user.p12-password&gt;\nssl.keystore.type=PKCS12\n\n## Certificate Authority of your Kafka cluster\nssl.truststore.password=&lt;trustore.p12-password&gt;\nssl.truststore.location=&lt;truststore.p12-location&gt;\nssl.truststore.type=PKCS12\n</code></pre> <p>where</p> <ul> <li><code>&lt;ES_NAME&gt;</code> is the name of the IBM Event Streams instance deployed you are trying to connect to.</li> <li><code>&lt;NAMESPACE&gt;</code> is the namespace the IBM Event Streams instance you are trying to connect to is deployed in.</li> <li><code>&lt;user.p12-location&gt;</code> is the location of the <code>user.p12</code> truststore containing the <code>user.key</code> and <code>user.crt</code> client certificate key-pair for the application or client mTLS authentication as explained above.</li> <li><code>&lt;user.p12-password&gt;</code> is the password of the <code>&lt;user.p12&gt;</code> truststore.</li> <li><code>&lt;truststore.p12-location&gt;</code> is the location of the Certificate Authority of your Kafka cluster to establish mTLS encryted communication between your IBM Event Streams instance and your application or Kafka client.</li> <li><code>&lt;trustore.p12-password&gt;</code> is the password for the <code>truststore.p12</code> truststore.</li> </ul> <p>When the application is deployed on OpenShift, certificates will be mounted to the application pod. Below is an example of a Quarkus app deployment  descriptor, with environment variables:</p> <pre><code>env:\n  - name: KAFKA_SSL_TRUSTSTORE_FILE_LOCATION\n  value: /deployments/certs/server/ca.p12\n- name: KAFKA_SSL_TRUSTSTORE_TYPE\n  value: PKCS12\n- name: KAFKA_SSL_KEYSTORE_FILE_LOCATION\n  value: /deployments/certs/user/user.p12\n- name: KAFKA_SSL_KEYSTORE_TYPE\n  value: PKCS12\n- name: KAFKA_SECURITY_PROTOCOL\n  value: SSL\n- name: KAFKA_USER\n  value: tls-user\n- name: KAFKA_CERT_PWD\n  valueFrom:\n    secretKeyRef:\n      key: ca.password\n      name: kafka-cluster-ca-cert\n- name: USER_CERT_PWD\n  valueFrom:\n    secretKeyRef:\n      key: user.password\n      name: tls-user\n# ...\n        volumeMounts:\n        - mountPath: /deployments/certs/server\n          name: kafka-cert\n          readOnly: false\n          subPath: \"\"\n        - mountPath: /deployments/certs/user\n          name: user-cert\n          readOnly: false\n          subPath: \"\"\n      volumes:\n      - name: kafka-cert\n        secret:\n          optional: true\n          secretName: kafka-cluster-ca-cert\n      - name: user-cert\n        secret:\n          optional: true\n          secretName: tls-user\n</code></pre>"},{"location":"technology/security/#external-tls","title":"External tls","text":"<p>The Kafka properties configuration to get your application or Kafka client to properly connect and communicate through the external secured (TLS encrypted) Kafka listener on port <code>9094</code> called <code>external</code>, which also enforces authentication throughout SCRAM credentials, and that is exposed through a route will be as follows:</p> <pre><code># External listener SCRAM\n# =======================\nbootstrap.servers=&lt;ES_NAME&gt;-kafka-bootstrap-&lt;NAMESPACE&gt;.&lt;OPENSHIFT_APPS_DNS&gt;\\:443\n\nsecurity.protocol=SASL_SSL\nssl.protocol=TLSv1.2\n\n## Certificate Authority of your Kafka cluster\nssl.truststore.password=&lt;trustore.p12-password&gt;\nssl.truststore.location=&lt;truststore.p12-location&gt;\nssl.truststore.type=PKCS12\n\n## Scram credentials\nsasl.mechanism=SCRAM-SHA-512\nsasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\"&lt;SCRAM_USERNAME&gt;\" password\\=\"&lt;SCRAM_PASSWORD&gt;\";\n</code></pre> <p>where</p> <ul> <li><code>&lt;ES_NAME&gt;</code> is the name of the IBM Event Streams instance deployed you are trying to connect to.</li> <li><code>&lt;NAMESPACE&gt;</code> is the namespace the IBM Event Streams instance you are trying to connect to is deployed in.</li> <li><code>&lt;OPENSHIFT_APPS_DNS&gt;</code> is your RedHat OpenShift DNS domain for application routes.</li> <li><code>&lt;truststore.p12-location&gt;</code> is the location of the Certificate Authority of your Kafka cluster to establish mTLS encryted communication between your IBM Event Streams instance and your application or Kafka client.</li> <li><code>&lt;trustore.p12-password&gt;</code> is the password for the <code>truststore.p12</code> truststore.</li> <li><code>&lt;SCRAM_USERNAME&gt;</code> and <code>&lt;SCRAM_PASSWORD&gt;</code> are your SCRAM credentials.</li> </ul>"},{"location":"technology/security/#tips","title":"Tips","text":"<p>Remember that if the application does not run in the same namespace as the kafka cluster then you need to copy the secrets so that  the application developers can access the required credentials and certificates from their own namespaces with something like</p> <pre><code>if [[ -z $(oc get secret ${TLS_USER} 2&gt; /dev/null) ]]\nthen\n   # As the project is personal to the user, we can keep a generic name for the secret\n   oc get secret ${TLS_USER} -n ${KAFKA_NS} -o json | jq -r '.metadata.name=\"tls-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n\nif [[ -z $(oc get secret ${SCRAM_USER} 2&gt; /dev/null) ]]\nthen\n    # As the project is personal to the user, we can keep a generic name for the secret\n    oc get secret ${SCRAM_USER} -n ${KAFKA_NS} -o json |  jq -r '.metadata.name=\"scram-user\"' | jq -r '.metadata.namespace=\"'${YOUR_PROJECT_NAME}'\"' | oc apply -f -\nfi\n</code></pre>"},{"location":"technology/security/#kafka-connect","title":"Kafka Connect","text":"<p>For Kafka connector, you need to define authentication used to connect to the Kafka Cluster:</p> <pre><code>  authentication: \n    type: tls\n    certificateAndKey:\n      secretName: tls-user\n      certificate: user.crt\n      key: user.key\n</code></pre> <ul> <li>Get TLS public cluster certificate:</li> </ul> <pre><code>  tls: \n    trustedCertificates:\n      - secretName: dev-cluster-ca-cert\n        certificate: ca.crt\n</code></pre>"},{"location":"technology/security/#working-with-certificates","title":"Working with certificates","text":"<p>To extract a PEM-based certificate from a JKS-based truststore, you can use the following command:</p> <pre><code>keytool -exportcert -keypass {truststore-password} -keystore {provided-kafka-truststore.jks} -rfc -file {desired-kafka-cert-output.pem}\n</code></pre> <p>To build a PKCS12 from a pem do</p> <pre><code>openssl pkcs12 -export -in cert.pem -out cert.p12\n# if you want jks\nkeytool -importkeystore -srckeystore cert.p12 -srcstoretype pkcs12 -destkeystore cert.jks\n</code></pre>"},{"location":"technology/spring/","title":"Spring Cloud and Spring Cloud Stream","text":"<p>Audience: Developers</p>"},{"location":"technology/spring/#spring-cloud","title":"Spring Cloud","text":"<p>Spring Cloud is based on Spring boot programming model but focusing on cloud native deployment and distributed computing. As other spring boot app it includes jetty or tomcat, health checks, metrics... It supports the following patterns:</p> <ul> <li>Distributed/versioned configuration: externalize config in distributed system with config server.</li> <li>Service registration and discovery: uses Netflix Eureka, Apache Zookeeper or Consul to keep service information. </li> <li>Routing: supports HTTP (Open Feign or  Netflix Ribbon for load balancing) and messaging (RabbitMQ and Kafka)</li> <li>Service-to-service calls: Sptring Cloud Gateway and Netflix Zuul is used</li> <li>Load balancing</li> <li>Circuit Breakers: based on Netflix Hystrix: if the request fails for n time, the circuit open.   </li> <li>Global locks</li> <li>Leadership election and cluster state</li> <li>Distributed messaging</li> </ul> <p>It also supports pipelines for ci/cd and contract testing for interface validation. </p>"},{"location":"technology/spring/#getting-started","title":"Getting started","text":"<p>Use start.spring.io to create the application starting code using Kafka, Actuator, Cloud Stream or add the Spring Cloud BOM to your maven <code>pom.xml</code> file. See the Adding Spring Cloud To An Existing Spring Boot Application section.</p> <p>As most of the microservices expose REST resource, we may need to add the starter web:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>We also need to install the Spring Cloud CLI.</p> <p>Then add the Spring cloud starter as dependency. When using config server, we need to add the config client. </p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-conflig-client&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>For centralized tracing uses, starter-sleuth, and zipkin.</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>For service discovery add netflix-eureka-client.</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Using the Spring Cloud CLI we can get the service registry, config server, central tracing started in one command:</p> <pre><code>spring cloud eureka configserver zipkin\n</code></pre>"},{"location":"technology/spring/#spring-cloud-config","title":"Spring Cloud config","text":"<p>Use the concept of Config Server you have a central place to manage external properties for applications across all environments.  As an application moves through the deployment pipeline from dev to test and into production you can manage the configuration between those environments and be certain that applications have everything they need to run when they migrate. </p> <pre><code>  @Value(\"${config.in.topic}\")\n  String topicName = \"orders\";\n</code></pre> <p>The value of the <code>config.in.topic</code> comes from local configuration or remote config server. The config server will serve content from a git. See this sample for such server.</p>"},{"location":"technology/spring/#spring-cloud-stream","title":"Spring Cloud Stream","text":"<p>Spring Cloud Stream is a framework for building highly scalable event-driven microservices connected with shared messaging systems. It unifies lots of popular messaging platforms behind one easy to use API including RabbitMQ, Apache Kafka, Amazon Kinesis, Google PubSub, Solace PubSub+, Azure Event Hubs, and Apache RocketMQ. </p> <p>Spring Cloud Stream is an abstraction that uses the following important concepts to supporet middleware encapsulation: destination binders (integration with messaging systems like Kafka or RabbitMQ), destination bindings (bridge code to external systems) and message (canonical data model to communicate between producer and consumer). </p> <p>As other Spring boot application, it uses extrernal properties to manage most of the configuration of the binders and binding.</p> <p>Spring Cloud Stream Applications are standalone executable applications that communicate over messaging middleware such as Apache Kafka and RabbitMQ. The app is using uber-jars to get the minimal required library and code The following diagram illustrates those concepts for a Spring cloud app:</p> <p></p> <p>Attention Spring Cloud Stream is not Kafka Streams or Kafka API, it is similar but it represents another abstraction. From a Kafka developer's point of view, it does not seem relevant, as why not using Kafka API and Kafka Streams API, but this is a way to encapsulate any middleware supporting pub/sub and queueing. It may be more comparable to Microprofile reactive messaging specifications and APIs, but not compatible with it. For example binding can be compared to channel of the microprofile reactive messaging constructs.</p> <p>So the development decision will be around middleware abstraction and the way to simplify going from one middleware to another. Now with Kafka, because of its long retention time, it means we can have any type of consumers to read the messages at any time. Those consumers may use Kafka API (Python app or nodejs apps), in this case using the Kafka API within the Spring boot application is a better approach, as the way the abstraction is used may not be fully compatible to any Kafka consumer types.</p> <p>With Kafka based application the best practice is also to define the message structure, using Avro or Protbuf, and use schema registry to ensure compatibility management between applications. To support that Spring Cloud Stream support using native (to the middleware) serialization, which in the case of Kafka could be any serdes APIs or avro API. We will cover that in later section.</p>"},{"location":"technology/spring/#example-of-kafka-binding","title":"Example of Kafka binding","text":"<p>The order service spring cloud template is a simple example of order service that exposes CRUD operations on the Order entity via a controller. Instead of writing to a database, this service immediately generates a message to Kafka and then the repository class consumes the message to get the data to write to the database. This is a simple way to implement 'transaction' by using the Append log of Kafka partition as a transaction log.</p> <p></p> <p>The way to generate code from a POST or an internal processing is to use StreamBridge, which exposes a send function to send the record.</p> <pre><code>    @Autowired\n    private StreamBridge streamBridge;\n\n    public Order processANewOrder(Order order) {\n        order.status = OrderStatus.OPEN;\n        order.orderID = UUID.randomUUID().toString();\n        order.creationDate = LocalDate.now();\n        streamBridge.send(BINDING_NAME, order);\n        return order;\n    }\n</code></pre> <p>As a good practice is to send a Kafka Record with a Key, which is specialy needed when sending messages to a multi partition topic: The messages with the same key will always go to the same partition. If the partition key is not present, messages will be partitioned in round-robin fashion. Spring Cloud Stream is little bit confusing as it created two concepts for partitioning: the partitionKey and the message key. The partition key is the way to support the same mechanism as Kafka is doing but for other middleware. So for Kafka we do not need to use partitionKey, but then it is important to use the message key construct. As Kafka is evolving on the partition allocation, it is recommended to do not interfere with Kafka mechanims and use the following approach:</p> <ul> <li> <p>Provide the message key as a SpEL expression property for example in the header: </p> <pre><code>spring.cloud.stream.bindings.&lt;binding-name&gt;.producer.message-key-expression: headers['messageKey']\n</code></pre> </li> <li> <p>Then in your application, when publishing the message, add a header called <code>kafka_messagekey</code> with the value set from the attribute to use as key. Spring Cloud Stream will use the value for this header to assign it to the Kafka record Key:</p> <pre><code> Message&lt;Order&gt; toSend = MessageBuilder.withPayload(order)\n        .setHeader(KafkaHeaders.MESSAGE_KEY, order.customerID.getBytes())\n        .setHeader(MessageHeaders.CONTENT_TYPE, MimeTypeUtils.APPLICATION_JSON).build();\n    streamBridge.send(BINDING_NAME, toSend);\n</code></pre> </li> </ul> <p>You can also build composite key with a special java bean class for that and use instance of this class as key.</p> <pre><code>```java\n CustomerCompanyKey cck = new CustomerCompanyKey(order.customerID,customer.company);\n Message&lt;Order&gt; toSend = MessageBuilder.withPayload(order)\n        .setHeader(KafkaHeaders.MESSAGE_KEY, cck)\n        .setHeader(MessageHeaders.CONTENT_TYPE, MimeTypeUtils.APPLICATION_JSON).build();\n    streamBridge.send(BINDING_NAME, toSend);\n```\n</code></pre> <p>The following screen shot illustrates that all records with the same \"customerID\" are in the same partition:</p> <p></p> <p>If you want to use the partition key as an alternate way to do partition allocation using Spring Cloud Stream strategy then use a partitionKey:</p> <pre><code>spring.cloud.stream.bindings.&lt;binding-name&gt;.producer.partition-key-expression: headers['partitionKey']\n</code></pre> <p>and then in the code:</p> <pre><code>Message&lt;Order&gt; toSend = MessageBuilder.withPayload(order)\n            .setHeader(\"partitionKey\", order.customerID.getBytes())\n            .setHeader(MessageHeaders.CONTENT_TYPE, MimeTypeUtils.APPLICATION_JSON).build();\n        streamBridge.send(BINDING_NAME, toSend);\n</code></pre>"},{"location":"technology/spring/#consuming-message","title":"Consuming message","text":"<p>With the last release of Spring Cloud Stream, consumers are single beans of type <code>Function</code>, <code>Consumer</code> or <code>Supplier</code>. Here is an example of consumer only.</p> <pre><code>    @Bean\n    public Consumer&lt;Message&lt;Order&gt;&gt; consumeOrderEvent(){\n        return msg -&gt; saveOrder(msg.getPayload());\n    }\n</code></pre> <p>For thew binding configuration the name of the method gives the name of the binding:</p> <pre><code>spring.cloud.stream:\n  bindings:\n    consumeOrderEvent-in-0:\n      destination: orders\n      contentType: application/json\n      group: orderms-grp\n      useNativeDecoding: true\n  kafka:\n    bindings:\n      consumeOrderEvent-in-0:\n        consumer:\n          ackMode: MANUAL\n          configuration:\n            value.deserializer: ibm.eda.demo.infrastructure.events.OrderDeserializer\n</code></pre> <p>The deserialization is declared in a specific class:</p> <pre><code>package ibm.eda.demo.infrastructure.events;\n\nimport org.springframework.kafka.support.serializer.JsonDeserializer;\n\npublic class OrderDeserializer extends JsonDeserializer&lt;Order&gt; {\n\n}\n</code></pre> <p>In this example above as the goal is to save to the database, we should not auto commit the offset reading. So the following settings are needed on the consumer side:</p> <pre><code>spring.cloud.stream.kafka:\n    bindings:\n      consumeOrderEvent-in-0:\n        consumer:\n          autoCommitOffset: false\n          startOffset: latest\n          ackMode: MANUAL\n</code></pre> <p>And the consumer code is now looking at the acknowledge header property if present or not and perform manual acknowledge once the save operation is successful.</p> <pre><code> @Bean\n    public Consumer&lt;Message&lt;Order&gt;&gt; consumeOrderEvent(){\n        return msg -&gt; {\n            Acknowledgment acknowledgment = msg.getHeaders().get(KafkaHeaders.ACKNOWLEDGMENT, Acknowledgment.class);\n            saveOrder(msg.getPayload());\n            if (acknowledgment != null) {\n                acknowledgment.acknowledge();\n            }\n        };\n</code></pre>"},{"location":"technology/spring/#kafka-spring-cloud-stream-app-basic","title":"Kafka spring cloud stream app basic","text":"<p>The approach to develop such application includes the following steps:</p> <ul> <li>A spring boot application, with REST spring web starter</li> <li>Define a resource and a controller for the REST API.</li> <li>Define inbound and/or outbound binding to communicate to underlying middleware</li> <li>Add method to process incoming message, taking into account the underlying middleware and serialization. For example with Kafka, most of the consumers may not auto commit the read offset but control the commit by using manual commit. </li> <li>Add logic to produce message using middleware </li> </ul> <p>To add a consumer from a Kafka topic for example, we can add a function that will process the message, and declare it as a Bean. </p> <p><pre><code> @Bean\n    public Consumer&lt;Message&lt;CloudEvent&gt;&gt; consumeCloudEventEvent(){\n        return msg -&gt; {\n            Acknowledgment acknowledgment = msg.getHeaders().get(KafkaHeaders.ACKNOWLEDGMENT, Acknowledgment.class);\n            saveOrder((Order)msg.getPayload().getData());\n            if (acknowledgment != null) {\n                System.out.println(\"Acknowledgment provided\");\n                acknowledgment.acknowledge();\n            }\n        };\n    }\n</code></pre> This previous code is also illustrating manual offset commit.</p> <p>Then we add configuration to link to the binders queue or topic:</p> <pre><code>    consumeOrderEvent-in-0:\n        consumer:\n          autoCommitOffset: false\n          startOffset: latest\n          ackMode: MANUAL\n          configuration:\n            value.deserializer: ibm.eda.demo.infrastructure.events.CloudEventDeserializer\n</code></pre>"},{"location":"technology/spring/#avro-serialization","title":"Avro serialization","text":"<pre><code>producer:\n        useNativeEncoding: true\n</code></pre>"},{"location":"use-cases/connect-cos/","title":"Kafka Connect to IBM COS","text":"<p>Info</p> <p>Updated 02/22/2022</p>"},{"location":"use-cases/connect-cos/#introduction","title":"Introduction","text":"<p>One of the classical use case for event driven solution based on Kafka is to keep the message for a longer time period than the retention time of the Kafka topic. This demonstration illustrates how to do so using IBM Event Streams and IBM Cloud Object Storage as a service.</p> <p>We have created a very simple Quarkus (a super sonic and sub-atomic Kubernetes native framework for Java) application that  uses MicroProfile Reactive Messaging in order to send a stream of data to our Event Streams/Kafka topic. </p> <p>The Kafka Connect cluster includes a IBM Cloud Object Storage Connector to grab messages fomr the topoc and  place into an IBM COS Bucket.</p> <p></p>"},{"location":"use-cases/connect-cos/#skill-level","title":"Skill level","text":"<p>The skill level of this learning path is for a beginner.</p>"},{"location":"use-cases/connect-cos/#estimated-time-to-complete","title":"Estimated time to complete","text":"<p>It will take you approximately 15 minuttes to complete this entire demonstration.</p>"},{"location":"use-cases/connect-cos/#scenario-prerequisites","title":"Scenario Prerequisites","text":"<p>OpenShift Container Platform Cluster   - This scenario will assume you have a 4.7+ Cluster as we will make use of Operators.</p> <p>Cloud Pak for Integration   - Updated with 2021.0.3 release of the Cloud Pak for Integration installed on OpenShift.  This story will also assume you have followed the installation instructions for Event Streams  outlined in the 2021-3 product documentation or used one of our GitOps repository approach  to deploy Event Streams Operator.</p> <p>Git   - We will need to clone repositories.</p> <p>An IBM Cloud Account (free)   - A free (Lite) IBM Cloud Object Storage trial Service account IBM Cloud Object Storage</p> <p>If you want to modify the code then you need:</p> <p>Java   - Java Development Kit (JDK) v1.11+ (Java 11+)</p> <p>Maven   - The scenario uses Maven v3.6.3</p> <p>Gradle   - Ideally v4.0+ (Note - the gradle shadowJar command might not work on Java versions newer to Java 8)</p> <p>An IDE of your choice   - Visual Studio Code is used in this scenario.</p>"},{"location":"use-cases/connect-cos/#use-case-guided-tour","title":"Use Case Guided Tour","text":"<p>The use case is quite simple, but very common. Topic has configuration to keep messages in the kafka broker disks for a certain time perior or until the log reach a certain size. Which means records will be removed from the brokers over time. Some companies put retention time to a value to keep data foreever. This is possible when the amount of data in such topic is not reasonable. But most of the time we need to design solution to move data to longer persistence. S3 buckets is one of such long term persistence as it will bring elasticity, transparent replication and geolocalization. </p> <p>An infrastructure engineer needs to prepare the Cloud Object Storage service and organize the bucket strategy to keep data. Then on the OpenShift production cluster, Event Streams is deployed and Kafka connector cluster defined. The connector configuration will stipulate the COS connection credentials and the Event Streams connection URL and credentials.</p> <p>Once the connector is started all messages in the designated topics will go to the COS bucket. </p> <p>IBM Event Streams team has explanation of the Cloud Object Storage Sink Kafka connector configuration in this repository.</p>"},{"location":"use-cases/connect-cos/#full-demo-narration","title":"Full Demo Narration","text":""},{"location":"use-cases/connect-cos/#1-create-an-ibm-cos-service-and-cos-bucket","title":"1- Create an IBM COS Service and COS Bucket","text":"<p>In this section, we are going to see how to create an IBM Cloud Obeject Storage (IBM COS) Service in your IBM Cloud account and a bucket within your  IBM COS Service. We assume you already have an IBM Cloud account and, if not, you can sign up for one here at IBM Cloud.</p> <ol> <li> <p>Once you are inside your IBM Cloud account, traverse to the <code>Catalog</code> section. In the search type in <code>IBM Cloud Object Storage</code></p> <p></p> </li> <li> <p>Name your IBM COS Service with something unique. Since this is a free account, we can stick with the <code>Lite Plan</code>.</p> <p></p> </li> <li> <p>Now that the IBM Cloud Object Storage Service is created, create a new bucket. On the <code>Create Bucket</code> screen pick <code>Custom Bucket</code>.</p> <p></p> </li> <li> <p>When selecting options for the bucket, name your bucket something unique. For <code>Resiliency</code> let's select <code>Regional</code>.  For location select an area from the drop-down that you want. Use <code>Standard</code> for <code>Storage Class</code>.  Leave everything else as-is and hit <code>Create Bucket</code>.</p> <p></p> </li> </ol>"},{"location":"use-cases/connect-cos/#2-create-ibm-cos-service-credentials","title":"2- Create IBM COS Service Credentials","text":"<p>Now that we have created our IBM Cloud Object Storage Service and bucket, we need to create the Service Credential so that Kafka connector can connect to it.</p> <ol> <li> <p>Inside your IBM COS Service, select <code>Service Credentials</code> and then click the <code>New Credential</code> button.</p> <p></p> </li> <li> <p>Name your credential and select <code>Manager</code> from the <code>Role:</code> drop-down menu and click <code>Add</code>.</p> <p></p> </li> <li> <p>Expand your newly created Service Credential and write down the values for <code>\"apikey\"</code> and <code>\"resource_instance_id\"</code>.  You will need this later in the Build and Apply IBM COS Sink Connector section.</p> <p></p> </li> </ol>"},{"location":"use-cases/connect-cos/#3-create-a-demo-project","title":"3- Create a demo project","text":"<p>To isolate the demonstration in the OpenShift Cluster, we will deploy the demo code, event streams cluster and Kafka Connect in one project.</p> <ol> <li> <p>Clone the tutorial project</p> <pre><code>git clone https://github.com/ibm-cloud-architecture/eda-quickstarts\n</code></pre> </li> <li> <p>Create an OpenShift project (k8s namespace) named: <code>eda-cos</code></p> <pre><code>oc apply -k gitOps/env/base\n</code></pre> </li> </ol>"},{"location":"use-cases/connect-cos/#4-deploy-event-streams-cluster","title":"4- Deploy Event Streams Cluster","text":"<p>If you do not have a Event Stream cluster already deployed on OpenShift, we propose to deploy one in a demo project, using Event Streams Operator monitoring All Namespaces</p> <ol> <li> <p>Using IBM entitled registry entitlement key   define your a secret so deployment process can download IBM Event Streams images:</p> <pre><code>KEY=&lt;yourentitlementkey&gt;\noc create secret docker-registry ibm-entitlement-key \\\n    --docker-username=cp \\\n    --docker-password=$KEY \\\n    --docker-server=cp.icr.io \\\n    --namespace=eda-cos\n</code></pre> </li> <li> <p>Deploy Event Streams Cluster.</p> <pre><code>oc apply -k gitOps/services/es \n# --&gt; Results\neventstreams.eventstreams.ibm.com/dev created\nkafkatopic.eventstreams.ibm.com/edademo-orders created\n</code></pre> <p>It will take sometime to get the cluster created. Monitor with <code>oc get pod -w</code>. You should get:</p> <pre><code>dev-entity-operator-6d7d94f68f-6lk86   3/3    \ndev-ibm-es-admapi-ffd89fdf-x99lq       1/1    \ndev-ibm-es-ui-74bf84dc67-qx9kk         2/2    \ndev-kafka-0                            1/1    \ndev-zookeeper-0                        1/1\n</code></pre> <p>With this deployment there is no external route, only on bootstrap URL: <code>dev-kafka-bootstrap.eda-cos.svc:9092</code>. The Kafka listener is using PLAINTEXT connection. So no SSL encryption and no authentication.</p> </li> <li> <p>Deploy the existing application (the image we built is in quay.io/ibmcase) using:</p> <pre><code>oc apply -k gitOps/apps/eda-cos-demo/base/\n</code></pre> </li> <li> <p>Test the deployed app by accessing its route and GET API: </p> <pre><code>HOST=$(oc get routes eda-cos-demo  -o jsonpath='{.spec.host}')\ncurl -X GET http://$HOST/api/v1/version\n</code></pre> </li> </ol>"},{"location":"use-cases/connect-cos/#5-deploy-kafka-connector","title":"5- Deploy Kafka Connector","text":"<p>We have already built a kafka connect image with the Cloud Object Storage jar and push it as image to quay.io.</p> <ol> <li>Deploy the connector cluster by using the KafkaConnect custom resource: </li> </ol> <pre><code>oc apply -f gitOps/services/kconnect/kafka-connect.yaml\n# Verify cluster is ready\noc get kafkaconnect\n</code></pre>"},{"location":"use-cases/connect-cos/#6-deploy-the-cos-sink-connector","title":"6- Deploy the COS sink connector","text":"<ol> <li> <p>Create a new file named <code>kafka-cos-sink-connector.yaml</code> and past the following code in it.</p> <pre><code>apiVersion: eventstreams.ibm.com/v1alpha1\nkind: KafkaConnector\nmetadata:\n  name: cos-sink-connector\n  labels:\n    eventstreams.ibm.com/cluster: YOUR_KAFKA_CONNECT_CLUSTER_NAME\nspec:\n  class: com.ibm.eventstreams.connect.cossink.COSSinkConnector\n  tasksMax: 1\n  config:\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.kafka.connect.storage.StringConverter\n    topics: TOPIC_NAME\n    cos.api.key: IBM_COS_API_KEY\n    cos.bucket.location: IBM_COS_BUCKET_LOCATION\n    cos.bucket.name: IBM_COS_BUCKET_NAME\n    cos.bucket.resiliency: IBM_COS_RESILIENCY\n    cos.service.crn: \"IBM_COS_CRM\"\n    cos.object.records: 5\n    cos.object.deadline.seconds: 5\n    cos.object.interval.seconds: 5\n</code></pre> <p>where:</p> <ul> <li><code>YOUR_KAFKA_CONNECT_CLUSTER_NAME</code>: is the name you gave previously to your Kakfa Connect cluster.</li> <li><code>TOPIC_NAME</code>: is the name of the topic you created in IBM Event Streams at the beginning of this lab.</li> <li><code>IBM_COS_API_KEY</code>: is your IBM Cloud Object Storage service credentials <code>apikey</code> value. Review first sections of this lab if you don't remember where and how to find this value.</li> <li><code>IBM_COS_BUCKET_LOCATION</code>: is your IBM Cloud Object Storage bucket location. Review first sections of this lab if you don't remember where and how to find this value (it usually is in the form of something like <code>us-east</code> or <code>eu-gb</code> for example).</li> <li><code>IBM_COS_RESILIENCY</code>: is your IBM Cloud Object Storage resiliency option. Review first sections of this lab if you don't remember where and how to find this value (it should be <code>regional</code>).</li> <li><code>IBM_COS_CRM</code>: is your IBM Cloud Object Storage CRN. Review first sections of this lab if you don't remember where and how to find this value. It usually ends with a double <code>::</code> at the end of it. IMPORTANT: you might need to retain the double quotation marks here as the CRN has colons in it and may collide with yaml syntax.</li> </ul> </li> <li> <p>Apply the yaml which will create a <code>KafkaConnnector</code> custom resource behind the scenes and register/set up the IBM COS Sink Connector in your Kafka Connect cluster.</p> <pre><code>oc apply -f kafka-cos-sink-connector.yaml\n</code></pre> </li> <li> <p>The initialization of the connector can take a minute or two. You can check the status of the connector to see if everything connected succesfully.</p> <pre><code>oc describe kafkaconnector cos-sink-connector\n</code></pre> </li> <li> <p>When the IBM COS Sink connector is successfully up and running you should see something similar to the below.</p> <p></p> </li> <li> <p>You should also see a new connector being registered if you exec into the Kafka Connect cluster pod and query for the existing connectors again:</p> <pre><code>oc exec -it YOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx bash\nbash-4.4$ curl localhost:8083/connectors\n[\"cos-sink-connector\"]\n</code></pre> </li> </ol>"},{"location":"use-cases/connect-cos/#7-test-by-sending-some-records","title":"7- Test by sending some records","text":"<ol> <li> <p>Post 10 orders:</p> <p><code>sh curl -X POST  'http://$HOST/api/v1/start'   -H 'accept: application/json' -H 'Content-Type: application/json' -d '10'</code></p> </li> <li> <p>Verify whether the messages from the IBM Event Streams topic are getting propagated to your IBM Cloud Object Storage bucket.  If you go to your IBM COS bucket, you should find some files in it. The name of the file inside the bucket has starting offset and ending offset.  You can download one of these object files to make sure that the value inside matches the value inside your <code>INBOUND</code> topic.</p> <p></p> </li> </ol>"},{"location":"use-cases/connect-cos/#developer-corner","title":"Developer Corner","text":"<p>The https://github.com/ibm-cloud-architecture/eda-quickstarts.git cos-tutorial folder includes a microprofile reactive messaging producer app basd on quarkus runtime.</p> <p>You have two choices for this application, reuse the existing code as-is or start by using quarkus CLI.</p> <ul> <li>Clone the <code>eda-quickstarts</code> repository.</li> </ul> <pre><code>git clone  https://github.com/ibm-cloud-architecture/eda-quickstarts.git\ncd cos-tutorial\nquarkus dev\n</code></pre> <ul> <li>If you want to start on your own, then create the Quarkus project.</li> </ul> <pre><code>quarkus create app cos-tutorial \ncd cos-tutorial \nquarkus ext add  reactive-messaging-kafka, mutiny, openshift\n</code></pre> <p>and get inspiration from the DemoController.java.</p> <ul> <li> <p>Review the <code>DemoController.java</code> file.</p> <ul> <li>The <code>@Channel</code> annotation indicates that we're sending to a channel defined with reactive messaging in the application.properties.</li> <li>The <code>startDemo()</code> function generate n orders and send to the channel.</li> </ul> </li> <li> <p>The reactive messaging is defined in the <code>application.properties</code>. The setting are using the a kafka cluster named <code>dev</code>, we encourage to keep this name or you need to modify a lot of yaml files.</p> </li> <li>See the COS tutorial README  to run the producer application locally with docker compose or quarkus dev.</li> </ul>"},{"location":"use-cases/connect-cos/older-section/","title":"Older section","text":""},{"location":"use-cases/connect-cos/older-section/#set-up-the-kafka-connect-cluster","title":"Set up the Kafka Connect Cluster","text":"<p>In this section, we are going to see how to deploy a Kafka Connect cluster on OpenShift which will be the engine running the source and sink connector we decide to use for our use case. IMPORTANT: We assume you have deployed your IBM Event Streams instance with an internal TLS secured listener which your Kafka Connect cluster will use to connect. For more detail about listeners, check the IBM Event Streams documentation here.</p> <p>If you inspect your IBM Event Streams instance by executing the following command:</p> <pre><code>oc get EventStreams YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME -o yaml\n</code></pre> <p>You should see a <code>TlS</code> listener:</p> <p></p> <p>Now, follow the next steps in order to get your Kafka Connect cluster deployed:</p> <ol> <li>Go to you IBM Event Streams dashboard, click on the <code>Find more on the toolbox</code> option.</li> </ol> <p></p> <ol> <li>Click on the <code>Set up</code> button for the <code>Set up a Kafka Connect environment</code> option.</li> </ol> <p></p> <ol> <li>Click on <code>Download Kafka Connect ZIP</code> button.</li> </ol> <p></p> <ol> <li>The above downloads a zip file which contains a <code>kafka-connect-s2i.yaml</code> file. Open that yaml file and take note of the <code>productID</code> and <code>cloudpakId</code> values as you will need these in the following step.</li> </ol> <p></p> <ol> <li>Instead of using the previous yaml file, create a new <code>kafka-connect-s2i.yaml</code> file with the following contents:</li> </ol> <pre><code>apiVersion: eventstreams.ibm.com/v1beta1\nkind: KafkaConnectS2I\nmetadata:\n  name: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n  annotations:\n    eventstreams.ibm.com/use-connector-resources: \"true\"\nspec:\n  logging:\n    type: external\n    name: custom-connect-log4j\n  version: 2.6.0\n  replicas: 1\n  bootstrapServers: YOUR_INTERNAL_BOOTSTRAP_ADDRESS\n  template:\n    pod:\n      imagePullSecrets: []\n      metadata:\n        annotations:\n          eventstreams.production.type: CloudPakForIntegrationNonProduction\n          productID: YOUR_PRODUCT_ID\n          productName: IBM Event Streams for Non Production\n          productVersion: 10.2.0\n          productMetric: VIRTUAL_PROCESSOR_CORE\n          productChargedContainers: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n          cloudpakId: YOUR_CLOUDPAK_ID\n          cloudpakName: IBM Cloud Pak for Integration\n          cloudpakVersion: 2020.4.1\n          productCloudpakRatio: \"2:1\"\n  tls:\n      trustedCertificates:\n        - secretName: YOUR_CLUSTER_TLS_CERTIFICATE_SECRET\n          certificate: ca.crt\n  authentication:\n    type: tls\n    certificateAndKey:\n      certificate: user.crt\n      key: user.key\n      secretName: YOUR_TLS_CREDENTIALS_SECRET\n  config:\n    group.id: YOUR_KAFKA_CONNECT_CLUSTER_NAME\n    key.converter: org.apache.kafka.connect.json.JsonConverter\n    value.converter: org.apache.kafka.connect.json.JsonConverter\n    key.converter.schemas.enable: false\n    value.converter.schemas.enable: false\n    offset.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-offsets\n    config.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-configs\n    status.storage.topic: YOUR_KAFKA_CONNECT_CLUSTER_NAME-status\n    config.storage.replication.factor: 1\n    offset.storage.replication.factor: 1\n    status.storage.replication.factor: 1\n</code></pre> <p>where you will need to replace the following placeholders with the appropriate values for you IBM Event Streams cluster and service credentials:    * <code>YOUR_KAFKA_CONNECT_CLUSTER_NAME</code>: A name you want to provide your Kafka Connect cluster and resources with.    * <code>YOUR_INTERNAL_BOOTSTRAP_ADDRESS</code>: This is the internal bootstrap address of your IBM Event Streams instance. You can review how to find this url here. Use the internal bootstrap address which should be in the form of <code>YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-kafka-bootstrap.eventstreams.svc:9093</code>:        * <code>YOUR_TLS_CREDENTIALS_SECRET</code>: This is the name you give to your TLS credentials for your internal IBM Event Streams listener when you click on <code>Generate TLS credentials</code>:        * <code>YOUR_CLUSTER_TLS_CERTIFICATE_SECRET</code>: This is the secret name where IBM Event Streams stores the TLS certificate for establishing secure communications. This secret name is in the form of <code>YOUR_IBM_EVENT_STREAMS_INSTANCE_NAME-cluster-ca-cert</code>. You can always use <code>oc get secrets</code> to list all the secrets.    * <code>YOUR_PRODUCT_ID</code>: This is the <code>productID</code> value you noted down earlier.    * <code>YOUR_CLOUDPAK_ID</code>: This is the <code>cloudpakID</code> value you noted earlier.  </p> <ol> <li>Deploy your Kafka Connect cluster by executing</li> </ol> <pre><code>oc apply -f kafkaconnect-s2i.yaml \n</code></pre> <ol> <li>If you list the pods, you should see three new pods: one for the Kafka Connect build task, another for the Kafka Connect deploy task and the actual Kafka Connect cluster pod.</li> </ol> <pre><code>oc get pods\n\nNAME                                                 READY   STATUS      RESTARTS   AGE\nYOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build      0/1     Completed   0          18m\nYOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy     0/1     Completed   0          17m\nYOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-xxxxx      1/1     Running     0          17m\n</code></pre> <p>## Build and Inject IBM COS Sink Connector</p> <p>The IBM COS Sink Connector source code is availabe at this repository here.</p> <p>IMPORTANT: Make sure you have Java 8 installed on your workstation and that is the default Java version of your system since the IBM COS Sink Connector can only be built with that version of Java.</p> <ol> <li>Clone the Kafka Connect IBM COS Source Connector repository and then change your folder.</li> </ol> <pre><code>git clone https://github.com/ibm-messaging/kafka-connect-ibmcos-sink.git\ncd kafka-connect-ibmcos-sink/\n</code></pre> <p>IMPORTANT Part Two: Depending on your Gradle version you have installed on your machine you will need to update the connector's gradle build file. For example Gradle v7.x the <code>build.gradle</code> file should look something like this as the <code>compile()</code> method is deprecated in newer versions. You can downgrade your Gradle version if you so choose. This is the shadowJar repository for versioning information.</p> <pre><code>/*\n * Copyright 2019 IBM Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n */\nplugins {\n  id 'com.github.johnrengelman.shadow' version '4.0.3'\n  id 'java'\n  id 'eclipse'\n}\n\nrepositories {\n   mavenCentral()\n}\n\ndependencies {\n  implementation 'com.eclipsesource.minimal-json:minimal-json:0.9.5'\n  implementation 'com.ibm.cos:ibm-cos-java-sdk:2.4.5'\n  implementation 'org.apache.kafka:connect-api:2.2.1'\n\n  testImplementation 'org.mockito:mockito-all:1.10.19'\n  testImplementation 'junit:junit:4.12'\n  testImplementation 'com.squareup.okhttp3:mockwebserver:3.9.+'\n}\n\neclipse.project {\n  natures 'org.springsource.ide.eclipse.gradle.core.nature'\n}\n\nshadowJar {\n   classifier = 'all'\n   version = '7.0.0'\n}\n</code></pre> <ol> <li>Build the connect using <code>Gradle</code>.</li> </ol> <pre><code>gradle shadowJar\n</code></pre> <ol> <li>The newly built connector binaries are in the <code>build/libs/</code> folder. Move it into a <code>connectors</code> folder for ease of use.</li> </ol> <pre><code>mkdir connectors\ncp build/libs/kafka-connect-ibmcos-sink-*-all.jar connectors/\n</code></pre> <ol> <li>Now that we have the connector in the <code>connectors/</code> folder, we somehow need embed it into our Kakfa Connect cluster. For that, we need to trigger another build for our Kafka Connect cluster but this time specifying the files we want to get embedded. What the followin command does is it builds a new image with your provided connectors/plugins and triggers a new deployment for your Kafka Connect cluster.</li> </ol> <pre><code>oc start-build connect-cluster-101-connect --from-dir ./connectors/ --follow\n</code></pre> <ol> <li>Since the last commands triggers a new build, we should now see three new pods for the build task, the deploy task and the resulting Kafka Connect cluster. Also, we should see the previous Kafka Connect cluster pod if gone.</li> </ol> <pre><code>oc get pods\n\nNAME                                                 READY   STATUS      RESTARTS   AGE\nYOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-build      0/1     Completed   0          31m\nYOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-1-deploy     0/1     Completed   0          31m\nYOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-build      0/1     Completed   0          18m\nYOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-deploy     0/1     Completed   0          17m\nYOUR_KAFKA_CONNECT_CLUSTER_NAME-connect-2-xxxxx      1/1     Running     0          17m\n</code></pre>"},{"location":"use-cases/connect-cos/older-section/#connect-with-mtls","title":"Connect with mTLS","text":"<p>Update the <code>applications.properties</code> file if you are not connecting to Event Streams via PLAINTEXT security protocol.</p> <pre><code>quarkus.http.port=8080\nquarkus.log.console.enable=true\nquarkus.log.console.level=INFO\n\n# Event Streams Connection details\nmp.messaging.connector.smallrye-kafka.bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\nmp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\nmp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\nmp.messaging.connector.smallrye-kafka.sasl.mechanism=SCRAM-SHA-512\nmp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                username=REPLACE_WITH_YOUR_SCRAM_USERNAME \\\n                password=REPLACE_WITH_YOUR_SCRAM_PASSWORD;\nmp.messaging.connector.smallrye-kafka.ssl.truststore.location=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION\nmp.messaging.connector.smallrye-kafka.ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\n\n# Initial mock JSON message producer configuration\nmp.messaging.outgoing.INBOUND.connector=smallrye-kafka\nmp.messaging.outgoing.INBOUND.topic=REPLACE_WITH_YOUR_TOPIC\nmp.messaging.outgoing.INBOUND.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\nmp.messaging.outgoing.INBOUND.key.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n</code></pre> <pre><code>* `REPLACE_WITH_YOUR_BOOTSTRAP_URL`: Your IBM Event Streams bootstrap url.\n* `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_LOCATION`: The location where you downloaded your PCKS12 TLS certificate to.\n* `REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD`: Your PCKS12 TLS certificate password.\n* `REPLACE_WITH_YOUR_SCRAM_USERNAME`: Your SCRAM service credentials username.\n* `REPLACE_WITH_YOUR_SCRAM_PASSWORD`: Your SCRAM service credentials password.\n* `REPLACE_WITH_YOUR_TOPIC`: Name of the topic you created above.\n</code></pre> <p>Review the Common pre-requisites instructions if you don't know how to find out any of the config properties above. </p>"},{"location":"use-cases/connect-jdbc/","title":"Kafka Connect to JDBC Sink","text":"<p>Info</p> <p>Updated 11/03/2020. Lab works with one small issue needs to be fixed in test or jdbc connector.</p> <p>This scenario is using the IBM Kafka Connect sink connector for JDBC to get data from a kafka topic and write records to the <code>inventory</code> table in DB2. This lab explain the definition of the connector and how to run an integration test that sends data to the inventory topic.</p>"},{"location":"use-cases/connect-jdbc/#pre-requisites","title":"Pre-requisites","text":"<p>Pull in necessary pre-req context from Realtime Inventory Pre-reqs.</p> <p>As a pre-requisite you need to have a DB2 instance on cloud up and running with defined credentials. From the credentials you need the username, password and the <code>ssljdbcurl</code> parameter. Something like \"jdbc:db2://dashdb-tx....net:50001/BLUDB:sslConnection=true;\".</p> <ol> <li> <p>Build and deploy the <code>inventory-app</code>. This application is a simple Java microprofile 3.3 app exposing a set of end points for CRUD operations on stores, items and inventory. It is based on Quarkus. The instructions to build, and deploy this app is in the README. At the application starts, stores and items records are uploaded to the database.</p> </li> <li> <p>Verify the stores and items records are loaded</p> </li> <li> <p>If you deploy the <code>inventory-app</code> from previous step, then you will have the database created and populated with some stores and items automatically. If you want to drop the data use the drop sql script and then reload them the insert sql script from <code>src/main/resources</code> folder. For that you can use the <code>Run sql</code> menu in the DB2 console:</p> </li> </ol> <p></p> <p>Select the database schema matching the username used as credential, and then open the SQL editor:</p> <p></p> <p>Verify the items with <code>select * from items;</code></p> <p></p> <p>Verify the stores with <code>select * from stores;</code></p> <p></p> <p>The inventory has one record to illustrate the relationship between store, item and inventory.</p>"},{"location":"use-cases/connect-jdbc/#run-the-kafka-connector-in-distributed-mode","title":"Run the Kafka Connector in distributed mode","text":"<p>In the refarch-eda-tools repository the <code>labs/jdbc-sink-lab</code> folder includes a docker compose file to run the lab with kafka broker, zookeeper, the kafka connector running in distrbuted mode and an inventory app to get records from DB.</p> <pre><code># \ndocker-compose up -d\n</code></pre>"},{"location":"use-cases/connect-jdbc/#upload-the-db2-sink-definition","title":"Upload the DB2 sink definition","text":"<p>Update the file <code>db2-sink-config.json</code> with the DB2 server URL, DB2 username and password. The DB schema matches the user name, so update this setting for the <code>table.name.format</code> with the username.</p> <pre><code>  \"name\": \"jdbc-sink-connector\",\n  \"config\": {\n    \"connector.class\": \"com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\",\n    \"tasks.max\": \"1\",\n    \"topics\": \"inventory\",\n    \"connection.url\": \"jdbc:db2://....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\",\n    \"connection.user\": \"&lt;username&gt;\",\n    \"connection.password\": \"&lt;password&gt;\",\n    \"connection.ds.pool.size\": \"1\",\n    \"insert.mode.databaselevel\": \"true\",\n    \"table.name.format\": \"&lt;username&gt;.INVENTORY\"\n  }\n</code></pre> <p>Once done, you can run the <code>./sendJdbcSinkConfig.sh &lt;url-kafka-connect&gt;</code> to upload the above definition to the Kafka connect controller. When running locally the command is <code>./sendJdbcSinkConfig.sh localhost:8083</code>. This script delete previously define connector with the same name, and then perform a POST operation on the <code>/connectors</code> end point.</p> <p>The connector trace should have something like:</p> <pre><code>connector.class = com.ibm.eventstreams.connect.jdbcsink.JDBCSinkConnector\n    errors.log.enable = false\n    errors.log.include.messages = false\n    errors.retry.delay.max.ms = 60000\n    errors.retry.timeout = 0\n    errors.tolerance = none\n    header.converter = null\n    key.converter = null\n    name = jdbc-sink-connector\n    tasks.max = 1\n    transforms = []\n    value.converter = null\n</code></pre>"},{"location":"use-cases/connect-jdbc/#generate-some-records","title":"Generate some records","text":"<p>The <code>integration-tests</code> folder includes a set of python code to load some records to the expected topic.</p> <ol> <li>Start a python environment with <code>./startPython.sh</code></li> <li> <p>Within the bash, start python to execute the  <code>ProduceInventoryEvent.py</code> script, and specify the number of records to send via the --size argument.</p> <pre><code>python ProduceInventoryEvent.py --size 2  \n</code></pre> </li> <li> <p>The trace should have something like</p> <pre><code>Produce to the topic inventory\n\nending -&gt; {'schema': {'type': 'struct', 'fields': [{'type': 'string', 'optional': False, 'field': 'storeName'}, {'type': 'string', 'optional': False, 'field': 'sku'}, {'type': 'decimal', 'optional': False, 'field': 'id'}, {'type': 'decimal', 'optional': True, 'field': 'quantity'}, {'type': 'decimal', 'optional': True, 'field': 'price'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}], 'optional': False, 'name': 'Inventory'}, 'payload': {'storeName': 'Store_1', 'sku': 'Item_1', 'quantity': 16, 'price': 128, 'id': 0, 'timestamp': '05-Nov-2020 22:31:11'}}\nsending -&gt; {'schema': {'type': 'struct', 'fields': [{'type': 'string', 'optional': False, 'field': 'storeName'}, {'type': 'string', 'optional': False, 'field': 'sku'}, {'type': 'decimal', 'optional': False, 'field': 'id'}, {'type': 'decimal', 'optional': True, 'field': 'quantity'}, {'type': 'decimal', 'optional': True, 'field': 'price'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}], 'optional': False, 'name': 'Inventory'}, 'payload': {'storeName': 'Store_1', 'sku': 'Item_8', 'quantity': 13, 'price': 38, 'id': 1, 'timestamp': '05-Nov-2020 22:31:11'}}\n</code></pre> </li> </ol>"},{"location":"use-cases/connect-jdbc/#verify-records-are-uploaded-into-the-inventory-database","title":"Verify records are uploaded into the Inventory database","text":"<p>You can use two approaches to get the database, by using the inventory app or using the DB2 console, use the <code>select * from inventory;</code> SQL query to get the last records.</p> <p>The swagger is visible at the address http://localhost:8080/swagger-ui and get to the URL http://localhost:8080/inventory</p>"},{"location":"use-cases/connect-mq/","title":"Kafka to IBM MQ with Kafka Connector","text":"<p>This extended scenario supports different labs going from simple to more complex and  addresses how to integrate IBM MQ with Event Streams Kafka as part of Cloud Pak for Integration using Kafka Connect with IBM MQ Kafka Connectors. </p> <p>To run the same Lab with Kafka Confluent see the ibm-cloud-architecture/eda-lab-mq-to-kafka readme file.</p> <p> Pre-requisites to all labs Lab 1: MQ source to Event Streams Using the Admin Console Lab 2: MQ source to Event Streams using GitOps Lab 3: MQ Sink from Kafka Lab 4: MQ Connector with Kafka Confluent </p>"},{"location":"use-cases/connect-mq/#audience","title":"Audience","text":"<p>We assume readers have good knowledge of OpenShift to login, to naivgate into the Administrator console and use OC and GIT CLIs.</p>"},{"location":"use-cases/connect-mq/#pre-requisites-to-all-labs","title":"Pre-requisites to all labs","text":"<ul> <li>Access to an OpenShift Cluster and Console</li> <li>Login to the OpenShift Console and get the <code>access token</code> to use <code>oc login</code>.</li> <li>Access to git cli from your workstation</li> <li>Clone Lab repository</li> </ul> <pre><code>git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka.git\n</code></pre>"},{"location":"use-cases/connect-mq/#lab-1-mq-source-to-event-streams-using-the-admin-console","title":"Lab 1: MQ source to Event Streams Using the Admin Console","text":"<p>In this lab we assume Cloud Pak for Integration is deployed with Event Streams cluster created and at least one MQ Broker created. If not see this note to install Event Streams operator and defining one Kafka cluster using the OpenShift Console and this note to install one MQ Operator and one MQ Broker. </p> <p>The following figure illustrates what we will deploy:</p> <p></p> <ul> <li>The Sell Store simulator is a separate application available in the refarch-eda-store-simulator public github and  the docker image is accessible from the quay.io registry.</li> <li>The Event Streams cluster is named <code>dev</code></li> </ul>"},{"location":"use-cases/connect-mq/#get-setup","title":"Get setup","text":"<ul> <li>Get Cloud Pak for Integration <code>admin</code> user password</li> </ul> <p><pre><code>oc get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' -n ibm-common-services | base64 --decode &amp;&amp; echo \"\"\n</code></pre> * Access to the Event Streams console</p> <pre><code>oc get routes\n# \noc get route dev-ibm-es-ui -o jsonpath='{.spec.host}' &amp;&amp; echo \"\"\n</code></pre> <p>Use this URL to access to the Event Stream console:</p> <p></p> <ul> <li>Using the Topic menu, create the <code>items</code> topic with 1 partition, use default retention time, and 3 replicas</li> <li>Get Internal Kafka bootstrap URL</li> </ul> <p></p> <p>and generate TLS credentials with the  name <code>tls-mq-user</code> with the <code>Produce messages, consume messages and create topics and schemas</code> permissions, </p> <p></p> <p>on <code>items</code> topic:</p> <p></p> <p>All consumer groups, as we may reuse this user for consumers.</p> <p>You can download the certificates, but in fact it is not necessary as we will use the created secrets to configure MQ Kafka connector.</p> <ul> <li>Get MQ Console route from the namespace where MQ brokers run</li> </ul> <pre><code>oc get routes -n cp4i | grep mq-web\n</code></pre> <ul> <li>Access to MQ Broker Console &gt; Manage to see the Broker configured </li> </ul> <p></p> <ul> <li>Add the local <code>items</code> queue </li> </ul> <p></p> <p></p> <ul> <li>Verify MQ App channels defined</li> </ul> <p></p> <ul> <li>Create an OpenShift project named: <code>mq-es-demo</code> with the command: <code>oc new-project mq-es-demo</code></li> <li>Clone the git <code>ibm-cloud-architecture/eda-lab-mq-to-kafka</code> repository:</li> </ul> <pre><code>git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka\n</code></pre> <ul> <li>Update the configMap named <code>store-simulator-cm</code> in folder <code>kustomize/apps/store-simulator/overlays</code> for the store simulator: <code>Workloads &gt; ConfigMaps &gt; Create</code> and use  the parameters from MQ_HOST and MQ_CHANNEL.</li> </ul> <pre><code> apiVersion: v1\n kind: ConfigMap\n metadata:\n   name: store-simulator-cm\n data:\n   APP_VERSION: 0.0.6\n   APP_TARGET_MESSAGING: IBMMQ\n   MQ_HOST: store-mq-ibm-mq.cp4i.svc\n   MQ_PORT: \"1414\"\n   MQ_QMGR: QM1 \n   MQ_QUEUE_NAME: items\n   MQ_CHANNEL: DEV.APP.SVRCONN\n</code></pre>"},{"location":"use-cases/connect-mq/#deploy-store-simulator","title":"Deploy Store simulator","text":"<ul> <li>Deploy to the OpenShift project</li> </ul> <pre><code>oc project mq-es-demo\noc apply -k  kustomize/apps/store-simulator/overlays\n</code></pre> <ul> <li>Access to the application web interface:</li> </ul> <pre><code> oc get route store-simulator -o jsonpath='{.spec.host}' &amp;&amp; echo \"\"\n</code></pre>"},{"location":"use-cases/connect-mq/#deploy-kafka-connect","title":"Deploy Kafka Connect","text":"<p>The Event Streams MQ Source connector product documentation describes what need to be done in details. Basically we have to do two things:</p> <ul> <li>Build the connector image with the needed jars for all the connectos we want to use. For example we need jar files for the MQ connector and jars for the MQ client api.</li> <li>Start a specific connector with the parameters to connect to the external system and to kafka.</li> </ul> <p>To make the MQ source connector immediatly useable for the demonstration, we have build a connector image at <code>quay.io/ibmcase/demomqconnect</code> and the kafkaconnect.yaml is in this file.</p> <p>To deploy the kafka connect framework do:</p> <pre><code>oc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-lab-mq-to-kafka/main/kustomize/environment/kconnect/es-mq/kafka-connect.yaml\n</code></pre> <p>Then start the MQ source connector:</p> <pre><code>oc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/eda-lab-mq-to-kafka/main/kustomize/environment/kconnect/es-mq/mq-src-connector.yaml\n</code></pre>"},{"location":"use-cases/connect-mq/#demonstration-scripts","title":"Demonstration scripts","text":"<ol> <li>The solution has multiple stores as presented in the <code>stores</code> menu. </li> </ol> <ol> <li>In the <code>simulator</code> menu, select the IBMMQ backend, and send some messages randomly using the  left button after selecting the number of messages to send or run the  predefined scenario with the right button:</li> </ol> <ol> <li>In MQ Broker Console go to the <code>items</code> queue to see the messages generated from the simulator</li> </ol> <ol> <li>In Event Streams console goes to the <code>items</code> topic to see the same messages produced by the Kafka MQ Source connector.</li> </ol>"},{"location":"use-cases/connect-mq/#lab-2-mq-source-to-event-streams-using-gitops","title":"Lab 2: MQ source to Event Streams using GitOps","text":"<p>This labs uses GitOps approach with OpenShift GitOps product (ArgoCD) to deploy IBM Event Streams, MQ, the Store Simulator app, and Kafka Connect in the minimum of work. It can be used on a OpenShift Cluster witout any previously deployed Cloud Pak for Integration  opersators. Basically as a SRE you will jumpstart the GitOps operator and then starts ArgoCD.</p> <ul> <li>Clone the <code>store-mq-gitops</code> repo</li> </ul> <p><pre><code>git clone https://github.com/ibm-cloud-architecture/store-mq-gitops\n</code></pre> * Follow the instructions from the repository main Readme file.</p>"},{"location":"use-cases/connect-mq/#lab-3-mq-sink-from-kafka","title":"Lab 3: MQ Sink from Kafka","text":"<p>A may be less used Kafka connector will be to use the MQ Sink connector to get data from Kafka to MQ. This lab addresses such integration. The target deployment looks like in the following diagram:</p> <p></p> <p>Kafka is runnning in its own namespace, and we are using Event Streams from Cloud Pack for Integration.</p>"},{"location":"use-cases/connect-mq/#setup-mq-channel","title":"Setup MQ Channel","text":"<p>Open a shell on the remote container or use the user interface to define the communication channel to use for the Kafka Connection.</p> <ul> <li>Using CLI: Change the name of the Q manager to reflect what you defined in MQ configuration.</li> </ul> <pre><code>runmqsc EDAQMGR1\n# Define a app channel using server connection\nDEFINE CHANNEL(KAFKA.CHANNEL) CHLTYPE(SVRCONN)\n# Set the channel authentication rules to accept connections requiring userid and password\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(BLOCKUSER) USERLIST('nobody')\nSET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)\n# Set identity of the client connections\nALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)\n\nREFRESH SECURITY TYPE(CONNAUTH)\n# Define inventory queue\nDEFINE QLOCAL(INVENTORY)\n# Authorize the IBM MQ user ID to connect to and inquire the queue manager\nSET AUTHREC OBJTYPE(QMGR) PRINCIPAL('admin') AUTHADD(CONNECT,INQ)\n# Authorize the IBM MQ user ID to use the queue:\nSET AUTHREC PROFILE(INVENTORY) OBJTYPE(QUEUE) PRINCIPAL('admin') AUTHADD(ALLMQI)\n# done\nEND\n</code></pre> <ul> <li>As an alternate you can use the MQ administration console.</li> </ul> <p>TBD</p>"},{"location":"use-cases/connect-mq/#setup-kafka-connect-cluster","title":"Setup Kafka Connect Cluster","text":"<p>Connectors can be added to a Kafka Connect environment using OpenShift CLI commands and the source to image customer resource. We will use the Strimzi operator console to setup kafka connect environment. </p> <p>Once you downloaded the zip file, which is a yaml manifest, define the configuration for a KafkaConnectS2I instance. The major configuration settings are the server certificate settings and the authentication using Mutual TLS authentication, something like:</p> <pre><code>spec:\n  bootstrapServers: sandbox-rp-kafka-bootstrap.eventstreams.svc:9093\n  tls:\n    trustedCertificates:\n    - secretName: sandbox-rp-cluster-ca-cert\n      certificate: ca.crt\n  authentication:\n    type: tls\n    certificate: user.crt\n    key: user.key\n    secretName: sandbox-rp-tls-cred\n</code></pre> <p>If you change the name of the connect cluster in the metadata, modify also the name: <code>spec.template.pod.metadata.annotations. productChargedContainers</code> accordingly.</p> <p>The secrets used above, need to be accessible from the project where the connector is deployed. The simple way to do so is to copy the source certificates from the Event streams project to your current project with the commands like:</p> <pre><code>oc get secret  sandbox-rp-cluster-ca-cert  -n eventstreams --export -o yaml | oc apply -f -\noc get secret  sandbox-rp-tls-cred  -n eventstreams --export -o yaml | oc apply -f -\n</code></pre> <p>If you do not have a TLS client certificate from a TLS user, use this note to create one.</p> <ul> <li>Deploy the connector cluster</li> </ul> <pre><code>oc apply -f kafka-connect-s2i.yaml \n</code></pre> <p>An instance of this custom resource represents a Kafka Connect distributed worker cluster. In this mode, workload balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. Each connector is represented by another custom resource called KafkaConnector.</p> <pre><code>oc describe kafkaconnects2i eda-connect-cluster\n</code></pre>"},{"location":"use-cases/connect-mq/#add-the-mq-sink-connector","title":"Add the mq-sink connector","text":"<p>The product documentation details the available MQ connectors and the configuration process. Using the event streams console, the process is quite simple to get a connector configuration as json file. Here is an example of the final form to generate the json file:</p> <p></p> <ul> <li>Once the json is downloaded, complete the settings</li> </ul> <pre><code>{\n  \"name\": \"mq-sink\",\n  \"config\":\n  {\n      \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n      \"tasks.max\": \"1\",\n      \"topics\": \"inventory\",\n      \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n      \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n      \"mq.queue.manager\": \"EDAQMGR1\",\n      \"mq.connection.name.list\": \"eda-mq-lab-ibm-mq(1414)\",\n      \"mq.user.name\": \"admin\",\n      \"mq.password\": \"passw0rd\",\n      \"mq.user.authentication.mqcsp\": true,\n      \"mq.channel.name\": \"KAFKA.CHANNEL\",\n      \"mq.queue\": \"INVENTORY\",\n      \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n  }\n}\n</code></pre> <ul> <li>To run the connector within the cluster, we need to connector jar. You can download this jar file from the Event Stream adming console <code>&gt; Toolkit &gt; Add connector to your Kafka Connect environment &gt; Add Connector &gt; IBM MQ Connectors</code>,</li> </ul> <p></p> <p>or as an alternate, we cloned the mq-sink code, so a <code>mvn package</code> command under <code>kafka-connect-mq-sink</code> folder will build the jar. Copy this jar under <code>cp4i/my-plugins</code> folder.  * Build the connector with source to image component.</p> <p>With the correct credentials for IBM EventStreams and IBM MQ, Kafka Connect should connect to both services and pull data from the EventStreams topic configured to the MQ Queue configured.  You will see signs of success in the container output (via oc logs, or in the UI):</p> <pre><code>+ curl -X POST -H Content-Type: application/json http://localhost:8083/connectors --data @/opt/kafka-connect-mq-sink/config/mq-sink.json\n...\n{\"name\":\"mq-sink-connector\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"mq-service(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink-connector\"},\"tasks\":[{\"connector\":\"mq-sink-connector\",\"task\":0}],\"type\":\"sink\"}\n...\n[2020-06-23 04:26:26,054] INFO Creating task mq-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:419)\n...[2020-06-23 04:26:26,449] INFO Connection to MQ established (com.ibm.eventstreams.connect.mqsink.JMSWriter:229)\n[2020-06-23 04:26:26,449] INFO WorkerSinkTask{id=mq-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:306)\n</code></pre> <p>You should now have the Kafka Connector MQ Sink running on OpenShift.</p>"},{"location":"use-cases/connect-mq/#mq-sink-connector-on-virtual-or-baremetal-server-mq-and-event-streams-on-ibm-cloud","title":"MQ Sink Connector on virtual or baremetal server, MQ and Event Streams on IBM Cloud","text":"<p>In this second option, we are using our own laptop for the baremetal dedployment, but the current solution will work the same on virtual server.</p> <p></p>"},{"location":"use-cases/connect-mq/#pre-requisites","title":"Pre-requisites","text":"<p>We assume that you have an instance of IBM Event Streams already running on IBM Cloud or on OpenShift with at least administrator credentials created. The credentials will be needed later on for configuring the Kafka Connect framework to be able to connect and work with your IBM Event Streams instance. Also, this scenario requires a topic called <code>inventory</code> in your IBM Event Streams instance. For gathering the credentials and creating the topic required for this scenario, please review the Common pre-requisites. IMPORTANT: if you are sharing your IBM Event Streams instance, append some unique identifier to the topic you create in IBM Event Streams.</p>"},{"location":"use-cases/connect-mq/#create-local-ibm-mq-instance","title":"Create Local IBM MQ Instance","text":"<p>In this section we are going to use Docker to create a local IBM MQ instance to simulate an IBM MQ instance somewhere in our datacenter.</p> <ol> <li>Create a data directory to mount onto the container.</li> </ol> <pre><code>mkdir qm1data\n</code></pre> <ol> <li>Run the IBM MQ official Docker image by execting the following command.</li> </ol> <p><pre><code>docker run                     \\\n  --name mq                    \\\n  --detach                     \\\n  --publish 1414:1414          \\\n  --publish 9443:9443          \\\n  --publish 9157:9157          \\\n  --volume qm1data:/mnt/mqm    \\\n  --env LICENSE=accept         \\\n  --env MQ_QMGR_NAME=QM1       \\\n  --env MQ_APP_PASSWORD=admin  \\\n  --env MQ_ENABLE_METRICS=true \\\n  ibmcom/mq\n</code></pre>   where we can see that out container will be called <code>mq</code>, it will run in <code>detached</code> mode (i.e. in the background), it will expose the ports IBM MQ uses for communication and the image we are actually running <code>ibmcom/mq</code>.</p> <ol> <li>You could make sure your IBM MQ Docker image is running by listing the Docker containers in your workstation.</li> </ol> <pre><code>$ docker ps\nCONTAINER ID   IMAGE       COMMAND            CREATED         STATUS         PORTS                                                                    NAMES\na7b2a115a3c6   ibmcom/mq   \"runmqdevserver\"   6 minutes ago   Up 6 minutes   0.0.0.0:1414-&gt;1414/tcp, 0.0.0.0:9157-&gt;9157/tcp, 0.0.0.0:9443-&gt;9443/tcp   mq\n</code></pre> <ol> <li>You should also be able to log into the MQ server on port 9443 (https://localhost:9443) with default user <code>admin</code> and password <code>passw0rd</code>.</li> </ol> <p></p> <p>Now, we need to configure our local IBM MQ instance Queue Manager in order to define a server connection (<code>KAFKA.CHANNEL</code>) with authentication (user <code>admin</code>, password <code>admin</code>) and the queue (<code>INVENTORY</code>) where our messages from Kafka will be sinked to. And we are going to so it using the IBM MQ CLI.</p> <ol> <li>Get into the IBM MQ Docker container we have started above by executing the following command that will give us a <code>bash</code> interactive terminal:</li> </ol> <pre><code>docker exec -ti mq bash\n</code></pre> <ol> <li>Now that we are in the container, start the queue manager <code>QM1</code> by executing:</li> </ol> <pre><code>strmqm QM1\n</code></pre> <ol> <li>Start the <code>runmqsc</code> tool to configure the queue manager by executing:</li> </ol> <pre><code>runmqsc QM1\n</code></pre> <ol> <li>Create a server-connection channel called <code>KAFKA.CHANNEL</code> by executing:</li> </ol> <pre><code>DEFINE CHANNEL(KAFKA.CHANNEL) CHLTYPE(SVRCONN)\n</code></pre> <ol> <li>Set the channel authentication rules to accept connections requiring userid and password by executing:</li> </ol> <pre><code>SET CHLAUTH(KAFKA.CHANNEL) TYPE(BLOCKUSER) USERLIST('nobody')\nSET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)\nSET CHLAUTH(KAFKA.CHANNEL) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)\n</code></pre> <ol> <li>Set the identity of the client connections based on the supplied context, the user ID by executing:</li> </ol> <pre><code>ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)\n</code></pre> <ol> <li>Refresh the connection authentication information by executing:</li> </ol> <pre><code>REFRESH SECURITY TYPE(CONNAUTH)\n</code></pre> <ol> <li>Create the <code>INVENTORY</code> queue for the connector to use by executing:</li> </ol> <pre><code>DEFINE QLOCAL(INVENTORY)\n</code></pre> <ol> <li>Authorize <code>admin</code> to connect to and inquire the queue manager by executing:</li> </ol> <pre><code>SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('admin') AUTHADD(CONNECT,INQ)\n</code></pre> <ol> <li>Finally authorize <code>admin</code> to use the queue by executing:</li> </ol> <pre><code>SET AUTHREC PROFILE(INVENTORY) OBJTYPE(QUEUE) PRINCIPAL('admin') AUTHADD(ALLMQI)\n</code></pre> <ol> <li>End <code>runmqsc</code> by executing:</li> </ol> <pre><code>END\n</code></pre> <p>You should see the following output:</p> <pre><code>9 MQSC commands read.\nNo commands have a syntax error.\nOne valid MQSC command could not be processed.\n</code></pre> <p>Exit the container by executing:</p> <pre><code>exit\n</code></pre> <p>If you check your IBM MQ dashboard you should see your newly created <code>INVENTORY</code> queue:</p> <p></p>"},{"location":"use-cases/connect-mq/#create-mq-kafka-connector-sink","title":"Create MQ Kafka Connector Sink","text":"<p>The MQ Connector Sink can be downloaded from this Github. The Github site includes exhaustive instructions for further detail on this connector and its usage.</p> <ol> <li>Clone the repository with the following command:</li> </ol> <pre><code>git clone https://github.com/ibm-messaging/kafka-connect-mq-sink.git\n</code></pre> <ol> <li>Change directory into the <code>kafka-connect-mq-sink</code> directory:</li> </ol> <pre><code>cd kafka-connect-mq-sink\n</code></pre> <ol> <li>Build the connector using Maven:</li> </ol> <pre><code>mvn clean package\n</code></pre> <ol> <li>Create a directory (if it does not exist yet) to contain the Kafka Connect framework configuration and cd into it.</li> </ol> <pre><code>mkdir config\ncd config\n</code></pre> <ol> <li>Create a configuration file called <code>connect-distributed.properties</code> for the Kafka Connect framework with the following properties in it:</li> </ol> <pre><code># A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.\nbootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\nssl.enabled.protocols=TLSv1.2\nssl.protocol=TLS\nssl.truststore.location=/opt/kafka/config/es-cert.p12\nssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\nssl.truststore.type=PKCS12\nsecurity.protocol=SASL_SSL\nsasl.mechanism=SCRAM-SHA-512\nsasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\";\n\n# Consumer side configuration\nconsumer.bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\nconsumer.security.protocol=SASL_SSL\nconsumer.ssl.protocol=TLSv1.2\nconsumer.ssl.truststore.location=/opt/kafka/config/es-cert.p12\nconsumer.ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\nconsumer.ssl.truststore.type=PKCS12\nconsumer.sasl.mechanism=SCRAM-SHA-512\nconsumer.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\";\n\n# Producer Side\nproducer.bootstrap.servers=REPLACE_WITH_YOUR_BOOTSTRAP_URL\nproducer.security.protocol=SASL_SSL\nproducer.ssl.protocol=TLSv1.2\nproducer.ssl.truststore.location=/opt/kafka/config/es-cert.p12\nproducer.ssl.truststore.password=REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD\nproducer.ssl.truststore.type=PKCS12\nproducer.sasl.mechanism=SCRAM-SHA-512\nproducer.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"REPLACE_WITH_YOUR_SCRAM_USERNAME\" password=\"REPLACE_WITH_YOUR_SCRAM_PASSWORD\";\n\nplugin.path=/opt/kafka/libs\n\n# unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs\ngroup.id=mq-sink-cluster-REPLACE_WITH_UNIQUE_IDENTIFIER\n\n# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will\n# need to configure these based on the format they want their data in when loaded from or stored into Kafka\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply\n# it to\nkey.converter.schemas.enable=true\nvalue.converter.schemas.enable=true\n\n# Topic to use for storing offsets.\noffset.storage.topic=connect-offsets-REPLACE_WITH_UNIQUE_IDENTIFIER\noffset.storage.replication.factor=3\n#offset.storage.partitions=25\n\n# Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, and compacted topic.\nconfig.storage.topic=connect-configs-REPLACE_WITH_UNIQUE_IDENTIFIER\nconfig.storage.replication.factor=3\n\n# Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.\nstatus.storage.topic=connect-status-REPLACE_WITH_UNIQUE_IDENTIFIER\nstatus.storage.replication.factor=3\nstatus.storage.partitions=5\n\n# Flush much faster than normal, which is useful for testing/debugging\noffset.flush.interval.ms=10000\n</code></pre> <p>IMPORTANT: You must replace all occurences of the following placeholders in the properties file above with the appropriate values for the Kafka Connect framework to work with your IBM Event Streams instance:     * <code>REPLACE_WITH_YOUR_BOOTSTRAP_URL</code>: Your IBM Event Streams bootstrap url.     * <code>REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD</code>: Your PCKS12 TLS certificate password.     * <code>REPLACE_WITH_YOUR_SCRAM_USERNAME</code>: Your SCRAM service credentials username.     * <code>REPLACE_WITH_YOUR_SCRAM_PASSWORD</code>: Your SCRAM service credentials password.     * <code>REPLACE_WITH_UNIQUE_IDENTIFIER</code>: A unique identifier so that the resources your kafka connect cluster will create on your IBM Event Streams instance don't collide with other users' resources (Note: there are 4 placeholders of this type. Replace them all). </p> <p>Review the Common pre-requisites instructions if you don't know how to find out any of the config properties above. </p> <ol> <li> <p>Download the IBM Event Streams TLS certificate so that your Kafka Connect framework local instance can establish secure communication with your IBM Event Streams instance. IMPORTANT: download the PKCS12 certificate. How to get the certificate in the Common pre-requisites section. </p> </li> <li> <p>Create a log4j configuration file named <code>connect-log4j.properties</code> based on the template below.</p> </li> </ol> <pre><code>log4j.rootLogger=DEBUG, stdout\n\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c:%L)%n\n\nlog4j.logger.org.apache.kafka=INFO\n</code></pre> <ol> <li>Back out of the <code>config</code> directory to the <code>kafka-connect-mq-sink</code> directory:</li> </ol> <pre><code>cd ..\n</code></pre> <ol> <li>Build the Docker image for your Kafka Connect framework that will contain the IBM MQ Sink connector and all the properties files you have created and tailored earlier so that your Kafka Connect framework can work with your IBM Event Streams instance we have set up previously in this exercise (mind the dot at the end of the command. It is necessary):</li> </ol> <pre><code>docker build -t kafkaconnect-with-mq-sink:1.3.0 .\n</code></pre> <ol> <li>Run the Kafka Connect MQ Sink container.</li> </ol> <pre><code>docker run                                 \\\n  --name mq-sink                           \\\n  --detach                                 \\\n  --volume $(pwd)/config:/opt/kafka/config \\\n  --publish 8083:8083                      \\\n  --link mq:mq                             \\\n  kafkaconnect-with-mq-sink:1.3.0\n</code></pre> <ol> <li>Now that we have a Kafka Connect framework running with the configuration to connect to our IBM Event Streams instance and the IBM MQ Sink Connector jar file in it, is to create a JSON file called <code>mq-sink.json</code> to startup an instance of the IBM MQ Sink connector in our Kafka Connect framework with the appropriate config to work read messages from the IBM Event Streams topic we desire and sink those to the IBM MQ instance we have running locally. IMPORTANT: If you are sharing your IBM Event Streams instance and followed the instructions on this readme, you should have appended a unique identifier to the name of the topic (<code>inventory</code>) that you are meant to create in IBM Event Streams. As a result, modify the line <code>\"topics\": \"inventory\"</code> in the following JSON object accordignly.</li> </ol> <pre><code>{\n   \"name\": \"mq-sink\",\n   \"config\":\n   {\n       \"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\n       \"tasks.max\": \"1\",\n       \"topics\": \"inventory\",\n\n       \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n       \"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n\n       \"mq.queue.manager\": \"QM1\",\n       \"mq.connection.name.list\": \"mq(1414)\",\n       \"mq.user.name\": \"admin\",\n       \"mq.password\": \"passw0rd\",\n       \"mq.user.authentication.mqcsp\": true,\n       \"mq.channel.name\": \"KAFKA.CHANNEL\",\n       \"mq.queue\": \"INVENTORY\",\n       \"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\"\n   }\n}\n</code></pre> <ol> <li>Last piece of the puzzle is to tell our Kafka Connect framework to create and startup the IBM MQ Sink connector based on the configuration we have in the previous json file. To do so, execute the following <code>POST</code> request.</li> </ol> <pre><code>curl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./mq-sink.json\"\n\n# The response returns the metadata about the connector\n{\"name\":\"mq-sink\",\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"inventory\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"mq.queue.manager\":\"QM1\",\"mq.connection.name.list\":\"ibmmq(1414)\",\"mq.user.name\":\"admin\",\"mq.password\":\"passw0rd\",\"mq.user.authentication.mqcsp\":\"true\",\"mq.channel.name\":\"KAFKA.CHANNEL\",\"mq.queue\":\"INVENTORY\",\"mq.message.builder\":\"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\"name\":\"mq-sink\"},\"tasks\":[{\"connector\":\"mq-sink\",\"task\":0}],\"type\":\"sink\"}\n</code></pre> <ol> <li>You can query your Kafka Connect framework to make sure of this. Execute the following command that should return all the connectors up and running in your Kafka Connect cluster.</li> </ol> <pre><code>curl localhost:8083/connectors\n[\"mq-sink\"]\n</code></pre> <p>You should now have a working MQ Sink connector getting messages from your topic in IBM Event Streams and sending these to your local IBM MQ instance. In order to check that out working, we first need to send few messages to that IBM Event Streams topic and then check out our <code>INVENTORY</code> queue in our local IBM MQ instance.</p> <ol> <li>In order to send messages to IBM Event Streams, we are going to use the IBM Event Streams Starter application. You can find the instruction either in the IBM Event Streams official documentation [here] or on the IBM Event Streams dashboard:</li> </ol> <p></p> <ol> <li>Follow the instructions to run the IBM Event Streams starter application from your workstation. IMPORTANT: When generating the properties for your IBM Event Streams starter application, please choose to connect to an existing topic and select the topic you created previously as part of this exercise so that the messages we send into IBM Event Streams end up in the appropriate topic that is being monitored by your IBM MQ Sink connector runnning on the Kafka Connect framework.</li> </ol> <p></p> <ol> <li>Once you have your application running, open it up in your web browser, click on <code>Start producing</code>, let the application produce a couple of messages and then click on <code>Stop producing</code></li> </ol> <p></p> <ol> <li>Check out your those messages got into the Kafka topic</li> </ol> <p></p> <ol> <li>Check out your messages in the Kafka topic have already reached your <code>INVENTORY</code> queue in your local IBM MQ instance</li> </ol> <p></p> <ol> <li>You could also inspect the logs of your Kafka Connect Docker container running on your workstation:</li> </ol> <pre><code>docker logs mq-sink\n...\n[2021-01-19 19:44:17,110] DEBUG Putting record for topic inventory, partition 0 and offset 0 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:89)\n[2021-01-19 19:44:17,110] DEBUG Value schema Schema{STRING} (com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60)\n[2021-01-19 19:44:18,292] DEBUG Flushing up to topic inventory, partition 0 and offset 1 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:110)\n[2021-01-19 19:44:18,292] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 636: {inventory-0=OffsetAndMetadata{offset=1, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:346)\n[2021-01-19 19:44:18,711] DEBUG Putting record for topic inventory, partition 0 and offset 1 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:89)\n[2021-01-19 19:44:18,711] DEBUG Value schema Schema{STRING} (com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60)\n[2021-01-19 19:44:20,674] DEBUG Putting record for topic inventory, partition 0 and offset 2 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:89)\n[2021-01-19 19:44:20,675] DEBUG Value schema Schema{STRING} (com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder:60)\n[2021-01-19 19:44:28,293] DEBUG Flushing up to topic inventory, partition 0 and offset 3 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:110)\n[2021-01-19 19:44:28,293] INFO WorkerSinkTask{id=mq-sink-0} Committing offsets asynchronously using sequence number 637: {inventory-0=OffsetAndMetadata{offset=3, leaderEpoch=null, metadata=''}} (org.apache.kafka.connect.runtime.WorkerSinkTask:346)\n[2021-01-19 19:44:38,296] DEBUG Flushing up to topic inventory, partition 0 and offset 3 (com.ibm.eventstreams.connect.mqsink.MQSinkTask:110)\n</code></pre> <p>To cleanup your environment, you can do</p> <ol> <li>Remove the connector from your Kafka Connect framework instance (this isn't really needed if you are going to stop and remove the Kafka Connect Docker container)</li> </ol> <pre><code>curl -X DELETE http://localhost:8083/connectors/mq-sink\n</code></pre> <ol> <li>Stop both IBM MQ and Kakfa Connect Docker containers running on your workstation</li> </ol> <pre><code>docker stop mq mq-sink\n</code></pre> <ol> <li>Remove both IBM MQ and Kakfa Connect Docker containers from your workstation</li> </ol> <pre><code>docker rm mq mq-sink\n</code></pre>"},{"location":"use-cases/connect-mq/#lab-4-mq-connector-with-kafka-confluent","title":"Lab 4: MQ Connector with  Kafka Confluent","text":"<ul> <li>Clone the git <code>ibm-cloud-architecture/eda-lab-mq-to-kafka</code> repository and follow the readme.</li> </ul> <pre><code>git clone https://github.com/ibm-cloud-architecture/eda-lab-mq-to-kafka\n</code></pre>"},{"location":"use-cases/connect-rabbitmq/","title":"Kafka Connect to RabbitMQ Source Connector","text":"<p>Info</p> <p>Updated 11/10/2020 -  Lab completed on local computer. Instruction not completed for OpenShift deployment.</p> <p>This hands-on lab demonstrates how to use IBM RabbitMQ Kafka source connector to inject message to Event Streams On Premise or any Kafka cluster.  We are using the IBM messaging github: source Kafka connector for RabbitMQ open sourced component. The configuration for this connector is also done using Json config file, with a POST to the Kafka connectors URL.</p> <p>The following diagram illustrates the component of this demo / lab:</p> <p></p> <p>The configurations used in this use case are in the <code>refarch-eda-tools</code> repository that you should clone:</p> <pre><code>git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\n</code></pre> <p>under the labs/rabbitmq-source-lab folder.</p>"},{"location":"use-cases/connect-rabbitmq/#just-run-it","title":"Just run it!","text":"<p>For demonstration purpose, we have defined a docker compose file and a set of docker images ready to use. So it will be easier to demonstrate the scenario of sending messages to Rabbit MQ <code>items</code> queue, have a Kafka connector configured and a simple Kafka consumer which consumes Json doc from the <code>items</code> Kafka topic. </p> <p>To send message to RabbitMQ, we have implemented a simple store sale simulator, which sends item sale messages for a set of stores. The code is under eda-store-simulator repository and we have also uploaded the image into dockerhub ibmcase/eda-store-simulator.</p>"},{"location":"use-cases/connect-rabbitmq/#start-backend-services","title":"Start backend services","text":"<p>To quickly start a local demonstration environment, the github repository for this lab includes a docker compose file under the <code>labs/rabbitmq-source-lab</code> folder.</p> <pre><code>git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\ncd refarch-eda-tools/labs/rabbitmq-source-lab\ndocker-compose up -d\ndocker ps \n\nIMAGE                              NAMES                            PORT                  \nibmcase/kconnect:v1.0.0            rabbitmq-source-lab_kconnect_1   0.0.0.0:8083-&gt;8083/  \nstrimzi/kafka:latest-kafka-2.6.0   rabbitmq-source-lab_kafka_1      0.0.0.0:9092-&gt;9092/\nibmcase/eda-store-simulator        rabbitmq-source-lab_simulator_1  0.0.0.0:8080-&gt;8080/\nrabbitmq:3-management              rabbitmq-source-lab_rabbitmq_1   0.0.0.0:5672-&gt;5672/tcp, 15671/tcp, 15691-15692/tcp, 25672/tcp, 0.0.0.0:15672-&gt;15672/tcp   \nstrimzi/kafka:latest-kafka-2.6.0   rabbitmq-source-lab_zookeeper_1 0.0.0.0:2181-&gt;2181/                                                                                      \n</code></pre>"},{"location":"use-cases/connect-rabbitmq/#deploy-source-connector","title":"Deploy source connector","text":"<p>Once the different containers are started, you can deploy the RabbitMQ source connector using the following POST</p> <pre><code># under the folder use the curl to post configuration\n./sendRMQSrcConfig.sh localhost:8083\n\n# Trace should look like\n{\"name\":\"RabbitMQSourceConnector\",\n\"config\":{\"connector.class\":\"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\",\n\"tasks.max\":\"1\",\n\"kafka.topic\":\"items\",\n\"rabbitmq.host\":\"rabbitmq\",\n\"rabbitmq.port\":\"5672\",\n\"rabbitmq.queue\":\"items\",\n\"rabbitmq.username\":\"rabbit-user\",\n\"rabbitmq.password\":\"rabbit-pass\",\n\"rabbitmq.prefetch.count\":\"500\",\n\"rabbitmq.automatic.recovery.enabled\":\"true\",\n\"rabbitmq.network.recovery.interval.ms\":\"10000\",\n\"rabbitmq.topology.recovery.enabled\":\"true\",\n\"name\":\"RabbitMQSourceConnector\"},\n\"tasks\":[],\n\"type\":\"source\"}\n</code></pre> <ul> <li> <p>And then looking at the deployed connectors using http://localhost:8083/connectors or the RabbitMQ connector with http://localhost:8083/connectors/RabbitMQSourceConnector to get the same response as the above json.</p> </li> <li> <p>Create <code>items</code> topic into Kafka using the script:</p> </li> </ul> <pre><code>./createTopics.sh\n</code></pre> <ul> <li>Send some records using the simulator at the http://localhost:8080/#/simulator URL by first selecting RabbitMQ button and the number of message to send: </li> </ul> <p></p> <ul> <li>Verify the RabbitMQ settings</li> </ul> <p>In a Web Browser go to http://localhost:15672/ using the rabbit-user/rabbit-pass login.</p> <p>You should reach this console:</p> <p></p> <p>Go to the Queue tab and select the <code>items</code> queue:</p> <p></p> <p>And then get 1 message from the queue:</p> <p></p> <ul> <li>Now verify the Kafka connect-* topics are created successfully:</li> </ul> <pre><code>docker run -ti --network  rabbitmq-source-lab_default strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list\"\n# you should get at least\n__consumer_offsets\nconnect-configs\nconnect-offsets\nconnect-status\nitems\n</code></pre> <ul> <li>Verify the message arrived to the <code>items</code> in kafka topic:</li> </ul> <p>```shell  docker run -ti --network  rabbitmq-source-lab_default  strimzi/kafka:latest-kafka-2.6.0 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic items --from-beginning\"</p> <p># You may have a trace like which maps the messages sent   \"{\\\"id\\\":0,\\\"price\\\":11.0,\\\"quantity\\\":6,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_3\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395325\\\",\\\"type\\\":\\\"SALE\\\"}\" \"{\\\"id\\\":1,\\\"price\\\":68.5,\\\"quantity\\\":0,\\\"sku\\\":\\\"Item_5\\\",\\\"storeName\\\":\\\"Store_1\\\",\\\"timestamp\\\":\\\"2020-10-23T19:11:26.395447\\\",\\\"type\\\":\\\"SALE\\\"}\"  ```</p> <p>You validated the solution works end to end. Now we can try to deploy those components to OpenShift.</p>"},{"location":"use-cases/connect-rabbitmq/#deploy-on-openshift","title":"Deploy on OpenShift","text":"<p>In this section, you will deploy the Kafka connector and Rabbit MQ on OpenShift using Event Streams on Premise as Kafka cluster.</p>"},{"location":"use-cases/connect-rabbitmq/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>To run the demonstration be sure to have the event store simulator deployed: see instructions here.</li> </ul> <p>Pull in necessary pre-req from the Realtime Inventory Pre-reqs, like for example the deployment of the store simulator.</p>"},{"location":"use-cases/connect-rabbitmq/#deploy-rabbitmq-using-operators","title":"Deploy RabbitMQ using operators","text":"<p>See the installation instructions to get a RabbitMQ operator installed, or use our manifests from the refarch-eda-tools repository, which will set user 1000 and 999 for OpenShift deployment. The RabbitMQ cluster operator runs as user ID <code>1000</code>. The RabbitMQ pod runs the RabbitMQ container as user ID <code>999</code> and an init container as user ID <code>0</code>.</p> <ul> <li>Create a dedicated project to run RabbitMQ operator to observe any projects, and install CRD, service account, RBAC role, deployment... </li> </ul> <pre><code>oc new-project rabbitmq-system\n# under the labs/rabbitmq-source-lab folder do\nkubectl apply -f cluster-operator.yml\n\nkubectl get customresourcedefinitions.apiextensions.k8s.io\n</code></pre> <ul> <li>Create a second project for RabbitMQ cluster. It is recommended to have separate project for RabbitMQ cluster. Modify the namespace to be able to deploy RabbitMQ cluster into it. So we need to get user 999 authorized. For that we specify a security constraint named: <code>rabbitmq-cluster</code></li> </ul> <pre><code> #Define a Security Context Constraint\n oc apply -f rmq-scc.yaml\n</code></pre> <ul> <li>Update the name space to get openshift scc userid and groupid range. You can do that in two ways, by directly editing the namespace definition:</li> </ul> <pre><code> oc edit namespace rabbitmq-cluster\n # Then add the following annotations:\n annotations:\n   openshift.io/sa.scc.supplemental-groups: 999/1\n   openshift.io/sa.scc.uid-range: 0-999/1\n</code></pre> <p>Or by using our custom cluster definition: <code>oc apply -f rmq-cluster-ns.yaml</code></p> <ul> <li> <p>For every RabbitMQ cluster assign the previously created security context constraint to the cluster's service account: <code>oc adm policy add-scc-to-user rabbitmq-cluster -z rmqcluster-rabbitmq-server</code></p> </li> <li> <p>Finally create a cluster instance: <code>oc apply -f rmq-cluster.yaml</code>.</p> </li> <li>Define a route on the client service on port 15672</li> <li>Get the admin user and password as defined in the secrets</li> </ul> <pre><code> oc describe secret rmqcluster-rabbitmq-default-user\n # Get user name\n oc get secret rmqcluster-rabbitmq-default-user  -o jsonpath='{.data.username}' | base64 --decode \n # Get password\n oc get secret rmqcluster-rabbitmq-default-user  -o jsonpath='{.data.password}' | base64 --decode \n</code></pre> <p>To validate the RabbitMQ cluster runs correctly you can use the RabbitMQ console, and add the <code>items</code> queue.</p>"},{"location":"use-cases/connect-rabbitmq/#configure-the-kafka-connector-for-rabbitmq-source","title":"Configure the kafka connector for Rabbitmq source","text":"<p>The <code>rabbitmq-source.json</code> define the connector and the RabbitMQ connection parameters:</p> <pre><code>{\n    \"name\": \"RabbitMQSourceConnector\",\n    \"config\": {\n        \"connector.class\": \"com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceConnector\",\n        \"tasks.max\": \"1\",\n        \"kafka.topic\" : \"items\",\n        \"rabbitmq.host\": \"rabbitmq\",\n        \"rabbitmq.port\": 5672,\n        \"rabbitmq.queue\" : \"items\",\n        \"rabbitmq.prefetch.count\" : \"500\",\n        \"rabbitmq.automatic.recovery.enabled\" : \"true\",\n        \"rabbitmq.network.recovery.interval.ms\" : \"10000\",\n        \"rabbitmq.topology.recovery.enabled\" : \"true\"\n    }\n}\n</code></pre> <p>This file is uploaded to Kafka Connect via a PORT operation:</p> <pre><code>curl -X POST -H \"Content-Type: application/json\" http://localhost:8083/connectors   --data \"@./rabbitmq-source.json\"\n</code></pre> <p>To verify use: <code>curl -X GET http://localhost:8083/connectors</code>.</p> <p>In Kafka connect trace you should see:</p> <pre><code>[Worker clientId=connect-1, groupId=eda-kconnect] Connector RabbitMQSourceConnector config updated\n...\nStarting connector RabbitMQSourceConnector\n...\n Starting task RabbitMQSourceConnector-0\n</code></pre> <p>And Rabbitmq that get the connection from Kafka Connect.</p> <pre><code>rabbitmq_1  [info] &lt;0.1766.0&gt; accepting AMQP connection &lt;0.1766.0&gt; (172.19.0.3:33040 -&gt; 172.19.0.2:5672)\nkconnect_1  INFO Creating Channel (com.ibm.eventstreams.connect.rabbitmqsource.RabbitMQSourceTask:61)\nrabbitmq_1  connection &lt;0.1766.0&gt; (172.19.0.3:33040 -&gt; 172.19.0.2:5672): user 'rabbit-user' authenticated and granted access to vhost '/'\n</code></pre>"},{"location":"use-cases/connect-s3/","title":"Kafka Connect to S3 Source & Sink","text":""},{"location":"use-cases/connect-s3/#overview","title":"Overview","text":"<p>This scenario walkthrough will cover the usage of IBM Event Streams as a Kafka provider and Amazon S3 as an object storage service as systems to integrate with the Kafka Connect framework. Through the use of the Apache Camel opensource project, we are able to use the Apache Camel Kafka Connector in both a source and a sink capacity to provide bidirectional communication between IBM Event Streams and AWS S3.</p> <p>As different use cases will require different configuration details to accommodate different situational requirements, the Kafka to S3 Source and Sink capabilities described here can be used to move data between S3 buckets with a Kafka topic being the middle-man or move data between Kafka topics with an S3 Bucket being the middle-man. However, take care to ensure that you do not create an infinite processing loop by writing to the same Kafka topics and the same S3 buckets with both a Source and Sink connector deployed at the same time.</p>"},{"location":"use-cases/connect-s3/#scenario-prereqs","title":"Scenario prereqs","text":"<p>OpenShift Container Platform</p> <ul> <li>This deployment scenario was developed for use on the OpenShift Container Platform, with a minimum version of <code>4.2</code>.</li> </ul> <p>Strimzi - This deployment scenario will make use of the Strimzi Operator for Kafka deployments and the custom resources it manages. - A minimum version of <code>0.17.0</code> is required for this scenario. This scenario has been explicitly validated with version <code>0.17.0</code>. - The simplest scenario is to deploy the Strimzi Operator to watch all namespaces for relevant custom resource creation and management. - This can be done in the OpenShift console via the Operators &gt; Operator Hub page.</p> <p>Amazon Web Services account - As this scenario will make use of AWS S3, an active Amazon Web Services account is required. - Using the configuration described in this walkthrough, an additional IAM user can be created for further separation of permission, roles, and responsibilities. - This new IAM user should contain:   -  The <code>AmazonS3FullAccess</code> policy attached to it (as it will need both read and write access to S3),   -  An S3 Bucket Policy set on the Bucket to allow the IAM user to perform CRUD actions on the bucket and its objects. - Create a file named <code>aws-credentials.properties</code> with the following format:</p> <pre><code>aws_access_key_id=AKIA123456EXAMPLE\naws_secret_access_key=strWrz/bb8%c3po/r2d2EXAMPLEKEY\n</code></pre> <ul> <li>Create a Kubernetes Secret from this file to inject into the Kafka Connect cluster at runtime:</li> </ul> <p><pre><code>kubectl create secret generic aws-credentials --from-file=aws-credentials.properties\n</code></pre> - Additional work is underway to enable configuration of the components to make use of IAM Roles instead.</p> <p>IBM Event Streams API Key - This scenario is written with IBM Event Streams as the provider of the Kafka endpoints. - API Keys are required for connectivity to the Kafka brokers and interaction with Kafka topics. - An API key should be created with (at minimum) read and write access to the source and target Kafka topics the connectors will interact with. - A Kubernetes Secret must be created with the Event Streams API to inject into the Kafka Connect cluster at runtime:</p> <pre><code>kubectl create secret generic eventstreams-apikey --from-literal=password=&lt;eventstreams_api_key&gt;\n</code></pre> <p>IBM Event Streams Certificates on IBM Cloud Pak for Integration - If you are using an IBM Event Streams instance deployed via the IBM Cloud Pak for Integration, you must also download the generated truststore file to provide TLS communication between the connectors and the Kafka brokers. - This file, along with its password, can be found on the Connect to this cluster dialog in the Event Streams UI. - Once downloaded, it must be configured to work with the Kafka Connect certificate deployment pattern:</p> <pre><code>keytool -importkeystore -srckeystore es-cert.jks -destkeystore es-cert.p12 -deststoretype PKCS12\nopenssl pkcs12 -in es-cert.p12 -nokeys -out es-cert.crt\nkubectl create secret generic eventstreams-truststore-cert --from-file=es-cert.crt\n</code></pre> <p>IBM Event Streams Certificates on IBM Cloud - If you are using an IBM Event Streams instance deployed on IBM Cloud, you need to provide the root CA certificate to connect correctly from the Kafka Connect instance. - This file can be downloaded as <code>eventstreams.cloud.ibm.com.cer</code> from the endpoint defined in your service credentials via the <code>kafka_http_url</code> property. - Once downloaded, it must be configured to work with the Kafka Connect certificate deployment pattern:</p> <pre><code>openssl x509 -inform DER -in eventstreams.cloud.ibm.com.cer -out es-cert.crt\nkubectl create secret generic eventstreams-truststore-cert --from-file=es-cert.crt\n</code></pre>"},{"location":"use-cases/connect-s3/#kafka-connect-cluster","title":"Kafka Connect Cluster","text":"<p>We will take advantage of some of the developer experience improvements that OpenShift and the Strimi Operator brings to the Kafka Connect framework. The Strimzi Operator provides a <code>KafkaConnect</code> custom resource which will manage a Kafka Connect cluster for us with minimal system interaction. The only work we need to do is to update the container image that the Kafka Connect deployment will use with the necessary Camel Kafka Connector binaries, which OpenShift can help us with through the use of its Build capabilities.</p>"},{"location":"use-cases/connect-s3/#optional-create-configmap-for-log4j-configuration","title":"(Optional) Create ConfigMap for log4j configuration","text":"<p>Due to the robust nature of Apache Camel, the default logging settings for the Apache Kafka Connect classes will send potentially sensitive information to the logs during Apache Camel context configuration. To avoid this, we can provide an updated logging configuration to the <code>log4j</code> configuration that is used by our deployments.</p> <p>Save the properties file below and name it <code>log4j.properties</code>. Then create a ConfigMap via <code>kubectl create configmap custom-connect-log4j --from-file=log4j.properties</code>. This ConfigMap will then be used in our KafkaConnect cluster creation to filter out any logging output containing <code>accesskey</code> or <code>secretkey</code> permutations.</p> <pre><code># Do not change this generated file. Logging can be configured in the corresponding kubernetes/openshift resource.\nlog4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\nlog4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\nconnect.root.logger.level=INFO\nlog4j.rootLogger=${connect.root.logger.level}, CONSOLE\nlog4j.logger.org.apache.zookeeper=ERROR\nlog4j.logger.org.I0Itec.zkclient=ERROR\nlog4j.logger.org.reflections=ERROR\n\n# Due to back-leveled version of log4j that is included in Kafka Connect,\n# we can use multiple StringMatchFilters to remove all the permutations\n# of the AWS accessKey and secretKey values that may get dumped to stdout\n# and thus into any connected logging system.\nlog4j.appender.CONSOLE.filter.a=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.a.StringToMatch=accesskey\nlog4j.appender.CONSOLE.filter.a.AcceptOnMatch=false\nlog4j.appender.CONSOLE.filter.b=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.b.StringToMatch=accessKey\nlog4j.appender.CONSOLE.filter.b.AcceptOnMatch=false\nlog4j.appender.CONSOLE.filter.c=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.c.StringToMatch=AccessKey\nlog4j.appender.CONSOLE.filter.c.AcceptOnMatch=false\nlog4j.appender.CONSOLE.filter.d=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.d.StringToMatch=ACCESSKEY\nlog4j.appender.CONSOLE.filter.d.AcceptOnMatch=false\n\nlog4j.appender.CONSOLE.filter.e=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.e.StringToMatch=secretkey\nlog4j.appender.CONSOLE.filter.e.AcceptOnMatch=false\nlog4j.appender.CONSOLE.filter.f=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.f.StringToMatch=secretKey\nlog4j.appender.CONSOLE.filter.f.AcceptOnMatch=false\nlog4j.appender.CONSOLE.filter.g=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.g.StringToMatch=SecretKey\nlog4j.appender.CONSOLE.filter.g.AcceptOnMatch=false\nlog4j.appender.CONSOLE.filter.h=org.apache.log4j.varia.StringMatchFilter\nlog4j.appender.CONSOLE.filter.h.StringToMatch=SECRETKEY\nlog4j.appender.CONSOLE.filter.h.AcceptOnMatch=false\n</code></pre>"},{"location":"use-cases/connect-s3/#deploy-the-baseline-kafka-connect-cluster","title":"Deploy the baseline Kafka Connect Cluster","text":"<p>Review the YAML description for our <code>KafkaConnectS2I</code> custom resource below, named <code>connect-cluster-101</code>. Pay close attention to (using YAML notation): - <code>spec.logging.name</code> should point to the name of the ConfigMap created in the previous step to configure custom <code>log4j</code> logging filters (optional) - <code>spec.bootstrapServers</code> should be updated with your local Event Streams endpoints - <code>spec.tls.trustedCertificates[0].secretName</code> should match the Kubernetes Secret containing the IBM Event Streams certificate - <code>spec.authentication.passwordSecret.secretName</code> should match the Kubernetes Secret containing the IBM Event Streams API key - <code>spec.externalConfiguration.volumes[0].secret.secretName</code> should match the Kubernetes Secret containing your AWS credentials - <code>spec.config['group.id']</code> should be a unique name for this Kafka Connect cluster across all Kafka Connect instances that will be communicating with the same set of Kafka brokers. - <code>spec.config['*.storage.topic']</code> should be updated to provide unique topics for this Kafka Connect cluster inside your Kafka deployment. Distinct Kafka Connect clusters should not share metadata topics.</p> <pre><code>apiVersion: kafka.strimzi.io/v1alpha1\nkind: KafkaConnectS2I\nmetadata:\n  name: connect-cluster-101\n  annotations:\n    strimzi.io/use-connector-resources: \"true\"\nspec:\n  #logging:\n  #  type: external\n  #  name: custom-connect-log4j\n  replicas: 1\n  bootstrapServers: es-1-ibm-es-proxy-route-bootstrap-eventstreams.apps.cluster.local:443\n  tls:\n    trustedCertificates:\n      - certificate: es-cert.crt\n        secretName: eventstreams-truststore-cert\n  authentication:\n    passwordSecret:\n      secretName: eventstreams-apikey\n      password: password\n    username: token\n    type: plain\n  externalConfiguration:\n    volumes:\n      - name: aws-credentials\n        secret:\n          secretName: aws-credentials\n  config:\n    group.id: connect-cluster-101\n    config.providers: file\n    config.providers.file.class: org.apache.kafka.common.config.provider.FileConfigProvider\n    key.converter: org.apache.kafka.connect.json.JsonConverter\n    value.converter: org.apache.kafka.connect.json.JsonConverter\n    key.converter.schemas.enable: false\n    value.converter.schemas.enable: false\n    offset.storage.topic: connect-cluster-101-offsets\n    config.storage.topic: connect-cluster-101-configs\n    status.storage.topic: connect-cluster-101-status\n</code></pre> <p>Save the YAML above into a file named <code>kafka-connect.yaml</code>. If you created the ConfigMap in the previous step to filter out <code>accesskey</code> and <code>secretkey</code> values from the logs, uncomment the <code>spec.logging</code> lines to allow for the custom logging filters to be enabled during Kafka Connect cluster creation. Then this resource can be created via <code>kubectl apply -f kafka-connect.yaml</code>. You can then tail the output of the <code>connect-cluster-101</code> pods for updates on the connector status.</p>"},{"location":"use-cases/connect-s3/#build-the-camel-kafka-connector","title":"Build the Camel Kafka Connector","text":"<p>The next step is to build the Camel Kafka Connector binaries so that they can be loaded into the just-deployed Kafka Connect cluster's container images.</p> <ol> <li> <p>Clone the repository https://github.com/osowski/camel-kafka-connector to your local machine:</p> <ul> <li><code>git clone https://github.com/osowski/camel-kafka-connector.git</code></li> </ul> </li> <li> <p>Check out the <code>camel-kafka-connector-0.1.0-branch</code>:</p> </li> <li> <p><code>git checkout camel-kafka-connector-0.1.0-branch</code></p> </li> <li> <p>From the root directory of the repository, build the project components:</p> <ul> <li><code>mvn clean package</code></li> <li> <p>Go get a coffee and take a walk... as this build will take around 30 minutes on a normal developer workstation.</p> </li> <li> <p>To reduce the overall build scope of the project, you can comment out the undesired modules from the <code>&lt;modules&gt;</code> element of the <code>connectors/pom.xml</code> using <code>&lt;!-- --&gt;</code> notation.</p> </li> </ul> </li> <li> <p>Copy the generated S3 artifacts to the core package build artifacts:</p> <ul> <li><code>cp connectors/camel-aws-s3-kafka-connector/target/camel-aws-s3-kafka-connector-0.1.0.jar core/target/camel-kafka-connector-0.1.0-package/share/java/camel-kafka-connector/</code></li> </ul> </li> </ol> <p>Some items to note:</p> <ul> <li>The repository used here (https://github.com/osowski/camel-kafka-connector) is a fork of the official repository (https://github.com/apache/camel-kafka-connector) with a minor update applied to allow for dynamic endpoints to be specified via configuration, which is critical for our Kafka to S3 Sink Connector scenario.</li> <li>This step (and the next step) will eventually be eliminated by providing an existing container image with the necessary Camel Kafka Connector binaries as part of a build system.</li> </ul>"},{"location":"use-cases/connect-s3/#deploy-the-camel-kafka-connector-binaries","title":"Deploy the Camel Kafka Connector binaries","text":"<p>Now that the Camel Kafka Connector binaries have been built, we need to include them on the classpath inside of the container image which our Kafka Connect clusters are using.</p> <ol> <li> <p>From the root directory of the repository, start a new OpenShift Build, using the generated build artifacts:</p> <pre><code>oc start-build connect-cluster-101-connect --from-dir=./core/target/camel-kafka-connector-0.1.0-package/share/java --follow\n</code></pre> </li> <li> <p>Watch the Kubernetes pods as they are updated with the new build and rollout of the Kafka Connect Cluster using the updated container image (which now includes the Camel Kafka Connector binaries):</p> <pre><code>kubectl get pods -w\n</code></pre> </li> <li> <p>Once the <code>connect-cluster-101-connect-2-[random-suffix]</code> pod is in a <code>Running</code> state, you can proceed.</p> </li> </ol>"},{"location":"use-cases/connect-s3/#kafka-to-s3-sink-connector","title":"Kafka to S3 Sink Connector","text":"<p>Now that you have a Kafka Connect cluster up and running, you will need to configure a connector to actually begin the transmission of data from one system to the other. This will be done by taking advantage of Strimzi and using the <code>KafkaConnector</code> custom resource the Strimzi Operator manages for us.</p> <p>Review the YAML description for our <code>KafkaConnector</code> custom resource below, named <code>s3-sink-connector</code>. Pay close attention to: - The <code>strimzi.io/cluster</code> label must match the deployed Kafka Connect cluster you previously deployed (or else Strimzi will not connect the <code>KafkaConnector</code> to your <code>KafkaConnect</code> cluster) - The <code>topics</code> parameter (named <code>my-source-topic</code> here) - The S3 Bucket parameter of the <code>camel.sink.url</code> configuration option (named <code>my-s3-bucket</code> here)</p> <pre><code>apiVersion: kafka.strimzi.io/v1alpha1\nkind: KafkaConnector\nmetadata:\n  name: s3-sink-connector\n  labels:\n    strimzi.io/cluster: connect-cluster-101\nspec:\n  class: org.apache.camel.kafkaconnector.CamelSinkConnector\n  tasksMax: 1\n  config:\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.kafka.connect.storage.StringConverter\n    topics: my-source-topic\n    camel.sink.url: aws-s3://my-s3-bucket?keyName=${date:now:yyyyMMdd-HHmmssSSS}-${exchangeId}\n    camel.sink.maxPollDuration: 10000\n    camel.component.aws-s3.configuration.autocloseBody: false\n    camel.component.aws-s3.accessKey: ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id}\n    camel.component.aws-s3.secretKey: ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key}\n    camel.component.aws-s3.region: US_EAST_1\n</code></pre> <p>Once you have updated the YAML and saved it in a file named <code>kafka-sink-connector.yaml</code>, this resource can be created via <code>kubectl apply -f kafka-sink-connector.yaml</code>. You can then tail the output of the <code>connect-cluster-101</code> pods for updates on the connector status.</p> <p>NOTE: If you require objects in S3 to reside in a sub-folder of the bucket root, you can place a folder name prefix in the <code>keyName</code> query parameter of the <code>camel.sink.url</code> configuration option above. For example, <code>camel.sink.url: aws-s3://my-s3-bucket?keyName=myfoldername/${date:now:yyyyMMdd-HHmmssSSS}-${exchangeId}</code>.</p>"},{"location":"use-cases/connect-s3/#s3-to-kafka-source-connector","title":"S3 to Kafka Source Connector","text":"<p>Similar to the Kafka to S3 Sink Connector scenario, this scenario will make use of the Strimzi <code>KafkaConnector</code> custom resource to configure the specific connector instance.</p> <p>Review the YAML description for our <code>KafkaConnector</code> custom resource below, named <code>s3-source-connector</code>. Pay close attention to: - The <code>strimzi.io/cluster</code> label must match the deployed Kafka Connect cluster you previously deployed (or else Strimzi will not connect the <code>KafkaConnector</code> to your <code>KafkaConnect</code> cluster) - The <code>topics</code> and <code>camel.source.kafka.topic</code> parameters (named <code>my-target-topic</code> here) - The S3 Bucket parameter of the <code>camel.sink.url</code> configuration option (named <code>my-s3-bucket</code> here)</p> <p>Please note that it is an explicit intention that the topics used in the Kafka to S3 Sink Connector configuration and the S3 to Kafka Source Connector configuration are different. If these configurations were to use the same Kafka topic and the same S3 Bucket, we would create an infinite processing loop of the same information being endlessly recycled through the system. In our example deployments here, we are deploying to different topics but the same S3 Bucket.</p> <pre><code>apiVersion: kafka.strimzi.io/v1alpha1\nkind: KafkaConnector\nmetadata:\n  name: s3-source-connector\n  labels:\n    strimzi.io/cluster: connect-cluster-101\nspec:\n  class: org.apache.camel.kafkaconnector.CamelSourceConnector\n  tasksMax: 1\n  config:\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.camel.kafkaconnector.awss3.converters.S3ObjectConverter\n    topics: my-target-topic\n    camel.source.kafka.topic: my-target-topic\n    camel.source.url: aws-s3://my-s3-bucket?autocloseBody=false\n    camel.source.maxPollDuration: 10000\n    camel.component.aws-s3.accessKey: ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id}\n    camel.component.aws-s3.secretKey: ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key}\n    camel.component.aws-s3.region: US_EAST_1\n</code></pre> <p>Once you have updated the YAML and saved it in a file named <code>kafka-source-connector.yaml</code>, this resource can be created via <code>kubectl apply -f kafka-source-connector.yaml</code>. You can then tail the output of the <code>connect-cluster-101</code> pods for updates on the connector status.</p> <p>NOTE: If you require the connector to only read objects from a subdirecotry of the S3 bucket root, you can set the <code>camel.component.aws-s3.configuration.prefix</code> configuration option with the value of the subdirectory name. For example, <code>camel.component.aws-s3.configuration.prefix: myfoldername</code> .</p>"},{"location":"use-cases/connect-s3/#event-streams-v10-addendum","title":"Event Streams v10 Addendum","text":"<ul> <li>The steps up to now assumed usage of an OpenShift Container Platform v4.3.x cluster, IBM Cloud Pak for Integration v2020.1.1 and IBM Event Streams v2019.4.2</li> <li> <p>With the release of Event Streams v10 (which is built on top of the Strimzi Kafka Operator) and the new CP4I there are some things need that need to be adjusted in the yamls to reflect that.</p> </li> <li> <p>You can edit the previous <code>kafka-connect.yaml</code> or create a new file. kafka-connect-esv10.yaml <pre><code>apiVersion: eventstreams.ibm.com/v1beta1\nkind: KafkaConnectS2I\nmetadata:\n  name: connect-cluster-101\n  annotations:\n    eventstreams.ibm.com/use-connector-resources: \"true\"\nspec:\n  #logging:\n  #  type: external\n  #  name: custom-connect-log4j\n  version: 2.5.0\n  replicas: 1\n  bootstrapServers: {internal-bootstrap-server-address}\n  template:\n    pod:\n      imagePullSecrets: []\n      metadata:\n        annotations:\n          eventstreams.production.type: CloudPakForIntegrationNonProduction\n          productID: {product-id}\n          productName: IBM Event Streams for Non Production\n          productVersion: 10.0.0\n          productMetric: VIRTUAL_PROCESSOR_CORE\n          productChargedContainers: {connect-cluster-101}-connect\n          cloudpakId: {cloudpak-id}\n          cloudpakName: IBM Cloud Pak for Integration\n          cloudpakVersion: 2020.2.1\n          productCloudpakRatio: \"2:1\"\n  tls:\n      trustedCertificates:\n        - secretName: {your-es-instance-name}-cluster-ca-cert\n          certificate: ca.crt\n  authentication:\n    type: tls\n    certificateAndKey:\n      certificate: user.crt\n      key: user.key\n      secretName: {generated-secret-from-ui}\n  externalConfiguration:\n    volumes:\n      - name: aws-credentials\n        secret:\n          secretName: aws-credentials\n  config:\n    group.id: connect-cluster-101\n    config.providers: file\n    config.providers.file.class: org.apache.kafka.common.config.provider.FileConfigProvider\n    key.converter: org.apache.kafka.connect.json.JsonConverter\n    value.converter: org.apache.kafka.connect.json.JsonConverter\n    key.converter.schemas.enable: false\n    value.converter.schemas.enable: false\n    offset.storage.topic: connect-cluster-101-offsets\n    config.storage.topic: connect-cluster-101-configs\n    status.storage.topic: connect-cluster-101-status\n</code></pre></p> </li> <li> <p>There are a couple changes of note here.</p> </li> <li><code>apiVersion: eventstreams.ibm.com/v1beta1</code> instead of the previous Strimzi one.</li> <li><code>metadata.annotations.eventstreams.ibm.com/use-connector-resources: \"true\"</code> instead of the previous Strimzi one as well.</li> <li><code>spec.bootstrapservers:</code> Previously in Event Streams v2019.4.2 and prior there was only a single external facing route for this bootstrap server address. In Event Streams v10 there is an <code>External</code> and <code>Internal</code> one now. Replace <code>{internal-bootstrap-server-address}</code> with the <code>Internal</code> bootstrap server address from the Event Streams v10 UI.</li> <li><code>spec.template.pod.metadata.annotations.*</code> This entire section is new and represents new metadata that the Event Streams instance needs.</li> <li><code>spec.template.pod.metadata.annotations.productID</code> You can find this value when trying to deploy an Event Streams Custom Resource from the Installed Event Streams Operator YAML deployment or when applying this YAML through the command-line, OCP will provide you the proper values if they're wrong.</li> <li><code>spec.template.pod.metadata.annotations.cloudpakId</code> Same as productID.</li> <li><code>spec.tls.trustedCertificates</code> This is different from the previous one in that this certificate was automatically generated on creation of your Event Streams Kafka Cluster. If your Event Streams instance is named <code>eventstreams-dev</code> then this value should be <code>eventstreams-dev-cluster-ca-cert</code>.</li> <li><code>spec.authentication.*</code> This section is mostly different from the previous section. Prior we used a generated API Key, but on Event Streams v10 we will need to generate TLS credentials. In the <code>Internal</code> connection details there will be a <code>Generate TLS Credentials</code> button. Here you can name your secret and provide the proper access to it. Note 1 - This is automatically created by the Event Streams Operator into the same namespace as both a <code>KafkaUser</code> Custom Resource as well as a secret. If your secret is named <code>internal-secret</code> then there will be an automatically generated Kubernetes/OpenShift secret named that as well as a <code>KafkaUser</code>. <code>user.crt</code> and <code>user.key</code> are certificates automatically generated within said secret. Note 2 - Deletion of that secret will replicate itself. You will need to delete the associated and created <code>KafkaUser</code>.</li> </ul> <p>kafka-sink-connector.yaml-esv10.yaml <pre><code>apiVersion: eventstreams.ibm.com/v1alpha1\nkind: KafkaConnector\nmetadata:\n  name: s3-sink-connector\n  labels:\n    eventstreams.ibm.com/cluster: connect-cluster-101\nspec:\n  class: org.apache.camel.kafkaconnector.CamelSinkConnector\n  tasksMax: 1\n  config:\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.kafka.connect.storage.StringConverter\n    topics: my-topic\n    camel.sink.url: aws-s3://my-s3-bucket?keyName=${date:now:yyyyMMdd-HHmmssSSS}-${exchangeId}\n    camel.sink.maxPollDuration: 10000\n    camel.component.aws-s3.configuration.autocloseBody: false\n    camel.component.aws-s3.accessKey: ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id}\n    camel.component.aws-s3.secretKey: ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key}\n    camel.component.aws-s3.region: my-s3-bucket-region\n</code></pre> - For the most part identical besides two minor changes.   - <code>apiVersion: eventstreams.ibm.com/v1alpha1</code> instead of Strimzi.   - <code>metadata.labels.eventstreams.ibm.com/cluster: connect-cluster-101</code> again instead of Strimzi.</p> <p>kafka-source-connector.yaml-esv10.yaml <pre><code>apiVersion: eventstreams.ibm.com/v1alpha1\nkind: KafkaConnector\nmetadata:\n  name: s3-source-connector\n  labels:\n    eventstreams.ibm.com/cluster: connect-cluster-101\nspec:\n  class: org.apache.camel.kafkaconnector.CamelSourceConnector\n  tasksMax: 1\n  config:\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.camel.kafkaconnector.awss3.converters.S3ObjectConverter\n    topics: my-topic\n    camel.source.kafka.topic: my-topic\n    camel.source.url: aws-s3://my-s3-bucket?autocloseBody=false\n    camel.source.maxPollDuration: 10000\n    camel.component.aws-s3.accessKey: ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_access_key_id}\n    camel.component.aws-s3.secretKey: ${file:/opt/kafka/external-configuration/aws-credentials/aws-credentials.properties:aws_secret_access_key}\n    camel.component.aws-s3.region: my-s3-bucket-region\n</code></pre> - Similar to the S3 sink connector yaml.   - <code>apiVersion: eventstreams.ibm.com/v1alpha1</code> instead of Strimzi.   - <code>metadata.labels.eventstreams.ibm.com/cluster: connect-cluster-101</code> again instead of Strimzi.</p>"},{"location":"use-cases/connect-s3/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>The log output from the Kafka Connector instances will be available in the KafkaConnectS2I pods, named in the pattern of <code>{connect-cluster-name}-connect-{latest-build-id}-{random-suffix}</code>. This pattern will be referred to as <code>{connect-pod}</code> throughout the rest of the Troubleshooting section.</p> </li> <li> <p>To increase the logging output of the deployed connector instances:</p> </li> <li>Follow the steps for \"Create ConfigMap for log4j configuration\" in Kafka Connect Cluster above</li> <li>Update the <code>log4j.properties</code> ConfigMap to include the line <code>log4j.logger.org.apache.camel.kafkaconnector=DEBUG</code></li> <li> <p>Reapply the YAML configuration of the KafkaConnectS2I cluster</p> </li> <li> <p>To check the deployment status of applied Kafka Connect configuration via Kafka Connect's REST API, run the following commands:</p> </li> <li><code>oc exec {connect-pod} bash -- -c \"curl --silent http://localhost:8083/connectors/\"</code></li> <li><code>oc exec {connect-pod} bash -- -c \"curl --silent http://localhost:8083/connectors/{deployed-connector-name}/status\"</code></li> </ul>"},{"location":"use-cases/connect-s3/#next-steps","title":"Next steps","text":"<ul> <li>Enable use  of IAM Credentials in the Connector configuration (as the default Java code currently outputs <code>aws_access_key_id</code> and <code>aws_secret_access_key</code> to container runtime logs due to their existence as configuration properties)</li> <li>Optionally configure individual connector instances to startup with offset value of -1 (to enable to run from beginning of available messages)</li> <li>Implement a build system to produce container images with the Camel Kafka Connector binaries already present</li> </ul>"},{"location":"use-cases/connect-s3/#references","title":"References","text":"<ul> <li>Apache Camel Kafka Connector - Try it out on OpenShift with Strimzi</li> <li>Apache Camel - Available pattern elements for use in the <code>keyName</code> parameter of the <code>camel.sink.url</code> property.</li> <li>Apache Camel - Dynamic Endpoint reference</li> <li>Red Hat Developer Blog - Using secrets in Kafka Connect configuration</li> <li>Apache Kafka - Connect Overview</li> </ul>"},{"location":"use-cases/db2-debezium/","title":"DB2 Change Data Capture with Debezium","text":"<p>This lab goes over how to implement a change data capture on order events table created using the outbox pattern with the Debezium open source project.</p> <p>What you will learn is:</p> <ul> <li>DB2 settings for change data capture</li> <li>Configuring Debezium DB2 connector to publish OrderEvents to Kafka topic</li> <li>Validate the Kafka topic content.</li> </ul> <p>We expect existing knowledge of Kafka, and Kafka connector.</p> <p> Updated 11/13/2020 Ready for validation when running local. Need to be completed for OpenShift Deployment. </p>"},{"location":"use-cases/db2-debezium/#quick-summary-of-debezium","title":"Quick summary of Debezium","text":"<p>Debezium is an open source project, led by Red Hat, to support capturing changes to a database and generate those changes to Kafka. It runs in Kafka Connect so support High availability and horizontal scaling. </p> <p>To get started we recommend going into the tutorial, review the product documentation and for deeper dive you can leverage the Debezium examples. </p> <p>In an data pipeline architecture, Change Data Capture, helps to inject existing data from existing Database to Kafka and the event-driven microservice. It is important to note that the data generated will be close to what is in the data base, it is possible to do some data transformation to generate some 'business event' from the database updates. Or use raw data and add a Kafka Streams processing to do the data transformation. </p> <p>Debezium supports DB2 as data source as introduced by this project. As part of the Debezium tutorial in the Debezium examples, you can find a docker compose to start DB2 and Debezium.</p> <p>For most of development effort, we are using Docker Compose to run a basic infrastructure with Kafka and Kafka Connect. </p> <p>Once DB server and Kafka Connect are started, the approach is to register the DB connector using a json file like below. CDC uses a specific schema to keep source table update. We will detail that in next section.</p> <pre><code> {\n   \"name\": \"order-connector\",\n   \"config\": {\n       \"connector.class\" : \"io.debezium.connector.db2.Db2Connector\",\n       \"tasks.max\" : \"1\",\n       \"database.server.name\" : \"vaccine_lot_db\",\n       \"database.hostname\" : \"db2\",\n       \"database.port\" : \"50000\",\n       \"database.user\" : \"db2inst1\",\n       \"database.password\" : \"db2inst1\",\n       \"database.dbname\" : \"TESTDB\",\n       \"database.cdcschema\": \"ASNCDC\",\n       \"database.history.kafka.bootstrap.servers\" : \"kafka:9092\",\n       \"database.history.kafka.topic\": \"db_history_vaccine_orders\",\n       \"topic.creation.default.replication.factor\": 1,  \n       \"topic.creation.default.partitions\": 1,  \n       \"topic.creation.default.cleanup.policy\": \"compact\", \n       \"table.include.list\" : \"DB2INST1.ORDEREVENTS\",\n       \"tombstones.on.delete\" : \"false\"\n   }\n }\n</code></pre>"},{"location":"use-cases/db2-debezium/#db2-connector","title":"DB2 connector","text":"<p>The  project documentation presents in detail this connector, but below is a quick summary of the features:</p> <ul> <li>Tables to monitor are in capture mode, so they have associated chage data table. </li> <li>The Db2 connector reads change events from change-data tables and emits the events to Kafka topics.</li> <li>The Debezium Db2 connector is based on the ASN Capture/Apply agents. A capture agent:</li> <li>Generates change-data tables for tables that are in capture mode.</li> <li>Monitors tables in capture mode and stores change events for updates to those tables in their corresponding change-data tables.</li> <li>A user defined function is needed to start or stop the ADN agent, put expected tables in capture mode, create the ASN schema abd change data tables. </li> <li>The connector emits a change event for each row-level insert, update, and delete operation to a Kafka topic that has the same name as the changed table.</li> <li>When the Db2 connector first connects to a particular Db2 database, it starts by performing a consistent snapshot of each table that is in capture mode</li> <li>The connector keeps the log sequence number (LSN) of the change data table entry.</li> <li>Database schema is also replicated so it supports schema updates</li> <li>Each event contains the structure of its key and the payload. Or a reference for a schema registry entry.</li> </ul>"},{"location":"use-cases/db2-debezium/#use-case-overview","title":"Use Case overview","text":"<p>The use case is part of a larger scenario about order vaccines management. Vaccine orders are managed by an order microservice and using the outbox pattern order created and order updated events are produced to a specific table which is captured by the Debezium connector.</p> <p></p> <p>The implementation of the outbox is done using Quarkus Debezium outbox extension and explained in this separate note.</p> <p>In this lab, you will get the component running on you computer or on OpenShift.</p>"},{"location":"use-cases/db2-debezium/#run-locally","title":"Run locally","text":"<p>Clone the order management service:</p> <pre><code>git clone https://github.com/ibm-cloud-architecture/vaccine-order-mgr\n</code></pre> <p>And then start the five processes (one Kafka broker, one ZooKeeper, one DB2 container for the persistence, one Kafka Connect with the Debezium code and DB2 JDBC driver and one vaccine-order-service) with Docker Compose.</p> <pre><code>cd environment\n# with the option to build the db2, and debezium cdc container images\ndocker-compose -f strimzi-docker-compose.yaml up -d --build\n# or with pre-existing images coming from dockerhub\ndocker-compose -f strimzi-docker-compose.yaml  up -d\n</code></pre> <ul> <li>Create the needed topics</li> </ul> <pre><code>  # Under environment folder\n  ./createTopic.sh\n  # validate topics created\n  ./listTopics.sh\n\n  __consumer_offsets\n  db_history_vaccine_orders\n  src_connect_configs\n  src_connect_offsets\n  src_connect_statuses\n  vaccine_shipmentplan\n</code></pre> <p>The <code>db_history_vaccine_orders</code> is the topic used to include database schema change on the vaccine orders table. </p>"},{"location":"use-cases/db2-debezium/#verify-starting-states","title":"Verify starting states","text":"<p>To validate the DB2 settings, you can do one of the following troubleshooting commands:</p> <pre><code># connect to DB2 server\ndocker exec -ti db2 bash\n# Access the database \ndb2 connect to TESTDB USER DB2INST1\n# use db2inst1 as password\n# list existing schemas\n db2 \"select * from syscat.schemata\"\n# list tables\ndb2 list tables\n# this is the outcomes if the order services was started\nTable/View                      Schema          Type  Creation time             \n------------------------------- --------------- ----- --------------------------\nORDERCREATEDEVENT               DB2INST1        T     2020-11-12-01.50.10.400490\nORDEREVENTS                     DB2INST1        T     2020-11-12-01.50.10.650172\nORDERUPDATEDEVENT               DB2INST1        T     2020-11-12-01.50.10.796566\nVACCINEORDERENTITY              DB2INST1        T     2020-11-12-01.50.10.874172\n# Verify the content of the current orders\ndb2 \"select * from vaccineorderentity\"\n# List the table for the change data capture\ndb2 list tables for schema asncdc\n</code></pre> <p>The DB2 container was built to define ASNCDC schema to support table capture. The setup is described in this note and supported by this script.</p> <p>When reusing this asset, the only thing you need to configure is the startup-cdc.sql to specify the table(s) you want to capture.</p> <pre><code>VALUES ASNCDC.ASNCDCSERVICES('status','asncdc');\nCALL ASNCDC.ADDTABLE('DB2INST1', 'ORDEREVENTS' ); \nVALUES ASNCDC.ASNCDCSERVICES('reinit','asncdc');\n</code></pre> <p>and tune the content of the <code>register-db2.json</code> file to configure the Kafka Connector (see next section). </p> <p>The application may have some issue to start as DB2 may take some time to configure, so it is important to verify the containers running (if you know how to add healthcheck on DB2 container... open a PR):</p> <pre><code>docker ps \nCONTAINER ID         IMAGES                             NAMES\n5ddd45b5856e        ibmcase/vaccineorderms             vaccineorderms\n5bce18c820fc        ibmcase/cdc-connector              cdc-connector\n7fd6951972df        strimzi/kafka:latest-kafka-2.6.0   kafka\nb0f9127c874e        strimzi/kafka:latest-kafka-2.6.0   zookeeper\n7f356633ea2f        ibmcase/db2orders                  db2\n# Get come logs using the container name\ndocker logs vaccineorderms\n</code></pre> <p>If for any reason the <code>vaccineorderms</code>, doing a <code>docker-compose up -d</code> will restart the container.</p> <p>You can use the User interface to get the current order loaded. At the starting time there should be only one record. http://localhost:8080/#/Orders.</p> <p></p>"},{"location":"use-cases/db2-debezium/#define-the-cdc-connector","title":"Define the CDC connector","text":"<p>Deploy and start the Debezium DB2 connector. The connector definition is in register-db2.json. The important elements of this file are below:</p> <pre><code># the namespace for the server that will be used in the topic created\n\"database.server.name\" : \"vaccine_lot_db\",\n# database credentials\n# The list of table to capture\n  \"table.include.list\" : \"DB2INST1.ORDEREVENTS\",\n# name for the topic to keep track of the database schema changes.\n \"database.history.kafka.topic\": \"db_history_vaccine_orders\",  \n</code></pre> <p>To deploy to the Kafka Connector instance, perform a POST with the previous configuration as:</p> <pre><code># under environment/cdc\ncurl -i -X POST -H \"Accept:application/json\" -H  \"Content-Type:application/json\" http://localhost:8083/connectors/ -d @cdc/register-db2.json\n</code></pre> <ul> <li> <p>Get the status of the Kafka connector at:  http://localhost:8083/connectors/orderdb-connector/</p> </li> <li> <p>Verify the newly created topics:</p> </li> </ul> <pre><code>  ./listTopics.sh \n  vaccine_lot_db\n  vaccine_lot_db.DB2INST1.ORDEREVENTS\n</code></pre> <p>The newly created <code>vaccine_lot_db</code> topic includes definition of the database and the connector. It does not aim to be used by application. The one to be used to get business events is <code>vaccine_lot_db.DB2INST1.ORDEREVENTS</code>.</p> <p>The connector is doing a snapshot of the <code>DB2INST1.ORDEREVENTS</code> table to send existing records to the topic.</p>"},{"location":"use-cases/db2-debezium/#start-consumer","title":"Start consumer","text":"<p>Start a Kafka consumer, using the console consumer tool: </p> <pre><code>docker-compose exec kafka /opt/kafka/bin/kafka-console-consumer.sh     --bootstrap-server kafka:9092     --from-beginning     --property print.key=true     --topic db2server.DB2INST1.ORDERS\n</code></pre>"},{"location":"use-cases/db2-debezium/#create-an-order","title":"Create an order","text":"<p>You can use the user interface to add a new order, </p> <p></p> <p>or use the swagger operation at http://localhost:8080/swagger-ui/#/default/post_api_v1_orders</p> <p>Use the following JSON:</p> <pre><code>{\n   \"deliveryDate\": \"2021-07-25\",\n   \"deliveryLocation\": \"Milano\",\n   \"askingOrganization\": \"Italy gov\",\n   \"priority\": 1,\n   \"quantity\": 100,\n   \"type\": \"COVID-19\"\n}\n</code></pre> <p>The expected result in the topic consumer should have the following records in the Kafka topic:</p> <pre><code>{\"ID\":\"lvz4gYs/Q+aSqKmWjVGMXg==\"}  \n{\"before\":null,\"after\":{\"ID\":\"lvz4gYs/Q+aSqKmWjVGMXg==\",\"AGGREGATETYPE\":\"VaccineOrderEntity\",\"AGGREGATEID\":\"21\",\"TYPE\":\"OrderCreated\",\"TIMESTAMP\":1605304440331350,\"PAYLOAD\":\"{\\\"orderID\\\":21,\\\"deliveryLocation\\\":\\\"London\\\",\\\"quantity\\\":150,\\\"priority\\\":2,\\\"deliveryDate\\\":\\\"2020-12-25\\\",\\\"askingOrganization\\\":\\\"UK Governement\\\",\\\"vaccineType\\\":\\\"COVID-19\\\",\\\"status\\\":\\\"OPEN\\\",\\\"creationDate\\\":\\\"13-Nov-2020 21:54:00\\\"}\"},\"source\":{\"version\":\"1.3.0.Final\",\"connector\":\"db2\",\"name\":\"vaccine_lot_db\",\"ts_ms\":1605304806596,\"snapshot\":\"last\",\"db\":\"TESTDB\",\"schema\":\"DB2INST1\",\"table\":\"ORDEREVENTS\",\"change_lsn\":null,\"commit_lsn\":\"00000000:0000150f:0000000000048fca\"},\"op\":\"r\",\"ts_ms\":1605304806600,\"transaction\":null}\n</code></pre>"},{"location":"use-cases/db2-debezium/#references","title":"References","text":"<ul> <li>Outbox pattern for quarkus</li> <li>Blog on the outbox pattern</li> <li>Db2 Debezium connector</li> </ul>"},{"location":"use-cases/gitops/","title":"Event-driven solution GitOps approach","text":"<p>Info</p> <p>Updated 2/15/2022</p> <p>Audience: Architects, Application developers, Site reliability engineers, Administrators</p>"},{"location":"use-cases/gitops/#overview","title":"Overview","text":"<p>The purpose of the tutorial is to teach architects, developers and operations staff how to deploy a production-ready  event-driven solution  OpenShift Container Platform. It makes extensive use of the IBM Cloud Pak for Integration (CP4I) and other cloud native technologies such as Tekton, Kustomize, ArgoCD, Prometheus, Grafana and Kibana.</p> <p>GitOps is a declarative way to implement continuous deployment for cloud-native applications. The Red Hat\u00ae OpenShift\u00ae Container  Platform offers the OpenShift GitOps operator,  which manages the entire lifecycle for Argo CD and its components.</p> <p>Argo applications are added to the Argo CD server. An application defines the source of the Kubernetes resources and the target cluster where  those resources should be deployed. The Argo CD server \"installs\" a Cloud Pak by synchronizing the applications representing the Cloud Pak into  the target cluster.</p>"},{"location":"use-cases/gitops/#system-context","title":"System context","text":"<p>A system context diagram helps us understand how our system interacts with its different users and other systems. For a generic event-driven solution the diagram looks like</p> <p></p> <p>We can see the different entities that interact with a typical event-driven solution deployment. These include users as well as applications and messaging as a service infrastructure which includes event backbone, queueing systems, schema registry, API management, governance platform and monitoring components.</p> <p>We'll be developing the Event-driven solution deployment at the centre of the diagram. We can see that it connects applications to systems and infrastructure. Its users are at least developers, SREs, Kubernetes administrators, architects...</p>"},{"location":"use-cases/gitops/#components-for-gitops","title":"Components for GitOps","text":"<p>The following diagram shows the technical components used in a typical event-driven solution production deployment.</p> <p></p> <p>(the source code of this diagram is a <code>`.drawio</code> format and is in the diagrams folder.)</p> <p>The diagram organizes the components according to when they are introduced in system development (earlier or later)  and whether they are a relatively high level application-oriented component, or a relatively low level system- oriented component. For example, GitHub is a system component that is fundamental to how we structure the event-driven solution deployment.  In contrast, streaming or event-driven applications are higher level components, and requires other components to be deployed prior to them.</p> <p>The color coding illustrates that blue components are part of the solution, red are part of the GitOps on OpenShift and green components are externals to OpenShit cluster, most likely event if they could be. Kustomize represents way to define deployment, and Sealed secret is a service to manage secrets.</p> <p>As part of the later components to deploy, we have addressed everything to monitor the solution and the infrastructure. </p> <p>Here is a brief introduction of those components:</p>"},{"location":"use-cases/gitops/#event-driven-applications","title":"Event-driven applications","text":"<p>Those applications are supporting business logic, microservice based, and using Reactive messaging, MQ or Kafka APIs. Those applications  provide OpenAPIs to the mobile or web applications but also AsyncAPI when they produce events to Kafka or messages to MQ. OpenAPI and AsyncAPI definitions are managed by API manager and event end-point manager. </p> <p>Schema definitions are managed by a Schema Registry.</p>"},{"location":"use-cases/gitops/#event-streaming-applications","title":"Event-streaming applications","text":"<p>Those applications are also supporting business logic, but more with stateful processing using Kafka Stream APIs or  different product such as Apache Flink.</p>"},{"location":"use-cases/gitops/#queue-manager","title":"Queue manager","text":"<p>A queue manager provides queueing services via one of the many MQ APIs. A queue manager hosts the queues that store the messages produced and consumed by connected applications and systems. Queue managers can be connected together via network channels to allow messages to flow between disparate systems and applications on different platforms including on-premise and cloud systems.</p>"},{"location":"use-cases/gitops/#openshift-gitops-or-argocd","title":"OpenShift GitOps or ArgoCD","text":"<p>OpenShift GitOps (ArgoCD) is used for the continuous deployment of software components to the Kubernetes cluster. OpenShift GitOps watches a Git repository for new or changed Kubernetes resource definitions, and applies them to a cluster. In this way, OpenShift GitOps ensures that the component configuration stored in GitHub always reflects the state of the cluster.</p> <p>OpenShift GitOps also has the added benefit of being able to monitor resources that it has deployed to ensure that if they drift from their desired values, they will be automatically restored to those values by OpenShift GitOps.</p>"},{"location":"use-cases/gitops/#openshift-pipelines-or-tekton","title":"OpenShift Pipelines or Tekton","text":"<p>OpenShift Pipelines (Tekton) is used to automate manual tasks using the concept of a pipeline. A pipeline comprises a set of tasks that are executed in a specified order in order to accomplish a specific objective.</p> <p>We use pipelines as part of the continuous integration process to build, test and deliver event-driven applications ready for deployment by OpenShift GitOps.</p>"},{"location":"use-cases/gitops/#queue-manager_1","title":"Queue manager","text":"<p>A queue manager provides queueing services via one of the many MQ APIs. A queue manager hosts the queues that store the messages produced and consumed by connected applications and systems. Queue managers can be connected together via network channels to allow messages to flow between disparate systems and applications on different platforms including on-premise and cloud systems.</p>"},{"location":"use-cases/gitops/#sealed-secrets","title":"Sealed secrets","text":"<p>Very often a component has a Kubernetes secret associated with it. Inside the secret might be a private key to access the IBM entitled container registry, for example.  For obvious reasons, we don't want to store the secrets in GitHub with the rest of the configuration.</p> <p>A sealed secret solves this problem by introducing a new kind of Kubernetes resource. A sealed secret is created from a regular secret, and can be safely stored in a Git repository. A deployment time, the sealed secret controller will recreate the secret in its original form so that it can be access by components with the appropriate RBAC authority.</p>"},{"location":"use-cases/gitops/#image-registry","title":"Image Registry","text":"<p>OpenShift contains a registry for storing container images. Images are built and stored by OpenShift Pipelines  as part of the CICD process. Tekton pipelines and ArgoCD also retrieve the latest best images from the image registry to ensure that what's being tested or deployed in higher environments is the same as what's tested in development environments.</p> <p>We often refer to uploading images as pushing and downloading images as pulling.</p>"},{"location":"use-cases/gitops/#cert-manager","title":"Cert manager","text":"<p>Managing certificates is a difficult process; certificate creation requires a Certificate Authority (CA), certificates expire after a period of time, and private keys can sometimes be compromised -- requiring a certificate to be revoked and a new one issued.</p> <p>Cert manager makes all these processes relatively straightforward by introducing new Kubernetes resources for certificate issuers and certificates. These resource types radically simplify the management of certificates: their creation, expiry and revocation.</p> <p>Moreover, Cert manager makes it feasible to adopt mutual TLS (mTLS) as an authorization strategy in Kafka based solution.</p>"},{"location":"use-cases/gitops/#prometheus","title":"Prometheus","text":"<p>Prometheus is used in conjunction with Grafana.  It stores the different component's metrics as a set of tuples in a time-series , which allows it to be subsequently used to create Grafana views to assist with monitoring Kafka brokers, MQ queue managers, schema registry....</p>"},{"location":"use-cases/gitops/#kustomize","title":"Kustomize","text":"<p>Kubernetes resources have their operational properties defined using YAMLs. As these resources move through environments such as dev, stage and prod, Kustomize provides a natural way to adapt (customize!) these YAMLs to these environments.  For example, we might want to change the CPU or memory available to a service in a production environment compared to a development environment.</p> <p>Because Kustomize is built into the <code>kubectl</code> and <code>oc</code> commands via the <code>-k</code> option, it makes configuration management both easy and natural.</p>"},{"location":"use-cases/gitops/#github","title":"GitHub","text":"<p>This popular version control system is based on git and stores the event-driven application source code and configuration as well as the other Kubernetes resources. By keeping our event-driven applications and  configurations in Git, and using that to build, test and deploy our applications to the Kubernetes cluster, we have a single source of truth -- what's in Git is running in the cluster.</p> <p>Moreover, by using Git operations such as pull, push and merge to make changes, we can exploit the extensive governance and change control provided by Git when managing our event-driven solution estate.</p>"},{"location":"use-cases/gitops/#openshift-kubernetes-cluster","title":"OpenShift (Kubernetes) Cluster","text":"<p>This is the \"operating system\" used to orchestrate our applications and related component containers. Kubernetes is portable across on-premise and cloud systems and allows us to easily scale our workloads across these environments as required.</p>"},{"location":"use-cases/gitops/#high-level-architecture-view","title":"High-level architecture view","text":"<p>Using a GitOps approach we can design a high-level architecture view for the deployment of all the previously listed components.  It is important to recall that most of the RedHat and IBM products used in event-driven solution are using Operators and Custom resources manifests to deploy operands.</p> <p>Operator is a long running process that performs products (Operands) deployment and Day 2 operations, like upgrades, failover, or scaling.  Operator is constantly watching your cluster\u2019s desired state for the software installed.  Helm and Operators represent two different phases in managing complex application workloads  on Kubernetes. Helm\u2019s primary focus is on the day-1 operation of deploying Kubernetes  artifacts in a cluster. The \u2018domain\u2019 that it understands is that of Kubernetes YAMLs that  are composed of available Kubernetes Resources / APIs. Operators, on the other hand, are  primarily focused on addressing day-2 management tasks of stateful / complex workloads  such as Postgres, Cassandra, Spark, Kafka, SSL Cert Mgmt etc. on Kubernetes.</p> <p>Operator Lifecycle Manager (OLM) helps you to deploy, and update, and generally  manage the lifecycle of all of the Operators (and their associated services) running  across your clusters</p> <p>The Operators deployment is part of a bootstraping step of the GitOps process. We are using a special  Git repository to manage a catalog of operator definitions/ subscriptions. This is the goal of the eda-gitops-catalog repository.</p> <p>A solution will have a specific gitops repository that manages services (operands) and application specifics deployment manifests.</p> <p>With this base, the following figure illustrates a potential architecture:</p> <p></p> <p>(the source code of this diagram is a <code>`.drawio</code> format and is in the diagrams folder.)</p> <p>Here are the assumptions we define for any event-driven solution:</p> <ul> <li>Single admin team for OCP cluster and production projects within the cluster.</li> <li>Developers manages staging and dev environment. This is a functional team developing the solution</li> <li>For the solution one gitops will define all environments and apps/services of the solution.  </li> <li>Developers will not have access to OpenShift cluster administration</li> <li>Cloud Pak for integration operators are installed in all namespaces, and there is only one instance of each operator. </li> <li>Only one Platform Navigator installed per cluster (in all namespaces) and it displays instances of  capabilities from the whole cluster.</li> <li><code>ibm-common-services</code> is unique to the cluster. </li> </ul> <p>For real production deployment, the production OpenShift cluster will be separated from dev and staging, running in different infrastructure, but using the same github source. The top-right cluster is for dev and staging, and each of those environmentd will be in different namespace. To enforce isolation and clear separation of concern, each of those <code>dev</code> or <code>staging</code> namespace, may have Event Streams, MQ brokers, schema registry deployed.</p> <p>The <code>openshift-operators</code> is used to deploy Operators that manage multiple namespaces. The <code>openshift-gitops</code> is for the ArgoCD server and the ArgoCD apps.</p>"},{"location":"use-cases/gitops/#gitops-model","title":"GitOps Model","text":"<p>Gitops is a way of implementing Continuous Deployment for containerized applications.</p> <p>The core idea of GitOps is having a Git repository that always contains declarative descriptions  of the infrastructure currently desired in the production environment and an automated process  to make the production environment matches the described state in the repository.</p> <p>From the diagram above, we can see two main components that are essentials to a production-ready event-driven solution deployment:</p> <ul> <li> <p>A Kubernetes cluster containing:</p> <ul> <li>event-driven applications per namespace / project to facilitate isolation</li> <li>Event Streams, API mangement, MQ operators per namespace / project </li> <li>Kafka brokers, Kafka connect, mirror maker, schema registry, End point event gateway, API manager, MQ queue managers per namespace / project </li> <li>OpenShit GitOps, Pipelines,...</li> </ul> </li> <li> <p>GitHub as a source of truth for the cluster runtime containing:</p> <ul> <li>application source with schema definitions</li> <li>application configuration</li> <li>Kafka Cluster configuration</li> <li>Topic configuration</li> <li>OpenAPI and AsyncAPI documents</li> <li>Kafka Connnector configuration</li> <li>Mirror maker configuration</li> <li>Queue manager configuration</li> </ul> </li> </ul>"},{"location":"use-cases/gitops/#github-repositories","title":"GitHub repositories","text":"<p>We propose to use one GitOps Catalog to centralize the Operator subscription definitions with Kustomize overlays to control operator versioning.  An example of such catalog is the eda-gitops-catalog. Each operator is defined with the subscription manifest and then overlays to change the product version. Here is an example for Event Streams:</p> <pre><code>\u251c\u2500\u2500 event-streams\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 README.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 operator\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 base\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 subscription.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 overlays\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u251c\u2500\u2500 v2.3\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 patch-channel.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u251c\u2500\u2500 v2.4\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 patch-channel.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 v2.5\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u2514\u2500\u2500 patch-channel.yaml\n</code></pre> <p>The subscription.yam is classical operator definition:</p> <pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: ibm-eventstreams\n  namespace: openshift-operators\nspec:\n  channel: stable\n  name: ibm-eventstreams\n  installPlanApproval: Automatic\n  source: ibm-operator-catalog\n  sourceNamespace: openshift-marketplace\n</code></pre> <p>And then each overlays use this manifest as a base resource and apply patch for channel and version:</p> <pre><code># kustomization.yaml under an overlays\nbases:\n  - ../../base\n\npatchesJson6902:\n- path: patch-channel.yaml\n  target:\n    kind: Subscription\n    name: ibm-eventstreams\n</code></pre> <pre><code># patch-channel.yaml\n- op: replace\n  path: /metadata/namespace\n  value: cp4i\n- op: replace\n  path: /spec/channel\n  value: v2.5\n- op: replace\n  path: /spec/startingCSV\n  value: ibm-eventstreams.v2.5.1\n</code></pre> <p>The second major GitOps will be for the solution itself. We use KAM CLI to bootstrap its creation. KAM's goal is to help creating a GitOps project for an existing application as  day 1 operations  and then add more services as part of <code>day 2 operation</code>.</p>"},{"location":"use-cases/gitops/#examples-of-solution-gitops","title":"Examples of solution GitOps","text":"<p>The following solution GitOps repositories are illustrating the proposed approach:</p> <ul> <li>refarch-kc-gitops: For the shipping fresh food overseas solution we have defined. It includes the SAGA choreography pattern implemented with Kafka</li> <li>eda-kc-gitops: For the shipping fresh food overseas solution we have defined. It includes the SAGA orchestration pattern implemented with MQ</li> <li>eda-rt-inventory-gitops to deploy the demo of real-time inventory</li> </ul> <p>As a lab example, you may want to clone the real-time inventory demo and bootstrap the GitOps apps to deploy services and apps:</p> <pre><code>git clone https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops\n</code></pre> <p>See the main readme for up to date instructions. All the images are in the public image registry: <code>quay.io/ibmcase</code></p>"},{"location":"use-cases/kafka-mm2/","title":"Kafka Mirror Maker 2 Hand-on Labs","text":"<p>Info</p> <p>The last up to date mirror maker 2 enablement is in the tech academy content.</p>"},{"location":"use-cases/kafka-mm2/#pre-requisites","title":"Pre-requisites","text":"<p>The following steps need to be done to get the configurations for the different scenario and the docker-compose file to start a local cluster.</p> <ol> <li>Clone the lab repository</li> </ol> <p><pre><code>git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\n</code></pre> 1. Get Docker desktop and docker compose on your local computer.</p>"},{"location":"use-cases/kafka-mm2/#kafka-mirror-maker-2-labs","title":"Kafka Mirror Maker 2 Labs","text":"<p>These hands-on labs walk you through building and testing Kafka Mirror Maker 2 replication.</p> Scenario Description Link Lab 1 Replicate from Event Streams on Cloud to local Kafka. Kafka Mirror Maker 2 - Lab 1 Lab 2 Replicate from Event Streams CP4I to local Kafka. Kafka Mirror Maker 2 - Lab 2 Lab 3 Using Mirror Maker 2 Active Passive mirroring Kafka Mirror Maker 2 - Lab 3"},{"location":"use-cases/kafka-mm2/lab-1/","title":"Mirror Maker 2 - Event Streams Service to Local Kafka - Lab 1","text":"<p>Updated 01/08/2021</p>"},{"location":"use-cases/kafka-mm2/lab-1/#overview","title":"Overview","text":"<p>For this scenario the source cluster is an Event Streams on IBM Cloud as managed service, and the target is a local Kafka cluster running with docker compose: it simulates an on-premise deployment. The figure below presents the components used:</p> <p></p> <ol> <li>Mirror maker 2 runs in standalone mode on local server</li> <li>A python producer write to <code>products</code> topic defined on Evenstreams on IBM Cloud </li> <li>A consumer using Kafka console consumer tool to validate the replicated records from the 'products' topic</li> </ol> <p>As a pre-requisite you need to run your local cluster by using the docker compose as introduced in this note.</p>"},{"location":"use-cases/kafka-mm2/lab-1/#start-the-local-kafka-cluster","title":"Start the local Kafka cluster","text":"<p>In the <code>refarch-eda-tools/labs/mirror-maker2/es-ic-to-local</code> folder there is a docker compose file to start a local three brokers cluster with one Zookeeper node.</p> <ol> <li>In one Terminal window, start the local cluster using the command:</li> </ol> <pre><code>docker-compose up -d\n</code></pre> <p>The data are persisted on the local disk within the folder named <code>kafka-data</code>.</p> <p>Your local environment is up and running.</p>"},{"location":"use-cases/kafka-mm2/lab-1/#start-mirror-maker-2","title":"Start Mirror Maker 2","text":"<ol> <li>Rename the <code>.env-tmpl</code> file to <code>.env</code></li> <li>From Event Streams on Cloud &gt; Service Credentials, get the brokers address and the APIKEY. If needed read this note.</li> <li>If not done already, create a <code>products</code> topic (with one partition) in the EventStreams on Cloud cluster using the management console. See this note if needed, to see how to do it.</li> <li>Modify this <code>.env</code> file to set environment variables for Source Event Streams cluster brokers address and APIKEY.</li> </ol> <p><pre><code>ES_IC_BROKERS=broker-0-q.....cloud.ibm.com:9093\nES_IC_USER=token\nES_IC_PASSWORD=\"&lt;replace with apikey from event streams service credentials&gt;\"\nES_IC_SASL_MECHANISM=PLAIN\nES_IC_LOGIN_MODULE=org.apache.kafka.common.security.plain.PlainLoginModule\n</code></pre> 1. To configure Mirror Maker 2 in standalone mode, we need to define a <code>mm2.properties</code> file. We have define a template file which will be used by the script that launch Mirror Maker 2. The template looks like the following declaration:</p> <pre><code>```properties\nclusters=es-ic, target\nes-ic.bootstrap.servers=KAFKA_SOURCE_BROKERS\ntarget.bootstrap.servers=KAFKA_TARGET_BROKERS\n\nes-ic.security.protocol=SASL_SSL\nes-ic.ssl.protocol=TLSv1.2\nes-ic.ssl.endpoint.identification.algorithm=https\nes-ic.sasl.mechanism=SOURCE_KAFKA_SASL_MECHANISM\nes-ic.sasl.jaas.config=SOURCE_LOGIN_MODULE required username=KAFKA_SOURCE_USER password=KAFKA_SOURCE_PASSWORD;\nsync.topic.acls.enabled=false\nreplication.factor=1\ninternal.topic.replication.factor=1\nes-ic.offset.storage.topic=mm2-cluster-offsets\nes-ic.configs.storage.topic=mm2-cluster-configs\nes-ic.status.storage.topic=mm2-cluster-status\n# enable and configure individual replication flows\nes-ic-&gt;target.enabled=true\nes-ic-&gt;target.topics=products\n```\n</code></pre> <p>A lot of those properties are for the security settings. The <code>clusters</code> property defines the alias name for the source to target, and then the <code>es-ic-&gt;target.*</code> properties define the topic to replicate...</p> <ol> <li> <p>Start Mirror Maker2 using the launch script:</p> <pre><code># In the  es-ic-to-local folder\n./launchMM2.sh\n</code></pre> <p>This script updates the properties file from the environment variables defined in the <code>.env</code> file and starts a Kafka container with a command very similar as:</p> <pre><code> docker run -ti --network es-ic-to-local_default -v $(pwd):/home -v $(pwd)/mirror-maker-2/logs:/opt/kafka/logs strimzi/kafka:latest-kafka-2.6.0 /bin/bash -c \"/opt/kafka/bin/connect-mirror-maker.sh /home/mirror-maker-2/es-to-local/mm2.properties\"\n</code></pre> <p>The <code>mm2.properties</code> file is mounted in the <code>/home</code> within the container.  The network argument is important to get the host names resolved and the connector to connect to Kafka Brokers</p> </li> <li> <p>Verify the MM2 topics are created:</p> </li> </ol> <pre><code>docker exec -ti kafka1 /bin/bash -c \"/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9091 --list\"\n\n __consumer_offsets\n es-ic.checkpoints.internal\n es-ic.products\n es-ic.source.heartbeats\n heartbeats\n mm2-configs.es-ic.internal\n mm2-offsets.es-ic.internal\n mm2-status.es-ic.internal\n</code></pre>"},{"location":"use-cases/kafka-mm2/lab-1/#understanding-mirrormaker-2-trace","title":"Understanding MirrorMaker 2 trace","text":"<p>A lot of configuration validation at the beginning of the trace to get the connection to both clusters. Any configuration issue to define the connection is generally well reported. URL, TLS certificate, secrets are the common cause of connection issues.</p> <p>If some messages happen with NO_LEADER for one of the topics, this means MM2 is not able to create the topic on the target cluster and so it is needed to create the topic with command or User Interface.</p> <p>Then we can observe the following:  * It creates a producer to the target cluster for the offsets topics: <code>[Producer clientId=producer-1] Cluster ID: Bj7Ui3UPQaKtJx7HOkWxPw</code>  * It creates consumer for the 25 offset topic partitions: <code>[Consumer clientId=consumer-mirrormaker2-cluster-1, groupId=mirrormaker2-cluster] Subscribed to partition(s): mirrormaker2-cluster-offsets-0,....</code>  * One Kafka connect worker is started: <code>Worker started ... Starting KafkaBasedLog with topic mirrormaker2-cluster-status</code>  * Create a producer and consumers for the <code>mirrormaker2-cluster-status</code> topic for 5 partitions  * Create another producer and consumer for the <code>mirrormaker2-cluster-config</code> topic</p> <ul> <li>Create WorkerSourceTask{id=es-1-&gt;es-ic.MirrorHeartbeatConnector-0} is the connector - task for the internal HeartBeat.</li> <li>WorkerSourceTask{id=es-1-&gt;es-ic.MirrorSourceConnector-0} for the topic to replicate</li> </ul>"},{"location":"use-cases/kafka-mm2/lab-1/#start-consumer-from-target-cluster","title":"Start consumer from target cluster","text":"<p>Use Apache Kafka tool like Console consumer to trace the message received on a topic</p> <pre><code>docker exec -ti kafka2 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic es-ic.products --from-beginning\" \n</code></pre>"},{"location":"use-cases/kafka-mm2/lab-1/#start-producer-to-source-cluster","title":"Start Producer to source cluster","text":"<p>We are reusing a python environment as defined in the integration tests for the 'kcontainer' solution. https://hub.docker.com/r/ibmcase/kcontainer-python.</p> <p>This time the script is producing products data. Here are the steps to send 5 records.</p> <pre><code># in the es-ic-to-local folder\n./sendProductRecords.sh\n</code></pre> <p>The traces should look like:</p> <pre><code>[KafkaProducer] - {'bootstrap.servers': 'broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '..hidden...'}\n{'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2}\n{'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2}\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n</code></pre> <p>Validate the replication is done from the consumer terminal</p> <p><pre><code>{ \"product_id\": \"P01\", \"description\": \"Carrots\", \"target_temperature\": 4,\"target_humidity_level\": 0.4,\"content_type\": 1}\n{ \"product_id\": \"P02\", \"description\": \"Banana\", \"target_temperature\": 6,\"target_humidity_level\": 0.6,\"content_type\": 2}\n{ \"product_id\": \"P03\", \"description\": \"Salad\", \"target_temperature\": 4,\"target_humidity_level\": 0.4,\"content_type\": 1}\n</code></pre> Stop producer with Ctrl C.</p>"},{"location":"use-cases/kafka-mm2/lab-1/#clean-up","title":"Clean up","text":"<p>You are done with the lab, to stop everything:</p> <pre><code># stop mirror maker 2\ndocker stop mm2\n# stop local kafka cluster\ndocker-compose down\nrm -r logs\nrm -r kafka-data/\n</code></pre> <p>Or run <code>cleanLab.sh</code></p>"},{"location":"use-cases/kafka-mm2/lab-2/","title":"Mirror Maker 2 ES on RHOS to local cluster","text":"<p> Scenario Prerequisites Overview Start Strimzi Kafka Cluster Produce messages to source cluster Start Mirror Maker 2 Start Consumer from target cluster Clean up </p> <p>Updated 01/22/2021</p>"},{"location":"use-cases/kafka-mm2/lab-2/#overview","title":"Overview","text":"<p>For this scenario, the source cluster will be an IBM Event Streams instance on OpenShift and the target cluster will be another Kafka cluster (using Strimzi) running locally on your workstation. Mirror Maker 2 will also run locally on your workstation. This lab is similar to the previous Lab 1, but instead it uses IBM Event Streams within the Cloud Pak for Integration as illustrated in the figure below:</p> <p></p> <ol> <li>Mirror Maker 2 runs locally on your workstation.</li> <li>A producer to send records to the <code>products</code> topic that also runs locally although it could be deployed on OpenShift as a job as well.</li> <li>A Kafka cluster running locally on your workstation that will contain the replicated topic and a Kafka console consumer to see the replicated messages.</li> </ol>"},{"location":"use-cases/kafka-mm2/lab-2/#scenario-prerequisites","title":"Scenario Prerequisites","text":"<ul> <li>An IBM Event Streams instance running on OpenShift. See here for more detail about installing IBM Event Streams.</li> <li>Docker Compose</li> <li>Git CLI</li> </ul> <p>Complete the following steps in order to get ready for executing this lab scenario</p> <ol> <li> <p>Create the <code>products</code> topic in your IBM Event Streams instance running on OpenShift. IMPORTANT: Create the topic with just 1 partition. To do so, please review the instructions in the Common pre-requisites of this website here. IMPORTANT: If you are sharing the IBM Event Streams instance, append a unique identifier to the <code>products</code> topic name so that you don't collide with anyone else.</p> </li> <li> <p>If you did not complete Lab 1, clone the following GitHub repository to your local workstation to get the Mirror Maker 2 configuration files for this lab:</p> </li> </ol> <pre><code>git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\n</code></pre> <ol> <li>Change directory into <code>refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local</code></li> </ol> <pre><code>cd refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local\n</code></pre> <ol> <li>Rename the <code>.env-tmpl</code> properties file to <code>.env</code></li> </ol> <pre><code>mv .env-tmpl .env\n</code></pre> <ol> <li> <p>Download the IBM Event Streams TLS certificate so that your Kafka Connect framework local instance can establish secure communication with your IBM Event Streams instance. IMPORTANT: download the PKCS12 certificate. How to get the certificate in the Common pre-requisites section. </p> </li> <li> <p>The <code>.env</code> properties file will contain the properties needed for Mirror Maker 2 to be able to connect with your IBM Event Streams instance running on Openshift. Therefore, replace the following placeholder in the properties file:</p> <ul> <li><code>REPLACE_WITH_YOUR_BOOTSTRAP_URL</code>: Your IBM Event Streams bootstrap url.</li> <li><code>REPLACE_WITH_YOUR_PKCS12_CERTIFICATE_PASSWORD</code>: Your PCKS12 TLS certificate password.</li> <li><code>REPLACE_WITH_YOUR_SCRAM_USERNAME</code>: Your SCRAM service credentials username.</li> <li><code>REPLACE_WITH_YOUR_SCRAM_PASSWORD</code>: Your SCRAM service credentials password.</li> <li><code>REPLACE_WITH_YOUR_TOPIC</code>: Name of the topic you created above.</li> </ul> </li> </ol> <p>Review the Common pre-requisites instructions if you don't know how to find out any of the config properties above. </p>"},{"location":"use-cases/kafka-mm2/lab-2/#start-strimzi-kafka-cluster","title":"Start Strimzi Kafka Cluster","text":"<p>In this section, we are going to deploy and start a local Strimzi Kafka cluster which will act as your target cluster for Mirror Maker 2 to mirror the messages getting into the <code>products</code> topic in your IBM Event Streams instance to. In order to deploy this local Strimzi Kafka cluster, we are providing a Docker Compose file that will coordinate the startup of all the components in this Strimzi Kafka cluster.</p> <ol> <li> <p>Make sure you are in <code>refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local</code>.</p> </li> <li> <p>Execute the following command</p> </li> </ol> <pre><code>docker-compose up -d\n</code></pre> <ol> <li>The above command should start all the components in <code>detached</code> mode (<code>-d</code>) and you should see the following output:</li> </ol> <pre><code>Creating zookeeper1 ... done\nCreating kafka1     ... done\nCreating kafka2     ... done\nCreating kafka3     ... done\n</code></pre> <ol> <li>You should see the following Docker containers running on your workstation at the moment</li> </ol> <pre><code>docker ps\nCONTAINER ID   IMAGE                              COMMAND                  CREATED         STATUS         PORTS                                              NAMES\n1981f1913ab6   strimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/kafka-se\u2026\"   2 minutes ago   Up 2 minutes   0.0.0.0:9093-&gt;9093/tcp, 0.0.0.0:29093-&gt;29093/tcp   kafka3\n5f8fd3e80406   strimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/kafka-se\u2026\"   2 minutes ago   Up 2 minutes   0.0.0.0:9091-&gt;9091/tcp, 0.0.0.0:29091-&gt;29091/tcp   kafka1\nb19a05bd74dd   strimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/kafka-se\u2026\"   2 minutes ago   Up 2 minutes   0.0.0.0:9092-&gt;9092/tcp, 0.0.0.0:29092-&gt;29092/tcp   kafka2\n93f500c8517a   strimzi/kafka:latest-kafka-2.6.0   \"sh -c 'bin/zookeepe\u2026\"   2 minutes ago   Up 2 minutes   0.0.0.0:2181-&gt;2181/tcp                             zookeeper1\n</code></pre>"},{"location":"use-cases/kafka-mm2/lab-2/#produce-messages-to-source-cluster","title":"Produce messages to source cluster","text":"<p>In this section, we are going to finally send events to the <code>products</code> topic in your IBM Event streams instance, which is your source cluster, and then verify those messages get mirrored by Mirror Maker 2 into your local Strimzi Kafka cluster, which is your target cluster. We are going to use a shell script which, in turn, will run a Python application that will send the messages to the source cluster.</p> <ol> <li> <p>Since the application sending the messages to the source cluster is not a Java application, we will first need to download the <code>PEM</code> TLS certificate to allow the secure connection from the python application sending the messages to IBM Event Streams. Make sure you are in the <code>refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local</code> directory.  Download the <code>PEM</code> TLS certificate there. Review the Common pre-requisites instructions if you don't remember how to download the certificate.</p> </li> <li> <p>Now we are going to send five records. In a new terminal window, make sure you are in the <code>refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local</code> directory and execute the following bash script.</p> </li> </ol> <pre><code>./sendProductRecords.sh\n</code></pre> <ol> <li>You should see the following output indicating your messages have been delivered to the source cluster topic</li> </ol> <pre><code>--- This is the configuration for the producer: ---\n[KafkaProducer] - {'bootstrap.servers': 'kafka-bootstrap-integration.apps.net:443', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'test_user', 'sasl.password': '******', 'ssl.ca.location': '/home/es-cp4i-to-local/es-cert.pem'}\n---------------------------------------------------\n{'product_id': 'P01', 'description': 'Carrots', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P02', 'description': 'Banana', 'target_temperature': 6, 'target_humidity_level': 0.6, 'content_type': 2}\n{'product_id': 'P03', 'description': 'Salad', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P04', 'description': 'Avocado', 'target_temperature': 6, 'target_humidity_level': 0.4, 'content_type': 1}\n{'product_id': 'P05', 'description': 'Tomato', 'target_temperature': 4, 'target_humidity_level': 0.4, 'content_type': 2}\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n[KafkaProducer] - Message delivered to products [0]\n</code></pre> <ol> <li>If you go to the IBM Event Streams console, you should also see those messages in the topic</li> </ol> <p></p>"},{"location":"use-cases/kafka-mm2/lab-2/#start-mirror-maker-2","title":"Start Mirror Maker 2","text":"<p>In this section, we are going to go through the steps to get Mirror Maker 2 running locally on your workstation and configure it so that it replicates the messages from the <code>products</code> topic in your IBM Event Streams instance running on OpenShift to the local Strimzi Kafka cluster you deployed in the previous section as the target cluster for those messages.</p> <ol> <li> <p>Make sure you are in <code>refarch-eda-tools/labs/mirror-maker2/es-cp4i-to-local</code> and you have done all steps in the Scenario Prerequisites section.</p> </li> <li> <p>Start your local Mirror Maker 2 instance by executing the following bash script.</p> </li> </ol> <pre><code>./launchMM2.sh\n</code></pre> <ol> <li>After quite some long output on your screen, you should see the following messages with the name of your topic. Don't worry if you dont find these as there is a lot of ouput. You will make sure the messages are replicated in the next section.</li> </ol> <pre><code>INFO [Consumer clientId=consumer-null-14, groupId=null] Subscribed to partition(s): products-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)\nINFO Starting with 1 previously uncommitted partitions. (org.apache.kafka.connect.mirror.MirrorSourceTask:94)\nINFO [Consumer clientId=consumer-null-14, groupId=null] Seeking to offset 0 for partition products-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1596)\nINFO [Consumer clientId=consumer-null-15, groupId=null] Subscribed to partition(s): heartbeats-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)\nINFO task-thread-MirrorSourceConnector-0 replicating 1 topic-partitions es-cp4i-&gt;target: [products-0]. (org.apache.kafka.connect.mirror.MirrorSourceTask:98)\nINFO WorkerSourceTask{id=MirrorSourceConnector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:233)\nINFO Starting with 1 previously uncommitted partitions. (org.apache.kafka.connect.mirror.MirrorSourceTask:94)\nINFO [Consumer clientId=consumer-null-15, groupId=null] Seeking to offset 0 for partition heartbeats-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1596)\nINFO task-thread-MirrorSourceConnector-1 replicating 1 topic-partitions es-cp4i-&gt;target: [heartbeats-0]. (org.apache.kafka.connect.mirror.MirrorSourceTask:98)\nINFO WorkerSourceTask{id=MirrorSourceConnector-1} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:233)\n</code></pre>"},{"location":"use-cases/kafka-mm2/lab-2/#start-consumer-from-target-cluster","title":"Start consumer from target cluster","text":"<p>In this section, we are going to start a consumer to consume messages from the target cluster (your local Strimzi Kafka cluster) to make sure we receive mirrored messages from your source cluster (your IBM Event Streams instance running on OpenShift). We are going to use a couple of Apache Kafka tools comming with the open source Strimzi Kafka Docker image you already have running.</p> <ol> <li>Make sure your target mirrored topic has been created executing the following command on a new terminal window.</li> </ol> <pre><code>docker exec kafka2 bash -c \"/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server kafka1:9091\" \n\n__consumer_offsets\nes-cp4i.checkpoints.internal\nes-cp4i.heartbeats\nes-cp4i.products\nheartbeats\nmm2-configs.es-cp4i.internal\nmm2-offsets.es-cp4i.internal\nmm2-status.es-cp4i.internal\n</code></pre> <p>You should see a topic called <code>es-cp4i.YOUR_TOPIC</code> where <code>YOUR_TOPIC</code> should be the name of the topic you created before in the Scenario Prerequisites section.</p> <ol> <li>Now, execute the following command replacing the <code>TOPIC_NAME</code> placeholder with the name of the topic you verified above (<code>ex-cp4i.YOUR_TOPIC</code>)</li> </ol> <pre><code>docker exec -ti kafka2 bash -c \"/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9091 --topic TOPIC_NAME --from-beginning\" \n</code></pre> <ol> <li>You should see the mirrored messages now in your replicated topic in your target local Strimzi Kafka cluster</li> </ol> <pre><code>{\"product_id\": \"P01\", \"description\": \"Carrots\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 1}\n{\"product_id\": \"P02\", \"description\": \"Banana\", \"target_temperature\": 6, \"target_humidity_level\": 0.6, \"content_type\": 2}\n{\"product_id\": \"P03\", \"description\": \"Salad\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 1}\n{\"product_id\": \"P04\", \"description\": \"Avocado\", \"target_temperature\": 6, \"target_humidity_level\": 0.4, \"content_type\": 1}\n{\"product_id\": \"P05\", \"description\": \"Tomato\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 2}\n</code></pre>"},{"location":"use-cases/kafka-mm2/lab-2/#clean-up","title":"Clean up","text":"<p>You have now successfully finished the lab. You can stop the consumer and the Mirror Maker 2 console output pressing <code>ctrl+c</code> in their respective terminals. You can also stop and remove the Docker containers for both the Mirror Maker 2 and Strimzi Kafka clusters running on your workstation by executing the following script:</p> <pre><code>./cleanLab.sh\n</code></pre>"},{"location":"use-cases/kafka-mm2/lab-3/","title":"Mirror Maker 2 Active Passive","text":"<p> Overview Pre-requisites Start Mirror Maker 2 Start Producer to source cluster Consuming records on source Failover to target </p> <p>Updated 01/08/2021</p>"},{"location":"use-cases/kafka-mm2/lab-3/#overview","title":"Overview","text":"<p>This lab presents how to leverage Mirror Maker 2 between two on-premise Kafka clusters running on OpenShift, one having no consumer and producer connected to it: it is in passive mode. The cluster is still getting replicated data. The lab goes up to the failover and reconnect consumers to the newly promoted active cluster.</p> <p></p> <ol> <li>Mirror Maker 2 runs on OpenShift as pod in the same namespace as Event Streams on the target cluster</li> <li>A producer in python to send records to <code>products</code> topic, will run locally or could be deployed on OpenShift as a job</li> <li>A consumer, also in python is consuming n records, in auto commit, so there will be a consumer lag before the failover.</li> <li>For the failover, we will stop the producer. We could stop event streams, but the most important is that there is no more records coming from the source cluster to the target cluster via mirroring. The goal not is to connect the consumer to the target cluster and then continue from where the consumer on the source cluster has stopped. If those consumer was writing to a database then the DB in the passive environment will receive the new records. If the database is in a 3nd environment like a managed service in the cloud with HA, then new records will be added to the original one. If the database server was also doing replication between active and passive environments then it may be possible to get a gap in the data, depending of the DB replication settings. </li> </ol> <p>In the figure above, the offset numbering does not have to match with source. This is where mirror maker 2 is keeping offset metadata on its own topics. The commit offsets for each consumer groups is also saved, so consumers restarting on the target cluster will continue for the matching offset corresponding to the last read committed offset.</p>"},{"location":"use-cases/kafka-mm2/lab-3/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>We assume, you have access to two Kafka Clusters deployed on OpenShift. We use two Event Streams instances on the same OpensShift cluster for this lab.  </li> <li>Login to the OpenShift cluster using the console and get the API token</li> </ul> <pre><code>oc login --token=L0.... --server=https://api.eda-solutions.gse-ocp.net:6443\n</code></pre> <ul> <li>If not done from lab 1, clone the github to get access to the Mirror Maker 2 manifests we are using:</li> </ul> <p><pre><code>git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools\n</code></pre> * Create the <code>products</code> topic on the source cluster. * Under the labs/MirrorMaker2/active-passive folder, rename the <code>.env-tmpl</code> file to <code>.env</code>.  * Get the source and target bootstrap server external URLs for the producer and consumer code using <code>oc get routes | grep bootstrap</code>. We will use to demonstrate the offset management and consumer reconnection then modify the addresses in the <code>.env</code> file</p> <p><pre><code>ES_SRC_BROKERS=light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443\nES_TGT_BROKERS=gse-eda-dev-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443\n</code></pre> * Get SCRAM users for both cluster and set their names in the <code>.env</code> file</p> <p><pre><code>ES_SRC_USER=starter\nES_TGT_USER=user2\n</code></pre> * Get pem certificates from both clusters using each admin console web apps, or the CLI:</p> <pre><code>cloudctl es init\n# select one of the cluster, then...\ncloudctl es certificates --format pem\n# rename the es-cert.pem to \nmv es-cert.pem es-src-cert.pem\n# Get for the second cluster\ncloudctl es init\ncloudctl es certificates --format pem\nmv es-cert.pem es-tgt-cert.pem\n</code></pre> <ul> <li>Verify the Event Streams on OpenShift service end point URLs. Those URLs will be used to configure Mirror Maker 2. </li> </ul> <pre><code># Use the bootstrap internal URL\noc get svc | grep bootstrap\n# Get both internal URLs\nlight-es-kafka-bootstrap.evenstreams.svc:9092\ngse-eda-dev-kafka-bootstrap.evenstreams.svc:9092\n</code></pre>"},{"location":"use-cases/kafka-mm2/lab-3/#start-mirror-maker-2","title":"Start Mirror Maker 2","text":"<p>In this lab, Mirror Maker 2 will run on the same cluster as Event Streams within the same namespace (e.g. eventstreams). </p> <ul> <li>Define source and target cluster properties in a Mirror Maker 2 <code>es-to-es.yml</code> descriptor file. We strongly recommend to study the schema definition of this custom resource from this page. </li> </ul> <p>Here are some important parameters you need to consider: The namespace needs to match the event streams project, and the annotations the product version and ID. The connectCluster needs to match the alias of the target cluster. The alias <code>es-tgt</code> represents the kafka cluster Mirror Maker 2 needs to connect to:</p> <pre><code>apiVersion: eventstreams.ibm.com/v1alpha1\nkind: KafkaMirrorMaker2\nmetadata:\n  name: mm2\n  namespace: eventstreams\nspec:\n  template:\n    pod:\n      metadata:\n        annotations:\n          eventstreams.production.type: CloudPakForIntegrationNonProduction\n          productCloudpakRatio: \"2:1\"\n          productChargedContainers: mm2-mirrormaker2\n          productVersion: 10.1.0\n          productID: 2a79e49111f44ec3acd89608e56138f5\n          cloudpakName: IBM Cloud Pak for Integration\n          cloudpakId: c8b82d189e7545f0892db9ef2731b90d\n          productName: IBM Event Streams for Non Production\n          cloudpakVersion: 2020.3.1\n          productMetric: VIRTUAL_PROCESSOR_CORE\n  version: 2.6.0\n  replicas: 1\n  connectCluster: \"es-tgt\"\n</code></pre> <p>The version matches the Kafka version we use. The number of replicas can be set to 1 to start with or use the default of 3. The <code>eventstreams.production.type</code> is needed for Event Streams.</p> <p>Then the yaml defines the connection configuration for each clusters:</p> <pre><code>clusters:\n  - alias: \"es-src\"\n    bootstrapServers: \n    config:\n      config.storage.replication.factor: 3\n      offset.storage.replication.factor: 3\n      status.storage.replication.factor: 3\n    tls: {}\n</code></pre> <p>For Event Streams on premise running within OpenShift, the connection uses TLS, certificates and SCRAM credentials. As we run in a separate namespace the URL is the 'external' one.</p> <pre><code>- alias: \"es-src\"\n    bootstrapServers: light-es-kafka-bootstrap.integration.svc:9093\n    config:\n      ssl.endpoint.identification.algorithm: https\n    tls: \n      trustedCertificates:\n        - secretName: light-es-cluster-ca-cert\n          certificate: ca.crt\n    authentication:\n      type: tls\n      certificateAndKey:\n        certificate: user.crt\n        key: user.key\n        secretName: es-tls-user\n</code></pre> <p>Finally the <code>connectCluster</code> attribute defines the cluster alias used by MirrorMaker2 to define its hidden topics, it must match the target cluster of the replication in the list at <code>spec.clusters</code>.</p> <pre><code># under active-passive folder\noc apply -f es-to-es.yml\n</code></pre> <ul> <li>Verify the characteristics of the Mirror Maker 2 instance using the CLI</li> </ul> <p><pre><code>oc describe kafkamirrormaker2 mm2\n</code></pre> * See the logs:</p> <pre><code>oc get pods | grep mm2\noc logs mm2-mirrormaker2-...\n</code></pre>"},{"location":"use-cases/kafka-mm2/lab-3/#start-producer-to-source-cluster","title":"Start Producer to source cluster","text":"<p>As seen in lab 1, we will use the same python script to create products records. This time the script is producing product records to the <code>products</code> topic. </p> <p>Now send 100 records:</p> <pre><code>./sendProductRecords.sh --random 100\n</code></pre> <p>The trace looks like:</p> <p>```  --- This is the configuration for the producer: --- [KafkaProducer] - {'bootstrap.servers': 'light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443', 'group.id': 'ProductsProducer', 'delivery.timeout.ms': 15000, 'request.timeout.ms': 15000, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'starter', 'sasl.password': 'd8zsUzhK9qUZ', 'ssl.ca.location': '/home/active-passive/es-cert.pem'}</p> <p>{'product_id': 'T1', 'description': 'Product 1', 'target_temperature': 6.321923853806639, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T2', 'description': 'Product 2', 'target_temperature': 4.991504310455889, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T3', 'description': 'Product 3', 'target_temperature': 4.491634291119919, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T4', 'description': 'Product 4', 'target_temperature': 2.4855241432802613, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T5', 'description': 'Product 5', 'target_temperature': 4.286428275499635, 'target_humidity_level': 0.4, 'content_type': 1} {'product_id': 'T6', 'description': 'Product 6', 'target_temperature': 1.6025770613167736, 'target_humidity_level': 0.4, 'content_type': 1}  ```</p> <ul> <li>Going to the Event Streams Console we can see the produced messages in the <code>products</code> topic.</li> </ul> <p></p>"},{"location":"use-cases/kafka-mm2/lab-3/#start-the-consumer-to-source-cluster","title":"Start the Consumer to source cluster","text":"<p>To simulate the offset mapping between source and target, we will use a python consumer and read only n records.</p> <p><code>shell  ./receiveProductSrc.sh 20</code></p> <p>The trace may look like:</p> <p>```  --------- Start Consuming products -------------- [KafkaConsumer] - This is the configuration for the consumer: [KafkaConsumer] - ------------------------------------------- [KafkaConsumer] - Bootstrap Server:      light-es-kafka-bootstrap-eventstreams.gse-eda-2020-10-3-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud:443 [KafkaConsumer] - Topic:                 products [KafkaConsumer] - Topic timeout:         10 [KafkaConsumer] - Security Protocol:     SASL_SSL [KafkaConsumer] - SASL Mechanism:        SCRAM-SHA-512 [KafkaConsumer] - SASL Username:         starter [KafkaConsumer] - SASL Password:         d*****Z [KafkaConsumer] - SSL CA Location:       /home/active-passive/es-cert.pem [KafkaConsumer] - Offset Reset:          earliest [KafkaConsumer] - Autocommit:            True [KafkaConsumer] - ------------------------------------------- [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 0     key: P01     value: {\"product_id\": \"P01\", \"description\": \"Carrots\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 1} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 1     key: P02     value: {\"product_id\": \"P02\", \"description\": \"Banana\", \"target_temperature\": 6, \"target_humidity_level\": 0.6, \"content_type\": 2} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 2     key: P03     value: {\"product_id\": \"P03\", \"description\": \"Salad\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 1} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 3     key: P04     value: {\"product_id\": \"P04\", \"description\": \"Avocado\", \"target_temperature\": 6, \"target_humidity_level\": 0.4, \"content_type\": 1} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 4     key: P05     value: {\"product_id\": \"P05\", \"description\": \"Tomato\", \"target_temperature\": 4, \"target_humidity_level\": 0.4, \"content_type\": 2} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 5     key: T1     value: {\"product_id\": \"T1\", \"description\": \"Product 1\", \"target_temperature\": 6.321923853806639, \"target_humidity_level\": 0.4, \"content_type\": 1} [KafkaConsumer] - Next Message consumed from products partition: [0] at offset: 6</p> <p>```</p> <p>The python code is ProductConsumer.py.</p> <p>If we go to Event Streams consumer group monitoring user interface we can see the consumer only got 20 messages and so there is an offset lag, as illustrated in figure below:</p> <p></p>"},{"location":"use-cases/kafka-mm2/lab-3/#failover-to-target","title":"Failover to target","text":"<p>At this stage, the producer code is not running anymore, Mirror Maker 2 has replicated the data to the target topic named: <code>es-src.products</code>, the consumer has not read all the messages from source cluster. This simulate a crash on source cluster. So let now connect the consumer to the target cluster and continue to process the records. For that the consumer needs to get the offset mapping using the RemoteClusterUtils class to translate the consumer group offset from the source cluster to the corresponding offset for the target cluster. </p>"},{"location":"use-cases/kafka-streams/","title":"Kafka Streams Hands-on Labs","text":""},{"location":"use-cases/kafka-streams/#kafka-streams-labs","title":"Kafka Streams Labs","text":"<p>The following hands-on labs walk users through building and testing Kafka Streams-based applications developed with Kafka API and Quarkus.</p> Scenario Description Link Lab 0 Introduction to Kafka Streams application code and test capabilities Kafka Streams - Lab 0 Lab 1 Advanced Kafka Streams test cases and utilizing state stores Kafka Streams - Lab 1 Lab 2 Advanced Kafka Streams test cases and connecting Kafka Streams to IBM Event Streams instances Kafka Streams - Lab 2 Lab 3 Inventory management with Kafka Streams with IBM Event Streams on OpenShift Kafka Streams - Lab 3"},{"location":"use-cases/kafka-streams/lab-0/","title":"Kafka Streams Test Lab 0","text":"<p>An introduction to using test Kafka Streams Test Suite to test Kafka Streams Topologies.</p> <p>Info</p> <p>Updated 03/10/2022</p>"},{"location":"use-cases/kafka-streams/lab-0/#overview","title":"Overview","text":"<ul> <li>We are testing a Kafka Streams topology using Apache Kafka Streams TestDriver. </li> <li>The topology </li> <li>While using the TestDriver we will perform basic stateless operations and understand the testing infrastructure.</li> </ul> <p>The code for this lab is in this repository eda-kstreams-labs folder kstream-lab0</p>"},{"location":"use-cases/kafka-streams/lab-0/#scenario-prerequisites","title":"Scenario Prerequisites","text":"<p>Java</p> <ul> <li>For the purposes of this lab we suggest Java 8+</li> </ul> <p>Quarkus CLI</p> <p>Maven</p> <ul> <li>Maven will be needed for bootstrapping our application from the command-line and running our application.</li> </ul> <p>An IDE of your choice</p> <ul> <li>Ideally an IDE that supports Quarkus (such as Visual Studio Code)</li> </ul>"},{"location":"use-cases/kafka-streams/lab-0/#setting-up-the-quarkus-application","title":"Setting up the Quarkus Application","text":"<ul> <li>We will bootstrap the Quarkus application with the following command</li> </ul> <pre><code>quarkus create kstream-lab0\n</code></pre> <ul> <li>Since we will be using the Kafka Streams testing functionality, we will need to edit the <code>pom.xml</code> to add the dependency to our project. Open <code>pom.xml</code> and add the following:</li> </ul> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n    &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;\n    &lt;version&gt;3.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n    &lt;artifactId&gt;kafka-streams-test-utils&lt;/artifactId&gt;\n    &lt;version&gt;3.1.0&lt;/version&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.hamcrest&lt;/groupId&gt;\n    &lt;artifactId&gt;hamcrest&lt;/artifactId&gt;\n    &lt;version&gt;2.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The last dependency is for the hamcrest Domain Specific Language for test assertion.</p>"},{"location":"use-cases/kafka-streams/lab-0/#creating-your-first-test-class","title":"Creating your first Test Class","text":"<ul> <li> <p>Now let's create our first Test Class.</p> </li> <li> <p>Create the directory structure you will need for your Java file. (NOTE: If you are working in an IDE, this may be done for you when you create your package and classes.)</p> </li> </ul> <pre><code>mkdir -p src/test/java/ibm/eda/kstreams/lab0\n</code></pre> <ul> <li> <p>Create a new file named <code>src/test/java/eda/kstreams/lab0/FirstKafkaStreamsTest.java</code>.</p> </li> <li> <p>Paste the following content into the <code>FirstKafkaStreamsTest</code> class:</p> </li> </ul> <pre><code>package eda.kafka.streams;\n\nimport static org.hamcrest.CoreMatchers.equalTo;\nimport static org.hamcrest.CoreMatchers.is;\nimport static org.hamcrest.MatcherAssert.assertThat;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.common.serialization.StringDeserializer;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.TestInputTopic;\nimport org.apache.kafka.streams.TestOutputTopic;\nimport org.apache.kafka.streams.Topology;\nimport org.apache.kafka.streams.TopologyTestDriver;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.junit.jupiter.api.AfterEach;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\n\nimport io.quarkus.test.junit.QuarkusTest;\n\n@QuarkusTest\npublic class FirstKafkaStreamsTest {\n\n    private static TopologyTestDriver testDriver;\n    private static String inTopicName = \"my-input-topic\";\n    private static String outTopicName = \"my-output-topic\";\n\n    private static TestInputTopic&lt;String, String&gt; inTopic;\n    private static TestOutputTopic&lt;String, String&gt; outTopic;\n\n    @BeforeEach\n    public void buildTopology() {\n\n        final Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kstream-lab0\");\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummmy:2345\");\n        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n\n        final StreamsBuilder builder = new StreamsBuilder();\n        KStream&lt;String, String&gt; basicColors = builder.stream(inTopicName,Consumed.with(Serdes.String(), Serdes.String()));\n        basicColors.peek((key, value) -&gt; System.out.println(\"PRE-FILTER: key=\" + key + \", value=\" + value))\n            .filter((key, value) -&gt; (\"BLUE\".equalsIgnoreCase(value)))\n            .peek((key, value) -&gt; System.out.println(\"POST-FILTER: key=\" + key + \", value=\" + value))\n            .to(outTopicName);\n\n        Topology topology = builder.build();\n\n        testDriver = new TopologyTestDriver(topology, props);\n        inTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new StringSerializer());\n        outTopic = testDriver.createOutputTopic(outTopicName, new StringDeserializer(), new StringDeserializer());\n\n    }\n\n    @AfterEach\n    public void teardown() {\n        testDriver.close();\n    }\n\n}\n</code></pre> <ul> <li> <p>The above code does a lot in a few lines, so we'll walk through some of that here.</p> <ul> <li>The <code>@BeforeEach</code> annotation on the <code>buildTopology</code> method means that it will be run each time before each test is executed, while the <code>@AfterEach</code> annotation on the <code>teardown</code> method ensures that it will be run each time after each test execution. This allows us to spin up and tear down all the necessary components to test in isolation with each test case.</li> <li>The <code>buildTopology</code> method utilizes the <code>StreamsBuilder</code> class to construct a simple topology, reading from the input Kafka topic defined by the <code>inTopicName</code> String.</li> <li> <p>The topology, we build here, utilizes three of the stateless processors the Kafka Streams API:</p> <ul> <li><code>peek</code> allows us to look at the key and the value of the record passing through the stream and continue processing it unaffected (so we leverage this before and after the next processor used to see what is making its way through the topology)</li> <li><code>filter</code> allows us to drop records that do not meet the criteria specified (either for the key or the value). In this test class, we are filtering on any value that does not match the word <code>\"BLUE\"</code> (using a case-insensitive search)</li> <li><code>to</code> is the final processor used and to write the contents of the topology at that point to an output Kafka topic</li> </ul> </li> <li> <p>The Kafka Streams Test infrastructure provides us the capability to leverage driver classes that function as their own input and output topics, removing the need from connecting directly to a live Kafka instance. The <code>inTopic</code> and <code>outTopic</code> instantiation at the bottom of the <code>buildTopology</code> method hooks into this test infrastructure, so that our test methods can use them to write to and read from the topology.</p> </li> <li>The <code>teardown</code> method cleans up the topology and all the data that has been sent through it for any given test run, allowing us to reset and rerun test cases as needed.</li> </ul> </li> <li> <p>Build the application by running the following:</p> </li> </ul> <pre><code>./mvnw clean verify\n</code></pre> <ul> <li> <p>You should see output similar to the following: <pre><code>...\n[INFO]\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO]\n[INFO] Results:\n[INFO]\n[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0\n[INFO]\n...\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  29.470 s\n[INFO] Finished at: 2020-09-17T09:34:26-05:00\n[INFO] ------------------------------------------------------------------------\n</code></pre></p> </li> <li> <p>The build compiled and the test topology was successfully created. But no tests were run, because no tests were written!</p> </li> </ul>"},{"location":"use-cases/kafka-streams/lab-0/#add-your-first-tests","title":"Add your first Tests","text":"<ul> <li>Open <code>src/test/java/eda/kafka/streams/FirstKafkaStreamsTest.java</code> and add the following tests to the bottom of the <code>FirstKafkaStreamsTest</code> class:</li> </ul> <pre><code>    @Test\n    public void isEmpty() {\n        assertThat(outTopic.isEmpty(), is(true));\n    }\n\n    @Test\n    public void isNotEmpty() {\n        assertThat(outTopic.isEmpty(), is(true));\n        inTopic.pipeInput(\"C01\", \"blue\");\n        assertThat(outTopic.getQueueSize(), equalTo(1L) );\n        assertThat(outTopic.readValue(), equalTo(\"blue\"));\n        assertThat(outTopic.getQueueSize(), equalTo(0L) );\n    }\n\n    @Test\n    public void selectBlues() {\n        assertThat(outTopic.isEmpty(), is(true));\n\n        inTopic.pipeInput(\"C01\", \"blue\");\n        inTopic.pipeInput(\"C02\", \"red\");\n        inTopic.pipeInput(\"C03\", \"green\");\n        inTopic.pipeInput(\"C04\", \"Blue\");\n\n        assertThat(outTopic.getQueueSize(), equalTo(2L) );\n\n        assertThat(outTopic.isEmpty(), is(false));\n\n        assertThat(outTopic.readValue(), equalTo(\"blue\"));\n        assertThat(outTopic.readValue(), equalTo(\"Blue\"));\n\n        assertThat(outTopic.getQueueSize(), equalTo(0L) );\n\n    }\n</code></pre> <ul> <li>These are three simple tests:</li> <li>The <code>isEmpty</code> test method checks to make sure the output topic is empty when nothing is sent through the topology</li> <li>The <code>isNotEmpty</code> test method checks to make sure the output topic is not empty when an item matching our filters is sent through the topology</li> <li> <p>The <code>selectBlues</code> test method checks to make sure that our topology is filtering correctly when we send multiple items through the topology and the output topic empties correctly when the testing infrastructure reads from it.</p> </li> <li> <p>You should see the tests pass with the following output:</p> </li> </ul> <pre><code>[INFO]\n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running eda.kafka.streams.FirstKafkaStreamsTest\n2020-09-17 09:44:33,247 INFO  [io.sma.rea.mes.provider] (main) SRMSG00208: Deployment done... start processing\n2020-09-17 09:44:33,250 INFO  [io.sma.rea.mes.provider] (main) SRMSG00226: Found incoming connectors: [smallrye-kafka]\n2020-09-17 09:44:33,251 INFO  [io.sma.rea.mes.provider] (main) SRMSG00227: Found outgoing connectors: [smallrye-kafka]\n2020-09-17 09:44:33,252 INFO  [io.sma.rea.mes.provider] (main) SRMSG00229: Channel manager initializing...\n2020-09-17 09:44:33,254 INFO  [io.sma.rea.mes.provider] (main) SRMSG00209: Initializing mediators\n2020-09-17 09:44:33,255 INFO  [io.sma.rea.mes.provider] (main) SRMSG00215: Connecting mediators\n2020-09-17 09:44:33,382 INFO  [io.quarkus] (main) Quarkus 1.8.0.Final on JVM started in 2.029s. Listening on: http://0.0.0.0:8081\n2020-09-17 09:44:33,382 INFO  [io.quarkus] (main) Profile test activated.\n2020-09-17 09:44:33,382 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, mutiny, resteasy-jsonb, smallrye-context-propagation, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, vertx]\nPRE-FILTER: key=C01, value=blue\nPOST-FILTER: key=C01, value=blue\nPRE-FILTER: key=C02, value=red\nPRE-FILTER: key=C03, value=green\nPRE-FILTER: key=C04, value=Blue\nPOST-FILTER: key=C04, value=Blue\nPRE-FILTER: key=C01, value=blue\nPOST-FILTER: key=C01, value=blue\n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.722 s - in eda.kafka.streams.FirstKafkaStreamsTest\n2020-09-17 09:44:34,026 INFO  [io.sma.rea.mes.provider] (main) SRMSG00207: Cancel subscriptions\n2020-09-17 09:44:34,038 INFO  [io.quarkus] (main) Quarkus stopped in 0.024s\n[INFO]\n[INFO] Results:\n[INFO]\n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0\n[INFO]\n[INFO]\n</code></pre>"},{"location":"use-cases/kafka-streams/lab-0/#next-steps","title":"Next Steps","text":"<ul> <li>Now that you have finished the foundational Kafka Streams testing lab, you can proceed to Lab 1 for a deeper dive into more robust real-world Kafka Streams testing use cases!</li> </ul>"},{"location":"use-cases/kafka-streams/lab-1/","title":"Kafka Streams Test Lab 1","text":"<p>Updated 03/10/2022</p>"},{"location":"use-cases/kafka-streams/lab-1/#overview","title":"Overview","text":"<ul> <li>In this lab scenario we are still using Apache Kafka Streams TestDriver to test a Topology, a Stream and Table.</li> <li>While using the TestDriver we will perform operations such as groupBy, join with another Stream or Kafka Table.</li> </ul> <p>The code for this lab is in this repository eda-kstreams-labs folder LoadKtableFromTopic</p>"},{"location":"use-cases/kafka-streams/lab-1/#scenario-prerequisites","title":"Scenario Prerequisites","text":"<p>Java</p> <ul> <li>For the purposes of this lab we suggest Java 11+</li> </ul> <p>Quarkus CLI</p> <p>Maven</p> <ul> <li>Maven will be needed for bootstrapping our application from the command-line and running our application.</li> </ul> <p>An IDE of your choice</p> <ul> <li>Ideally an IDE that supports Quarkus (such as Visual Studio Code)</li> </ul>"},{"location":"use-cases/kafka-streams/lab-1/#setting-up-the-quarkus-application","title":"Setting up the Quarkus Application","text":"<ul> <li>We will bootstrap the Quarkus application with the following Maven command</li> </ul> <pre><code>quarkus create LoadKtableFromTopic\n</code></pre> <p>You can replace the fields within {} as you like.</p> <ul> <li>Since we will be using the Kafka Streams testing functionality we will need to edit the <code>pom.xml</code> to add the dependency to our project. Open <code>pom.xml</code> and add the following.</li> </ul> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n    &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;\n    &lt;version&gt;3.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n    &lt;artifactId&gt;kafka-streams-test-utils&lt;/artifactId&gt;\n    &lt;version&gt;3.1.0&lt;/version&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.hamcrest&lt;/groupId&gt;\n    &lt;artifactId&gt;hamcrest&lt;/artifactId&gt;\n    &lt;version&gt;2.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"use-cases/kafka-streams/lab-1/#creating-your-test-class","title":"Creating your Test Class","text":"<ul> <li>Open the <code>TestLoadKtableFromTopic.java</code> file and paste the following content.</li> </ul> <pre><code>package ibm.eda.kstreams.lab1;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport org.apache.kafka.streams.KeyValue;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.TestInputTopic;\nimport org.apache.kafka.streams.TestOutputTopic;\nimport org.apache.kafka.streams.TopologyTestDriver;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KTable;\nimport org.apache.kafka.streams.kstream.Materialized;\nimport org.apache.kafka.streams.processor.StateStore;\nimport org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;\nimport org.apache.kafka.streams.state.KeyValueIterator;\nimport org.apache.kafka.streams.state.KeyValueStore;\nimport org.apache.kafka.streams.state.Stores;\nimport org.apache.kafka.streams.state.ValueAndTimestamp;\nimport org.junit.jupiter.api.AfterAll;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\nimport io.quarkus.test.junit.QuarkusTest;\n\n/**\n * This is a simple example of loading some reference data from stream into a Ktable for\n * lookup. It uses a persistent state store.\n */\n@QuarkusTest\npublic class TestLoadKtableFromTopic {\n    private static TopologyTestDriver testDriver;\n    private static String companySectorsTopic = \"sector-types\";\n    private static String storeName = \"sector-types-store\";\n\n    private static TestInputTopic&lt;String, String&gt; inTopic;\n    private static TestOutputTopic&lt;String, Long&gt; outTopic;\n    private static TestOutputTopic&lt;String, String&gt; errorTopic;\n\n    public static Properties getStreamsConfig() {\n        final Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kstream-lab1\");\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummmy:1234\");\n        return props;\n    }\n\n    @BeforeAll\n    public static void buildTopology(){\n        final StreamsBuilder builder = new StreamsBuilder();\n        // Adding a state store is a simple matter of creating a StoreSupplier\n        // instance with one of the static factory methods on the Stores class.\n        // all persistent StateStore instances provide local storage using RocksDB\n        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);\n\n        // A KTable is created from the companySectorsTopic, with key and value deserialized.\n        // With Materialized.as() causing the Table to force a state store materialization (storeSupplier).\n        KTable&lt;String, String&gt; sectorTypeTable = builder.table(companySectorsTopic,\n                Consumed.with(Serdes.String(), Serdes.String()),\n                Materialized.as(storeSupplier));\n\n        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());\n        inTopic = testDriver.createInputTopic(companySectorsTopic, new StringSerializer(), new StringSerializer());\n\n    }\n\n    @AfterAll\n    public static void close(){\n        testDriver.close();\n    }\n\n    @Test\n    public void shouldHaveSixSectorTypes(){\n        inTopic.pipeInput(\"C01\",\"Health Care\");\n        inTopic.pipeInput(\"C02\",\"Finance\");\n        inTopic.pipeInput(\"C03\",\"Consumer Services\");\n        inTopic.pipeInput(\"C04\",\"Transportation\");\n        inTopic.pipeInput(\"C05\",\"Capital Goods\");\n        inTopic.pipeInput(\"C06\",\"Public Utilities\");\n\n        KeyValueStore&lt;String,ValueAndTimestamp&lt;String&gt;&gt; store = testDriver.getTimestampedKeyValueStore(storeName);\n        Assertions.assertNotNull(store);\n\n        ValueAndTimestamp&lt;String&gt; sector = store.get(\"C02\");\n        Assertions.assertNotNull(sector);\n        Assertions.assertEquals(\"Finance\", sector.value());\n        Assertions.assertEquals(6, store.approximateNumEntries());\n\n\n        // demonstrate how to get all the values from the table:\n        KeyValueIterator&lt;String, ValueAndTimestamp&lt;String&gt;&gt; sectors = store.all();\n        while (sectors.hasNext()) {\n            KeyValue&lt;String,ValueAndTimestamp&lt;String&gt;&gt; s = sectors.next();\n            System.out.println(s.key + \":\" + s.value.value());\n        }\n        for ( StateStore s: testDriver.getAllStateStores().values()) {\n            System.out.println(s.name());\n        }\n    }\n}\n</code></pre> <ul> <li> <p>The above code uses TopologyTestDriver to mimic a Topology. A Topology is basically a graph of stream processors (nodes) and the edges between these nodes are the streams.  In the first section we instantiate our <code>TopologyTestDriver</code> named <code>testDriver</code>, as well as the topic name and store name.</p> </li> <li> <p>Test the application by running the following:</p> </li> </ul> <pre><code>./mvnw clean verify\n</code></pre> <ul> <li>You should see the tests pass with the following output:</li> </ul> <pre><code>[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-16 14:20:26,488 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.089s. Listening on: http://localhost:8081\n2021-01-16 14:20:26,490 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-16 14:20:26,490 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.096 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-16 14:20:28,253 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-16 14:20:28,256 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.222 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-16 14:20:28,292 INFO  [io.quarkus] (main) Quarkus stopped in 0.028s\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0\n</code></pre> <ul> <li> <p>How this test topology creation flow works:</p> <ul> <li>A StreamsBuilder object (builder) from the Kafka Streams DSL API is created.</li> <li>A KeyValueBytesStoreSupplier (storeSupplier) is configured with String variable (storeName).</li> <li>A KTable is created reading from the topic (companySectorsTopic), deserialized and materialized as the previously create (storeSupplier).</li> <li>A TopologyTestDriver (testDriver) is built from the provided config properties and the KTable within the builder topology.</li> <li>Lastly test input topic (inTopic) is created from the testDriver topology.</li> <li>When <code>inTopic.pipeInput(\"C01\",\"Health Care\");</code> is invoked, it populates the topic, which then populates the KTable which ultimately persists in a KeyValue State Store.</li> </ul> </li> <li> <p>You should see the tests pass. These are three simple tests. The first of which checks that the value fetched from the Kafka Table is not null,the second makes sure that value retrieved from key <code>C02</code> is equal to <code>Finance</code> and lastly we make sure that the our state store (which was piped by ways of the Kafka Topic) indeed has six key-value pairs.</p> </li> </ul>"},{"location":"use-cases/kafka-streams/lab-1/#more-robust-kafka-streams-testing","title":"More Robust Kafka Streams Testing","text":"<ul> <li>add jsonb Serdes using Quarkus kafka client library:</li> </ul> <pre><code>quarkus  ext add kafka-client jsonb\n</code></pre> <ul> <li> <p>Now that we have tested some simple functionality by using the Kafka Streams API let's check out some other operators that we can use.</p> </li> <li> <p>Let's create a new class for our Plain Old Java Object (POJO) named FinancialMessage and copy and paste the following content into the newly created file.</p> </li> </ul> <pre><code>package ibm.eda.kstreams.lab.domain;\n\n\nimport io.quarkus.runtime.annotations.RegisterForReflection;\n@RegisterForReflection\npublic class FinancialMessage implements JSONSerdeCompatible {\n\n    public String userId;\n    public String stockSymbol;\n    public String exchangeId;\n    public int quantity;\n    public double stockPrice;\n    public double totalCost;\n    public int institutionId;\n    public int countryId;\n    public boolean technicalValidation;\n\n    public FinancialMessage() {\n\n    }\n\n    public FinancialMessage(String userId, String stockSymbol, String exchangeId,\n                            int quantity, double stockPrice, double totalCost,\n                            int institutionId, int countryId, boolean technicalValidation) {\n\n        this.userId = userId;\n        this.stockSymbol = stockSymbol;\n        this.exchangeId = exchangeId;\n        this.quantity = quantity;\n        this.stockPrice = stockPrice;\n        this.totalCost = totalCost;\n        this.institutionId = institutionId;\n        this.countryId = countryId;\n        this.technicalValidation = technicalValidation;\n    }\n}\n</code></pre> <p>Note: We have not provided any accessors (getters) or mutators (setters) for simplicity. You can set those at your own discretion.</p> <p>Add the following interface for JSon Serde</p> <pre><code>/**\n     * An interface for registering types that can be de/serialized with {@link JSONSerde}.\n     */\n    @SuppressWarnings(\"DefaultAnnotationParam\") // being explicit for the example\n    @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.PROPERTY, property = \"_t\")\n    @JsonSubTypes({\n                      @JsonSubTypes.Type(value = FinancialMessage.class, name = \"fm\")\n                  })\npublic interface JSONSerdeCompatible {\n\n}\n</code></pre> <ul> <li> <p>And the Generic JSONSerde from this class</p> </li> <li> <p>Now that we have our new Java class, let's create a new and separate Java Test class: <code>src/test/java/ibm/eda/kstreams/lab1/TestFinancialMessage.java</code>.</p> </li> </ul> <p>Copy the contents below:</p> <pre><code>package ibm.eda.kstreams.lab1;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.LongDeserializer;\nimport org.apache.kafka.common.serialization.Serde;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.common.serialization.StringDeserializer;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport org.apache.kafka.streams.KeyValue;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.TestInputTopic;\nimport org.apache.kafka.streams.TestOutputTopic;\nimport org.apache.kafka.streams.TopologyTestDriver;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.apache.kafka.streams.kstream.Materialized;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.kstream.Windowed;\nimport org.apache.kafka.streams.kstream.WindowedSerdes;\nimport org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;\nimport org.apache.kafka.streams.state.KeyValueStore;\nimport org.apache.kafka.streams.state.Stores;\nimport org.apache.kafka.streams.state.ValueAndTimestamp;\nimport org.junit.jupiter.api.AfterAll;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\nimport io.quarkus.kafka.client.serialization.JsonbSerde;\nimport io.quarkus.kafka.client.serialization.JsonbSerializer;\nimport io.quarkus.test.junit.QuarkusTest;\n\nimport com.ibm.garage.cpat.domain.*;\n\n\n@QuarkusTest\npublic class TestFinancialMessage {\n\n    private static TopologyTestDriver testDriver;\n    private static String inTopicName = \"transactions\";\n    private static String outTopicName = \"output\";\n    private static String errorTopicName = \"errors\";\n    private static String storeName = \"transactionCount\";\n    private static TestInputTopic&lt;String, FinancialMessage&gt; inTopic;\n    private static TestOutputTopic&lt;String, Long&gt; outTopic;\n    private static TestOutputTopic&lt;String, String&gt; errorTopic;\n\n    public static Properties getStreamsConfig() {\n        final Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kstream-lab2\");\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummmy:2345\");\n        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, JSONSerde.class);\n        return props;\n    }\n\n    @BeforeAll\n    public static void buildTopology() {\n        final StreamsBuilder builder = new StreamsBuilder();\n        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);\n\n        KStream&lt;String, FinancialMessage&gt; transactionStream =\n            builder.stream(\n                inTopicName,\n                Consumed.with(Serdes.String(), financialMessageSerde)\n            );\n\n        // First verify user id is present, if not route to error\n        Map&lt;String, KStream&lt;String,FinancialMessage&gt;&gt; branches = transactionStream\n            .split(Named.as(\"tx-\"))\n            .branch((key, value) -&gt; value.userId == null, Branched.as(\"no-userid\"))\n            .defaultBranch(Branched.as(\"non-null\"));\n\n        // Handle error by sending to the errors topic.\n        branches.get(\"tx-no-userid\").map(\n                (key, value) -&gt; {\n                    return KeyValue.pair(key, \"No customer id provided\");\n                })\n                .to(\n                        errorTopicName, Produced.with(Serdes.String(), Serdes.String()));\n\n        // use groupBy to swap the key, then count by customer id,\n        branches.get(\"tx-non-null\").groupBy(\n                    (key, value) -&gt; value.userId\n                )\n                .count(\n                    Materialized.as(storeSupplier)\n                )\n                .toStream()\n                .to(\n                    outTopicName,\n                    Produced.with(Serdes.String(), Serdes.Long())\n            );\n\n        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());\n        inTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new JSONSerde&lt;FinancialMessage&gt;());\n        outTopic = testDriver.createOutputTopic(outTopicName, new StringDeserializer(), new LongDeserializer());\n        errorTopic = testDriver.createOutputTopic(errorTopicName, new StringDeserializer(), new StringDeserializer());\n    }\n\n    @AfterAll\n    public static void close(){\n        testDriver.close();\n    }\n}\n</code></pre> <ul> <li>We have the setup for the TestTopology. Now, we can add a test that will insert two events into the topic. Add the following code to your test class:</li> </ul> <pre><code>    @Test\n    public void shouldHaveOneTransaction() {\n        // A FinancialMessage is mocked and set to the input topic. Within the Topology,\n        // this gets sent to the outTopic because a userId exists for the incoming message.\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        FinancialMessage mock2 = new FinancialMessage(\n            \"2\", \"ASDF\", \"HELLO\", 5, 1000.22, 4444.12, 38, 6, true\n        );\n\n        inTopic.pipeInput(\"T01\", mock);\n        inTopic.pipeInput(\"T02\", mock2);\n\n        Assertions.assertFalse(outTopic.isEmpty());\n        Assertions.assertEquals(1, outTopic.readKeyValue().value);\n\n        KeyValueStore&lt;String,ValueAndTimestamp&lt;FinancialMessage&gt;&gt; store = testDriver.getTimestampedKeyValueStore(storeName);\n        Assertions.assertEquals(1, store.approximateNumEntries());\n    }\n</code></pre> <ul> <li>Test the application by running the following:</li> </ul> <pre><code>./mvnw clean verify\n</code></pre> <ul> <li>You should see the following output:</li> </ul> <pre><code>[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-16 17:21:37,836 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 1.996s. Listening on: http://localhost:8081\n2021-01-16 17:21:37,837 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-16 17:21:37,838 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.234 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\n2021-01-16 17:21:39,460 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.273 s &lt;&lt;&lt; FAILURE! - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[ERROR] shouldHaveOneTransaction  Time elapsed: 0.09 s  &lt;&lt;&lt; FAILURE!\norg.opentest4j.AssertionFailedError: expected: &lt;1&gt; but was: &lt;2&gt;\n    at com.ibm.garage.cpat.lab.TestFinancialMessage.shouldHaveOneTransaction(TestFinancialMessage.java:132)\n\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-16 17:21:39,505 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-16 17:21:39,507 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.039 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-16 17:21:39,541 INFO  [io.quarkus] (main) Quarkus stopped in 0.028s\n[INFO] \n[INFO] Results:\n[INFO] \n[ERROR] Failures: \n[ERROR]   TestFinancialMessage.shouldHaveOneTransaction:132 expected: &lt;1&gt; but was: &lt;2&gt;\n[INFO] \n[ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0\n</code></pre> <ul> <li> <p>We see that our recently added test failed. And this is expected due to the fact that we inserted two records but our test expects one. To remedy this test we must change <code>Assertions.assertEquals(1, store.approximateNumEntries());</code> Set to 2 the comparisson.</p> </li> <li> <p>Next let's add another very simple test. Copy the following code to your Java test class:</p> </li> </ul> <pre><code>    @Test\n    public void testErrorTopicIsNotEmpty() {\n        FinancialMessage mock = new FinancialMessage(\n            null, \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n\n        inTopic.pipeInput(\"T03\", mock);\n\n        Assertions.assertFalse(errorTopic.isEmpty());\n    }\n</code></pre> <ul> <li>Test the application by running the following:</li> </ul> <pre><code>./mvnw clean verify\n</code></pre> <ul> <li>You should see the following output:</li> </ul> <pre><code>[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-16 17:29:34,258 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.470s. Listening on: http://localhost:8081\n2021-01-16 17:29:34,260 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-16 17:29:34,260 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.694 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\n2021-01-16 17:29:36,001 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.309 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-16 17:29:36,057 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-16 17:29:36,059 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.049 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-16 17:29:36,099 INFO  [io.quarkus] (main) Quarkus stopped in 0.031s\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0\n</code></pre> <p>As you can see here, our message payload is created with <code>null</code> for the userId field which means this message will branch out to the <code>errorTopic</code>. The purpose of the test is to check if our <code>errorTopic</code> is empty, which should not be. Since our <code>errorTopic.isEmpty()</code> resolves to false and our assertion is asserting that it is false as well, thus the test passes.</p> <ul> <li>Now that we have two simple tests, let's update our first branch to allow us to filter the stream on a condition that we want. Let's edit our <code>branches[1]</code> statement so that it will filter out and retain only the records where the <code>totalCost</code> is greater than 5000.</li> </ul> <pre><code>branches[1].filter(\n            (key, value) -&gt; (value.totalCost &gt; 5000)\n        )\n        .groupBy(\n            (key, value) -&gt; value.userId\n        )\n        .count(\n            Materialized.as(storeSupplier)\n        )\n        .toStream()\n        .to(\n            outTopicName,\n            Produced.with(Serdes.String(), Serdes.Long())\n        );\n</code></pre> <ul> <li>Test the application by running the following:</li> </ul> <pre><code>./mvnw clean verify\n</code></pre> <ul> <li>You should see the following output:</li> </ul> <pre><code>[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-16 17:40:50,765 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.102s. Listening on: http://localhost:8081\n2021-01-16 17:40:50,766 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-16 17:40:50,766 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.474 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\n2021-01-16 17:40:52,393 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.307 s &lt;&lt;&lt; FAILURE! - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[ERROR] shouldHaveOneTransaction  Time elapsed: 0.022 s  &lt;&lt;&lt; FAILURE!\norg.opentest4j.AssertionFailedError: expected: &lt;2&gt; but was: &lt;1&gt;\n    at com.ibm.garage.cpat.lab.TestFinancialMessage.shouldHaveOneTransaction(TestFinancialMessage.java:135)\n\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-16 17:40:52,445 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-16 17:40:52,447 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.046 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-16 17:40:52,487 INFO  [io.quarkus] (main) Quarkus stopped in 0.031s\n[INFO] \n[INFO] Results:\n[INFO] \n[ERROR] Failures: \n[ERROR]   TestFinancialMessage.shouldHaveOneTransaction:135 expected: &lt;2&gt; but was: &lt;1&gt;\n[INFO] \n[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0\n</code></pre> <p>We see that our first tests is now failing again. And this is expected because we are changing the logic of how <code>branches[1]</code> works to filter out those transactions less than <code>5000</code>. This makes the second record we send in to get filtered out. In order to fix our test again, we either decrease the assertion for the expected entries in our store back to 1 or we modify the amount of the second transaction to be greater than <code>5000</code>. Once we do that, if we test the application again, we should get all tests passing.</p>"},{"location":"use-cases/kafka-streams/lab-1/#next-steps","title":"Next Steps","text":"<ul> <li>Now that you have finished this initial part of Lab 1 you can optionally proceed to Lab 2</li> </ul>"},{"location":"use-cases/kafka-streams/lab-2/","title":"Kafka Streams Test Lab 2","text":""},{"location":"use-cases/kafka-streams/lab-2/#overview","title":"Overview","text":"<ul> <li>This is a continuation of the previous Lab 1. You should complete Lab 1 first before you get started here.</li> <li>There's a few more pre-reqs (if you so choose to use them) outlined below.</li> </ul>"},{"location":"use-cases/kafka-streams/lab-2/#scenario-prerequisites","title":"Scenario Prerequisites","text":"<p>Java</p> <ul> <li>For the purposes of this lab we suggest Java 8+</li> </ul> <p>Maven</p> <ul> <li>Maven will be needed for bootstrapping our application from the command-line and running our application.</li> </ul> <p>An IDE of your choice</p> <ul> <li>Ideally an IDE that supports Quarkus (such as Visual Studio Code)</li> </ul> <p>OpenShift Container Platform, IBM Cloud Pak for Integration and IBM Event Streams</p> <ul> <li> <p>This is an optional portion of the lab for those who have access to an OCP Cluster where IBM Cloud Pak for Integration has been installed on top and an IBM Event Streams instance deployed.</p> </li> <li> <p>The following are optional</p> </li> <li>OpenShift Container Platform v4.4.x</li> <li>IBM Cloud Pak for Integration CP4I2022.2</li> <li>IBM Event Streams: IBM Event Streams v10 or later preferrably. If you are using a previous version of IBM Event Streams, there are some differences as to how you would configure <code>application.properties</code> to establish the connection to IBM Event Streams.</li> </ul>"},{"location":"use-cases/kafka-streams/lab-2/#adding-in-more-kafka-streams-operators","title":"Adding in more Kafka Streams operators","text":"<p>In this section we are going to to add more functionality to our previous test class in order to see, work with and understand more Kafka Streams operators.</p> <ul> <li>Add the following definitions to the <code>TestFinancialMessage.java</code> Java class:</li> </ul> <pre><code>private static String tradingTable = \"tradingTable\";\nprivate static String tradingStoreName = \"tradingStore\";\nprivate static TestInputTopic&lt;String, String&gt; tradingTableTopic;\n</code></pre> <ul> <li>Add the following Store and KTable definitions inside the <code>buildTopology()</code> function to support the trading fuctionality we are adding to our application:</li> </ul> <pre><code>KeyValueBytesStoreSupplier tradingStoreSupplier = Stores.persistentKeyValueStore(tradingStoreName);\n\nKTable&lt;String, String&gt; stockTradingStore = builder.table(tradingTable,\n            Consumed.with(Serdes.String(), Serdes.String()),\n            Materialized.as(tradingStoreSupplier));\n</code></pre> <ul> <li>Add the following import to your Java class so that you can use objects of type KTable:</li> </ul> <pre><code>import org.apache.kafka.streams.kstream.KTable;\n</code></pre> <ul> <li>Edit the <code>branch[1]</code> logic again to create new <code>KeyValue</code> pairs of <code>userId</code> and <code>stockSymbol</code></li> </ul> <pre><code>branches[1].filter(\n            (key, value) -&gt; (value.totalCost &gt; 5000)\n        )\n        .map(\n            (key, value) -&gt; KeyValue.pair(value.userId, value.stockSymbol)\n        )\n        .to(\n            tradingTable,\n            Produced.with(Serdes.String(), Serdes.String())\n        );\n</code></pre> <p>Notice that, previously, we wrote straight to <code>outTopic</code>. However, we are now writing to a KTable which we can query in our tests by using the State Store it is materialised as.</p> <ul> <li> <p>Before we create a test for the new functionality, remove or comment out the previous existing tests cases as these do no longer apply.</p> </li> <li> <p>Create a new test with the code below:</p> </li> </ul> <pre><code>    @Test\n    public void filterAndMapNewPair() {\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        inTopic.pipeInput(\"1\", mock);\n\n        KeyValueStore&lt;String,ValueAndTimestamp&lt;String&gt;&gt; tableStore = testDriver.getTimestampedKeyValueStore(tradingStoreName);\n        Assertions.assertEquals(1, tableStore.approximateNumEntries());\n        Assertions.assertEquals(\"MET\", tableStore.get(\"1\").value());\n    }\n</code></pre> <p>The first assertion checks whether the store has a record. The second assertion checks that the mock record that we inserted has the correct value as our map function created new KeyValue pairs of type <code>&lt;userId, stockSymbol&gt;</code>.</p> <ul> <li>Test the application by running the following:</li> </ul> <pre><code>./mvnw clean verify\n</code></pre> <ul> <li>You should see the tests pass with the following output:</li> </ul> <pre><code>[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-16 20:47:06,478 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.086s. Listening on: http://localhost:8081\n2021-01-16 20:47:06,479 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-16 20:47:06,479 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.611 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\n2021-01-16 20:47:08,248 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.276 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-16 20:47:08,290 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-16 20:47:08,292 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.035 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-16 20:47:08,325 INFO  [io.quarkus] (main) Quarkus stopped in 0.026s\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0\n</code></pre> <p>Now we are going to do something a little bit more advanced. We are going to join a KStream with a KTable. The Streams API has an inner join, left join, and an outer join. KStream-KTable joins are non-windowed and asymmetric. By asymmetric we mean that a join only gets triggered if the left input KStream gets a new record while the right KTable holds the latest input records materialized.</p> <ul> <li>Add the following new attributes:</li> </ul> <pre><code>    private static String joinedTopicName = \"joinedTopic\";\n    private static TestOutputTopic&lt;String, String&gt; joinedTopic;\n    private static String joinedStoreName = \"joinedStore\";\n</code></pre> <ul> <li>Replace the <code>buildTopology()</code> function for the following new one:</li> </ul> <pre><code>public static void buildTopology() {\n        final StreamsBuilder builder = new StreamsBuilder();\n        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);\n        KeyValueBytesStoreSupplier tradingStoreSupplier = Stores.persistentKeyValueStore(tradingStoreName);\n        KeyValueBytesStoreSupplier joinedStoreSupplier = Stores.persistentKeyValueStore(joinedStoreName);\n\n        KStream&lt;String, FinancialMessage&gt; transactionStream =\n            builder.stream(\n                inTopicName,\n                Consumed.with(Serdes.String(), financialMessageSerde)\n            );\n\n        KTable&lt;String, String&gt; stockTradingStore = builder.table(tradingTable,\n            Consumed.with(Serdes.String(), Serdes.String()),\n            Materialized.as(tradingStoreSupplier));\n\n        KTable&lt;String, String&gt; joinedMessageStore = builder.table(joinedTopicName,\n            Consumed.with(Serdes.String(), Serdes.String()),\n            Materialized.as(joinedStoreSupplier));\n\n        KStream&lt;String, String&gt; joinedStream = transactionStream.join(\n            stockTradingStore,\n            (financialMessage, companyName) -&gt; \"userId = \" + financialMessage.userId + \" companyName = \" + companyName);\n\n        joinedStream.to(\n            joinedTopicName,\n            Produced.with(Serdes.String(), Serdes.String()));\n\n        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());\n        inTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new JsonbSerializer&lt;FinancialMessage&gt;());\n        tradingTableTopic = testDriver.createInputTopic(tradingTable, new StringSerializer(), new StringSerializer());\n        joinedTopic = testDriver.createOutputTopic(joinedTopicName, new StringDeserializer(), new StringDeserializer());\n    }\n</code></pre> <p>We can see that our <code>buildTopology()</code> function still contains the <code>transactionsStream</code> KStream that will contain the stream of <code>FinancialMessages</code> being received through the <code>inTopicName</code>. Then, We can see a KTable called <code>stockTradingStore</code>, which will get materialized as a State Store, that will contain the messages comming in through the input topic called <code>tradingTable</code>. This KTable will hold data about the tradding companies that will serve to enhance the incoming <code>FinancialMessages</code> with. The join between the KStream and the KTable is being done below and is called <code>joinedStream</code>. The result of this join is being outputed into a topic called <code>joinedTopicName</code>. Finally, this output topic is being store in a KTable called <code>joinedMessageStore</code>, and materialized in its respective State Store, in order to be able to query it later on in our tests. The inner join is performed on matching keys between the KStream and the KTable and the matched records produce a new <code>&lt;String, String&gt;</code> pair with the value of <code>userId</code> and <code>companyName</code>.</p> <p>In order to test the above new functionality of the <code>buildTopology()</code> function, remove or comment out the existing test and create the following new one:</p> <pre><code>    @Test\n    public void checkStreamAndTableJoinHasOneRecord() {\n\n        tradingTableTopic.pipeInput(\"1\", \"Metropolitan Museum of Art\");\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        inTopic.pipeInput(\"1\", mock);\n\n        KeyValueStore&lt;String,ValueAndTimestamp&lt;String&gt;&gt; joinedTableStore = testDriver.getTimestampedKeyValueStore(joinedStoreName);\n        Assertions.assertEquals(1, joinedTableStore.approximateNumEntries());\n        System.out.println(joinedTableStore.get(\"1\").value());\n    }\n</code></pre> <ul> <li>Test the application by running the following:</li> </ul> <pre><code>./mvnw clean verify\n</code></pre> <ul> <li>You should see the following output:</li> </ul> <pre><code>[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-16 21:53:03,682 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.134s. Listening on: http://localhost:8081\n2021-01-16 21:53:03,684 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-16 21:53:03,685 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.574 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\nuserId = 1 companyName = Metropolitan Museum of Art\n2021-01-16 21:53:05,432 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.294 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-16 21:53:05,482 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-16 21:53:05,484 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.044 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-16 21:53:05,518 INFO  [io.quarkus] (main) Quarkus stopped in 0.026s\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0\n</code></pre> <p>We can see that our test passes successfully as the <code>FinancialMessage</code> with <code>userId=1</code> and the KTable record with key <code>1</code> successfully joined to produce the output message: <code>userId = 1 companyName = Metropolitan Museum of Art</code></p> <p>We are now going to create yet another new test class. But first, we are going to create two new POJO classes to work with. These are <code>EnrichedMessage.java</code> and <code>AggregatedMessage.java</code>. Both of them should be placed where the previos POJO class is: <code>src/main/java/com/ibm/garage/cpat/domain</code>.</p> <ul> <li>Create the <code>EnrichedMessage.java</code> Java file with the following code:</li> </ul> <pre><code>package com.ibm.garage.cpat.domain;\n\n\npublic class EnrichedMessage {\n\n    public String userId;\n    public String stockSymbol;\n    public int quantity;\n    public double stockPrice;\n    public double totalCost;\n    public double adjustedCost;\n    public boolean technicalValidation;\n    public String companyName;\n\n    public EnrichedMessage (FinancialMessage message, String companyName) {\n        this.userId = message.userId;\n        this.stockSymbol = message.stockSymbol;\n        this.quantity = message.quantity;\n        this.stockPrice = message.stockPrice;\n        this.totalCost = message.totalCost;\n        this.companyName = companyName;\n\n        if (message.technicalValidation)\n        {\n            this.technicalValidation = message.technicalValidation;\n            this.adjustedCost = message.totalCost * 1.15;\n        }\n\n        else {\n            this.technicalValidation = message.technicalValidation;\n            this.adjustedCost = message.totalCost;\n        }\n    }\n}\n</code></pre> <ul> <li>Create the <code>AggregatedMessage.java</code> Java file with the following code:</li> </ul> <pre><code>package com.ibm.garage.cpat.domain;\n\nimport java.math.BigDecimal;\nimport java.math.RoundingMode;\n\n\npublic class AggregatedMessage {\n\n    public String userId;\n    public String stockSymbol;\n    public int quantity;\n    public double stockPrice;\n    public double totalCost;\n    public double adjustedCost;\n    public boolean technicalValidation;\n    public String companyName;\n    public int count;\n    public double sum;\n    public double average;\n\n    public AggregatedMessage updateFrom(EnrichedMessage message) {\n        this.userId = message.userId;\n        this.stockSymbol = message.stockSymbol;\n        this.quantity = message.quantity;\n        this.stockPrice = message.stockPrice;\n        this.totalCost = message.totalCost;\n        this.companyName = message.companyName;\n        this.adjustedCost = message.adjustedCost;\n        this.technicalValidation = message.technicalValidation;\n\n        this.count ++;\n        this.sum += message.adjustedCost;\n        this.average = BigDecimal.valueOf(sum / count)\n                    .setScale(1, RoundingMode.HALF_UP).doubleValue();\n\n        return this;\n    }\n}\n</code></pre> <ul> <li>Now create the new test class named <code>TestAggregate.java</code> in the same path we have the other test classes (<code>src/test/java/com/ibm/garage/cpat</code>) and paste the following code:</li> </ul> <pre><code>package com.ibm.garage.cpat.lab;\n\nimport java.util.Properties;\n\nimport org.apache.kafka.common.serialization.LongDeserializer;\nimport org.apache.kafka.common.serialization.Serde;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.common.serialization.StringDeserializer;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport org.apache.kafka.streams.KeyValue;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.TestInputTopic;\nimport org.apache.kafka.streams.TestOutputTopic;\nimport org.apache.kafka.streams.TopologyTestDriver;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KGroupedStream;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.apache.kafka.streams.kstream.KTable;\nimport org.apache.kafka.streams.kstream.Materialized;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.kstream.Windowed;\nimport org.apache.kafka.streams.kstream.WindowedSerdes;\nimport org.apache.kafka.streams.processor.StateStore;\nimport org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;\nimport org.apache.kafka.streams.state.KeyValueIterator;\nimport org.apache.kafka.streams.state.KeyValueStore;\nimport org.apache.kafka.streams.state.Stores;\nimport org.apache.kafka.streams.state.ValueAndTimestamp;\nimport org.junit.jupiter.api.AfterAll;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\nimport io.quarkus.kafka.client.serialization.JsonbDeserializer;\nimport io.quarkus.kafka.client.serialization.JsonbSerde;\nimport io.quarkus.kafka.client.serialization.JsonbSerializer;\nimport io.quarkus.test.junit.QuarkusTest;\n\nimport com.ibm.garage.cpat.domain.*;\n\n\n@QuarkusTest\npublic class TestAggregate {\n\n    private static TopologyTestDriver testDriver;\n    private static String inTopicName = \"financialMessages\";\n    private static String outTopicName = \"enrichedMessages\";\n    private static String storeName = \"financialStore\";\n    private static String aggregatedTopicName = \"aggregatedMessages\";\n\n    private static String companyTable = \"companyTable\";\n    private static String companyStoreName = \"companyStore\";\n\n    private static TestInputTopic&lt;String, FinancialMessage&gt; inTopic;\n    private static TestOutputTopic&lt;String, EnrichedMessage&gt; outTopic;\n    private static TestOutputTopic&lt;String, AggregatedMessage&gt; aggregatedTopic;\n    private static TestInputTopic&lt;String, String&gt; companyTableTopic;\n\n    private static final JsonbSerde&lt;FinancialMessage&gt; financialMessageSerde = new JsonbSerde&lt;&gt;(FinancialMessage.class);\n    private static final JsonbSerde&lt;EnrichedMessage&gt; enrichedMessageSerde = new JsonbSerde&lt;&gt;(EnrichedMessage.class);\n    private static final JsonbSerde&lt;AggregatedMessage&gt; aggregatedMessageSerde = new JsonbSerde&lt;&gt;(AggregatedMessage.class);\n\n\n    public static Properties getStreamsConfig() {\n        final Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"kstream-lab3\");\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummmy:3456\");\n        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        //props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, financialMessageSerde);\n        return props;\n    }\n\n    @BeforeAll\n    public static void buildTopology() {\n        final StreamsBuilder builder = new StreamsBuilder();\n        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);\n        KeyValueBytesStoreSupplier companyStoreSupplier = Stores.persistentKeyValueStore(companyStoreName);\n\n        // create a KStream for financial messages.\n        KStream&lt;String, FinancialMessage&gt; financialStream =\n            builder.stream(\n                inTopicName,\n                Consumed.with(Serdes.String(), financialMessageSerde)\n            );\n\n        // create a KTable from a topic for companies.\n        KTable&lt;String, String&gt; companyStore = builder.table(companyTable,\n            Consumed.with(Serdes.String(), Serdes.String()),\n            Materialized.as(companyStoreSupplier));\n\n        // join KStream with KTable and use aggregate.\n        KStream&lt;String, EnrichedMessage&gt; enrichedStream = financialStream.join(\n                companyStore,\n                //(financialMessage, companyName) -&gt; financialMessage.userId,\n                (financialMessage, companyName) -&gt; {\n                    return new EnrichedMessage(financialMessage, companyName);\n                }\n            );\n\n        enrichedStream.groupByKey()\n            .aggregate(\n                AggregatedMessage::new,\n                (userId, value, aggregatedMessage) -&gt; aggregatedMessage.updateFrom(value),\n                Materialized.&lt;String, AggregatedMessage&gt; as(storeSupplier)\n                                .withKeySerde(Serdes.String())\n                                .withValueSerde(aggregatedMessageSerde)\n            )\n            .toStream()\n            .to(\n                aggregatedTopicName,\n                Produced.with(Serdes.String(), aggregatedMessageSerde)\n            );\n\n        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());\n        inTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new JsonbSerializer&lt;FinancialMessage&gt;());\n        //outTopic = testDriver.createOutputTopic(outTopicName, new StringDeserializer(), new JsonbDeserializer&lt;&gt;(EnrichedMessage.class));\n        companyTableTopic = testDriver.createInputTopic(companyTable, new StringSerializer(), new StringSerializer());\n        aggregatedTopic = testDriver.createOutputTopic(aggregatedTopicName, new StringDeserializer(), new JsonbDeserializer&lt;&gt;(AggregatedMessage.class));\n    }\n\n    @AfterAll\n    public static void close(){\n        testDriver.close();\n    }\n}\n</code></pre> <p>Even though the code above might seem a completely new one at first glance, it is just an extra step of the previous code we have been working with. We can see that we still have both our <code>financialStream</code> KStream receiving <code>FinancialMessage</code> from the <code>inTopicName</code> topic and our <code>companyStore</code> KTable holding the latest being received from the <code>companyTable</code> input topic. Then, we also still have the join of the previous two KStream and KTable in our <code>enrichedStream</code> KStream that will be a KStream of <code>EnrichedMessage</code>. What is new in this code is the following aggregation we are doing on the resulting <code>enrichedStream</code> KStream. We are grouping <code>EnrichedMessage</code> objects by key and doing an aggregation of that grouping. The aggregation result will be an <code>AggregatedMessage</code> which will get materialized as <code>storeSupplier</code> so that we can query it later on. We are also converting that KTable to a KStream that will get outputed to <code>aggregatedTopicName</code>. The aggregate logic will simply work out some average and count of <code>EnrichedMessage</code> you can check out in the <code>EnrichedMessage.java</code> Java file.</p> <ul> <li>Now add a test that will make sure the functionality we have implemented in the <code>buildTopology()</code> function works as desired:</li> </ul> <pre><code>    @Test\n    public void aggregatedMessageExists() {\n\n        companyTableTopic.pipeInput(\"1\", \"Metropolitan Museum of Art\");\n\n        FinancialMessage mock = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n        FinancialMessage mock2 = new FinancialMessage(\n            \"1\", \"MET\", \"SWISS\", 12, 1822.38, 6634.56, 94, 7, true\n        );\n        inTopic.pipeInput(\"1\", mock);\n        inTopic.pipeInput(\"1\", mock2);\n\n        KeyValueStore&lt;String,ValueAndTimestamp&lt;AggregatedMessage&gt;&gt; aggregatedTableStore = testDriver.getTimestampedKeyValueStore(storeName);\n        Assertions.assertEquals(2, aggregatedTableStore.approximateNumEntries());\n        System.out.println(\"Average = \" + aggregatedTableStore.get(\"1\").value().average);\n        Assertions.assertEquals(16389.3, aggregatedTableStore.get(\"2\").value().average);\n    }\n</code></pre> <ul> <li>Test the application by running the following:</li> </ul> <pre><code>./mvnw clean verify\n</code></pre> <ul> <li>You should see the following output:</li> </ul> <pre><code>[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-17 13:13:02,533 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.178s. Listening on: http://localhost:8081\n2021-01-17 13:13:02,535 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-17 13:13:02,535 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.441 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestAggregate\nAverage = 16389.3\n2021-01-17 13:13:04,216 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.339 s &lt;&lt;&lt; FAILURE! - in com.ibm.garage.cpat.lab.TestAggregate\n[ERROR] aggregatedMessageExists  Time elapsed: 0.122 s  &lt;&lt;&lt; ERROR!\njava.lang.NullPointerException\n    at com.ibm.garage.cpat.lab.TestAggregate.aggregatedMessageExists(TestAggregate.java:145)\n\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\nuserId = 1 companyName = Metropolitan Museum of Art\n2021-01-17 13:13:04,283 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.055 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-17 13:13:04,333 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-17 13:13:04,335 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.043 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-17 13:13:04,372 INFO  [io.quarkus] (main) Quarkus stopped in 0.029s\n[INFO] \n[INFO] Results:\n[INFO] \n[ERROR] Errors: \n[ERROR]   TestAggregate.aggregatedMessageExists:145 NullPointer\n[INFO] \n[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0\n</code></pre> <p>We see the test fails, but that is expected. In this test we have two different <code>financialMessage</code> inserted with a key of <code>\"1\"</code> and there is only one entry in the KTable <code>(\"1\", \"Metropolitan Museum of Art\")</code>. Those two <code>financialMessage</code> will get enriched with the only record in the KTable. Later on, those two <code>EnrichedMessage</code> should get grouped by as they have the same key and result in an <code>AggregatedMessage</code>. There are two assertions in this test. The first one passes as the <code>aggregatedTableStore</code> contains two entries: The first <code>EnrichedMessage</code> that eas aggregated with a new empty <code>AggregatedMessage</code> as the initializer and then the second <code>EnrichedMessage</code> that it was aggregated with the resulting <code>AggregatedMessage</code> of the previous <code>EnrichedMessage</code>. However, the second second assertion fails. The reason for this is that even though there are two records in the <code>aggregatedTableStore</code>, this store is key based and will return, as a result, the latest <code>AggregatedMessage</code> it holds for a particular key. Then, if we want to retrieve the latest <code>AggregatedMessage</code> for out key <code>1</code> we need to change the assetion:</p> <pre><code>Assertions.assertEquals(16389.3, aggregatedTableStore.get(\"2\").value().average);\n</code></pre> <p>to use the appropriate key:</p> <pre><code>Assertions.assertEquals(16389.3, aggregatedTableStore.get(\"1\").value().average);\n</code></pre> <ul> <li>Test the application by running the following:</li> </ul> <pre><code>./mvnw clean verify\n</code></pre> <ul> <li>You should see the following output:</li> </ul> <pre><code>[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.ibm.GreetingResourceTest\n2021-01-17 13:14:40,370 INFO  [io.quarkus] (main) Quarkus 1.10.5.Final on JVM started in 2.025s. Listening on: http://localhost:8081\n2021-01-17 13:14:40,371 INFO  [io.quarkus] (main) Profile test activated. \n2021-01-17 13:14:40,372 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-streams, resteasy, resteasy-jsonb]\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.495 s - in com.ibm.GreetingResourceTest\n[INFO] Running com.ibm.garage.cpat.lab.TestAggregate\nAverage = 16389.3\n2021-01-17 13:14:42,114 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.33 s - in com.ibm.garage.cpat.lab.TestAggregate\n[INFO] Running com.ibm.garage.cpat.lab.TestFinancialMessage\nuserId = 1 companyName = Metropolitan Museum of Art\n2021-01-17 13:14:42,177 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.051 s - in com.ibm.garage.cpat.lab.TestFinancialMessage\n[INFO] Running com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\nC01:Health Care\nC02:Finance\nC03:Consumer Services\nC04:Transportation\nC05:Capital Goods\nC06:Public Utilities\nsector-types-store\n2021-01-17 13:14:42,227 WARN  [org.apa.kaf.str.sta.int.RocksDBStore] (main) Closing 1 open iterators for store sector-types-store\n2021-01-17 13:14:42,229 INFO  [org.apa.kaf.str.pro.int.StateDirectory] (main) stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.042 s - in com.ibm.garage.cpat.lab.TestLoadKtableFromTopic\n2021-01-17 13:14:42,272 INFO  [io.quarkus] (main) Quarkus stopped in 0.035s\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0\n</code></pre>"},{"location":"use-cases/kafka-streams/lab-2/#producing-to-and-consuming-from-a-kafka-topic-on-event-streams","title":"Producing to and Consuming from a Kafka Topic on Event Streams","text":"<p>When using the Kafka Streams API, we usually do it against a Kafka instance containing data already in, at least, one of its topics. In order to build up that scenario, we are going to use the MicroProfile Reactive Messaging library to send messages to a topic.</p> <ul> <li>For using the MicroProfile Reactive Messaging library, we first need to add such dependency to our <code>pom.xml</code> file:</li> </ul> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-smallrye-reactive-messaging-kafka&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <ul> <li>Create the <code>MockProducer.java</code> Java file in <code>src/main/java/com/ibm/garage/cpat/infrastructure</code> with the following code:</li> </ul> <pre><code>package com.ibm.garage.cpat.infrastructure;\n\nimport javax.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.reactivex.Flowable;\nimport io.smallrye.reactive.messaging.kafka.KafkaRecord;\n\nimport java.util.concurrent.TimeUnit;\nimport java.util.Random;\n\nimport com.ibm.garage.cpat.domain.*;\n\n\n@ApplicationScoped\npublic class MockProducer {\n\n    private Random random = new Random();\n\n    FinancialMessage mock = new FinancialMessage(\n        \"1\", \"MET\", \"SWISS\", 12, 1822.38, 21868.55, 94, 7, true\n        );\n\n    @Outgoing(\"mock-messages\")\n    public Flowable&lt;KafkaRecord&lt;String,FinancialMessage&gt;&gt; produceMock() {\n        return Flowable.interval(5, TimeUnit.SECONDS)\n                       .map(tick -&gt; {\n                            return setRandomUserId(mock);\n                        });\n    }\n\n    public KafkaRecord&lt;String, FinancialMessage&gt; setRandomUserId(FinancialMessage mock) {\n        mock.userId = String.valueOf(random.nextInt(100));\n\n        return KafkaRecord.of(mock.userId, mock);\n    }\n}\n</code></pre> <p>The producer code above produces a <code>FinancialMessage</code> every 5 seconds to the <code>mock-messages</code> channel with a random userId (out of 100). We will see later on how the <code>mock-messages</code> channel relates to a Kafka topic through configuration.</p> <ul> <li>Next, create the topology that we are going to build for processing the messages sent by our previous producer. Create a <code>FinancialMessageTopology.java</code> Java file in <code>src/main/java/com/ibm/garage/cpat/domain</code> with the following code:</li> </ul> <pre><code>package com.ibm.garage.cpat.domain;\n\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Produces;\n\nimport org.eclipse.microprofile.config.inject.ConfigProperty;\n\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.Topology;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.Produced;\n\nimport io.quarkus.kafka.client.serialization.JsonbSerde;\n\n\n@ApplicationScoped\npublic class FinancialMessageTopology {\n\n    @ConfigProperty(name = \"START_TOPIC_NAME\")\n    private String INCOMING_TOPIC;\n\n    @ConfigProperty(name = \"TARGET_TOPIC_NAME\")\n    private String OUTGOING_TOPIC;\n\n\n    @Produces\n    public Topology buildTopology() {\n\n        StreamsBuilder builder = new StreamsBuilder();\n\n        JsonbSerde&lt;FinancialMessage&gt; financialMessageSerde = new JsonbSerde&lt;&gt;(FinancialMessage.class);\n\n        // Stream reads from input topic, filters it by checking the boolean field on the message.\n        // If the boolean is true, it gets passed to the mapValues function which will then send that record\n        // to an outgoing topic.\n\n        builder.stream(\n            INCOMING_TOPIC,\n            Consumed.with(Serdes.String(), financialMessageSerde)\n        )\n        .filter (\n            (key, message) -&gt; checkValidation(message)\n        )\n        .mapValues (\n            checkedMessage -&gt; adjustPostValidation(checkedMessage)\n        )\n        .to (\n            OUTGOING_TOPIC,\n            Produced.with(Serdes.String(), financialMessageSerde)\n        );\n\n        return builder.build();\n    }\n\n    public boolean checkValidation(FinancialMessage message) {\n        return (message.technicalValidation);\n    }\n\n    public FinancialMessage adjustPostValidation(FinancialMessage message) {\n        message.totalCost = message.totalCost * 1.15;\n\n        return message;\n    }\n\n}\n</code></pre> <p>The code above builds a KStream from the messages in <code>INCOMING_TOPIC</code>. It will filter the <code>FinanacialMessage</code> based on a boolean property of them. Then, it will do some adjustment of the cost attribute within that <code>FinancialMessage</code> post message validation and send that out to the <code>OUTGOING_TOPIC</code>.</p> <p>Now, we have seen the code for both the mock producer and the topology we want to build are dependant on certain configuration variables such as the input and output topics they are meant to produce to and consume from. In a Quarkus application, all the configuration settings is done through a properties file, which makes the application portable across different environments. This property file, which we already had to configure in previous labs is called <code>application.properties</code> and is located in <code>src/main/resources</code>. </p> <ul> <li>In order to configure the application to work with an IBM Event Streams v10 or later, paste the following configuration in the <code>application.properties</code> file:</li> </ul> <pre><code>quarkus.http.port=8080\nquarkus.log.console.enable=true\nquarkus.log.console.level=INFO\n\n# Base ES Connection Details\nmp.messaging.connector.smallrye-kafka.bootstrap.servers=${BOOTSTRAP_SERVERS}\nmp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\nmp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\nmp.messaging.connector.smallrye-kafka.sasl.mechanism=SCRAM-SHA-512\nmp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                username=${SCRAM_USERNAME} \\\n                password=${SCRAM_PASSWORD};\nmp.messaging.connector.smallrye-kafka.ssl.truststore.location=${CERT_LOCATION}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.password=${CERT_PASSWORD}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.type=PKCS12\n\n\n# Initial mock JSON message producer configuration\nmp.messaging.outgoing.mock-messages.connector=smallrye-kafka\nmp.messaging.outgoing.mock-messages.topic=${START_TOPIC_NAME}\nmp.messaging.outgoing.mock-messages.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n\n\n\n# Quarkus Kafka Streams configuration settings\nquarkus.kafka-streams.bootstrap-servers=${BOOTSTRAP_SERVERS}\nquarkus.kafka-streams.application-id=financial-stream\nquarkus.kafka-streams.application-server=localhost:8080\nquarkus.kafka-streams.topics=${START_TOPIC_NAME},${TARGET_TOPIC_NAME}\nquarkus.kafka-streams.health.enabled=true\n\nquarkus.kafka-streams.security.protocol=SASL_SSL\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\nquarkus.kafka-streams.sasl.mechanism=SCRAM-SHA-512\nquarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \\\n                username=${SCRAM_USERNAME} \\\n                password=${SCRAM_PASSWORD};\nquarkus.kafka-streams.ssl.truststore.location=${CERT_LOCATION}\nquarkus.kafka-streams.ssl.truststore.password=${CERT_PASSWORD}\nquarkus.kafka-streams.ssl.truststore.type=PKCS12\n\n# pass-through options\nkafka-streams.cache.max.bytes.buffering=10240\nkafka-streams.commit.interval.ms=1000\nkafka-streams.metadata.max.age.ms=500\nkafka-streams.auto.offset.reset=latest\nkafka-streams.metrics.recording.level=DEBUG\n</code></pre> <ul> <li>If using a previous IBM Event Streams version (such as v2019.4.2) or on IBM Cloud, use the following configuration in the <code>application.properties</code> file:</li> </ul> <pre><code>quarkus.http.port=8080\nquarkus.log.console.enable=true\nquarkus.log.console.level=INFO\n\n# Base ES Connection Details\nmp.messaging.connector.smallrye-kafka.bootstrap.servers=${BOOTSTRAP_SERVERS}\nmp.messaging.connector.smallrye-kafka.security.protocol=SASL_SSL\nmp.messaging.connector.smallrye-kafka.ssl.protocol=TLSv1.2\nmp.messaging.connector.smallrye-kafka.sasl.mechanism=PLAIN\nmp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.PlainLoginModule required \\\n                username=\"token\" \\\n                password=${API_KEY};\n# If connecting to Event Streams on IBM Cloud the following truststore options are not needed.\nmp.messaging.connector.smallrye-kafka.ssl.truststore.location=${CERT_LOCATION}\nmp.messaging.connector.smallrye-kafka.ssl.truststore.password=password\n\n\n# Initial mock JSON message producer configuration\nmp.messaging.outgoing.mock-messages.connector=smallrye-kafka\nmp.messaging.outgoing.mock-messages.topic=${START_TOPIC_NAME}\nmp.messaging.outgoing.mock-messages.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n\n\n\n# Quarkus Kafka Streams configuration settings\nquarkus.kafka-streams.bootstrap-servers=${BOOTSTRAP_SERVERS}\nquarkus.kafka-streams.application-id=financial-stream\nquarkus.kafka-streams.application-server=localhost:8080\nquarkus.kafka-streams.topics=${START_TOPIC_NAME},${TARGET_TOPIC_NAME}\nquarkus.kafka-streams.health.enabled=true\n\nquarkus.kafka-streams.security.protocol=SASL_SSL\nquarkus.kafka-streams.ssl.protocol=TLSv1.2\nquarkus.kafka-streams.sasl.mechanism=PLAIN\nquarkus.kafka-streams.sasl.jaas.config=org.apache.kafka.common.security.scram.PlainLoginModule required \\\n                username=\"token\" \\\n                password=${API_KEY};\n# If connecting to Event Streams on IBM Cloud the following truststore options are not needed.\nquarkus.kafka-streams.ssl.truststore.location=${CERT_LOCATION}\nquarkus.kafka-streams.ssl.truststore.password=password\n\n# pass-through options\nkafka-streams.cache.max.bytes.buffering=10240\nkafka-streams.commit.interval.ms=1000\nkafka-streams.metadata.max.age.ms=500\nkafka-streams.auto.offset.reset=latest\nkafka-streams.metrics.recording.level=DEBUG\n</code></pre> <p>There are some environment variables our <code>application.properties</code> file depends on:</p> <ul> <li><code>START_TOPIC_NAME</code>: The Kafka topic the mock producer will produce <code>FinancialMessage</code> to and the topology consume messages from for processing. IMPORTANT: Use a topic name with some unique identifier if you are sharing the IBM Event Streams instance with other lab students. Also, you must create this topic in IBM Event Streams. See here for more details as to how to create a topic.</li> <li><code>TARGET_TOPIC_NAME</code>: The Kafka topic the topology will produce the processed <code>FinancialMessage</code> to. IMPORTANT: Use a topic name with some unique identifier if you are sharing the IBM Event Streams instance with other lab students. Also, you must create this topic in IBM Event Streams. See [here]((/use-cases/overview/pre-requisites) for more details as to how to create a topic</li> <li><code>BOOTSTRAP_SERVERS</code>: Your IBM Event Streams bootstrap server. See here for more details as to how to obtain these.</li> <li><code>CERT_LOCATION</code>: The location where the PKCS12 certificate for the SSL connection to the IBM Event Streams instance is. See here for more details as to how to obtain these.</li> <li><code>CERT_PASSWORD</code>: The password of the PKCS12 certificate. See here for more details as to how to obtain these.</li> <li><code>API_KEY</code> if you are using an IBM Event Streams instance in IBM Cloud or</li> <li><code>SCRAM_USERNAME</code> and <code>SCRAM_PASSWORD</code>: The SCRAM credentials for your application to get authenticated and authorized to work with IBM Event Streams. See here for more details as to how to obtain these.</li> </ul> <p>Export the variables and values on the terminal you will run the application from:</p> <ul> <li>IBM Event Strams v10 or later:</li> </ul> <pre><code>export BOOTSTRAP_SERVERS=your-bootstrap-server-address:443 \\\nexport START_TOPIC_NAME=name-of-topic-to-consume-from \\\nexport TARGET_TOPIC_NAME=name-of-topic-to-produce-to \\\nexport CERT_LOCATION=/path-to-pkcs12-cert/es-cert.p12 \\\nexport CERT_PASSWORD=certificate-password \\\nexport SCRAM_USERNAME=your-scram-username \\\nexport SCRAM_PASSWORD=your-scram-password\n</code></pre> <ul> <li>Previous IBM Event Streams versions:</li> </ul> <pre><code>export BOOTSTRAP_SERVERS=your-bootstrap-server-address:443 \\\nexport START_TOPIC_NAME=name-of-topic-to-consume-from \\\nexport TARGET_TOPIC_NAME=name-of-topic-to-produce-to \\\nexport CERT_LOCATION=/path-to-jks-cert/es-cert.jks \\\nexport API_KEY=your-api-key\n</code></pre> <ul> <li>Local Kafka Cluster:</li> </ul> <pre><code>export BOOTSTRAP_SERVERS=your-bootstrap-server-address:443 \\\nexport START_TOPIC_NAME=name-of-topic-to-consume-from \\\nexport TARGET_TOPIC_NAME=name-of-topic-to-produce-to \n</code></pre> <ul> <li>You can now test the Quarkus application</li> </ul> <pre><code>./mvnw quarkus:dev\n</code></pre> <ul> <li>You should now be able to see <code>FinancialMessage</code> events in the input topic and those processed messages in the output topic you have specified above in the IBM Event Streams user interface.</li> </ul>"},{"location":"use-cases/kafka-streams/lab-3/","title":"Kafka Streams Test Lab 3","text":"<p>Using Kafka Streams to compute real time inventory stock</p>"},{"location":"use-cases/kafka-streams/lab-3/#overview","title":"Overview","text":"<p>In this lab, we're going to use Quarkus to develop the near real-time inventory logic using Kafka Streams APIs and microprofile reactive messaging.</p> <p>The requirements to address are:</p> <ul> <li>consume item sold events from the <code>items</code> topic. Item has SKU as unique key. Item event has store ID reference</li> <li>the Kafka record in the <code>items</code> topic, uses the Store unique ID as key</li> <li>compute for each item its current stock cross stores</li> <li>compute the store's stock for each item</li> <li>generate inventory event for store - item - stock</li> <li>expose APIs to get stock for a store or for an item</li> </ul> <p></p> <p>The solution is using Kafka Streams and it includes two services. The components used are:</p> <p></p> <p>The goal of this lab, is to develop the green components which expose APIs to support Kafka Streams interactive query on top of the aggregates to keep the inventory views and saved in state store (light blue storage/per service deployed and persisted in Kafka as topic).</p> <p>We will be unit testing the stream logic using Apache Kafka Streams TopologyTestDriver class. </p> <p>This solution is deployed to OpenShift cluster with Strimzi running in the same cluster and namespace.</p> <p>This application needs the Item Store sell simulator to perform the end to end  testing and to demonstrate the end to end scenario.</p>"},{"location":"use-cases/kafka-streams/lab-3/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Access to an OpenShift Container Platform v4.6.x</li> <li>Kafka: The lab can use Event Streams on Cloud or Kafka Strimzi deployed on OpenShift.</li> <li> <p>Code Source: from the git repositories: </p> <ul> <li>https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory to compute item inventory cross store</li> <li>https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory to compute store inventory.</li> <li>https://github.com/ibm-cloud-architecture/eda-lab-inventory to get access to deployment configuration to deploy on OpenShift.</li> </ul> </li> </ul> <pre><code>git clone https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory\ngit clone https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory\ngit clone https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops\n</code></pre> <ul> <li>OC CLI</li> <li>As in previous labs, be connected to your Openshift cluster.</li> </ul> <p>We have automated the deployment of all the pieces making up this use case. </p>"},{"location":"use-cases/kafka-streams/lab-3/#ibm-openlabs","title":"IBM OpenLabs","text":"<p>In this section, we are going to see use the IBM OpenLabs hosted environment.</p> <ol> <li> <p>Go to IBM OpenLabs in a browser and click on <code>Launch Lab</code> button for Bring Your Own Application.</p> <p></p> </li> <li> <p>Sign in with your IBM Cloud account or register for an IBM Cloud account.</p> <p></p> </li> <li> <p>You will be presented with a dialog asking you whether you have an Opportunity Id or not. If you don't have it or don't no, just select No and click on Launch Lab.</p> </li> <li> <p>You should now see your IBM OpenLabs environment.</p> <p></p> </li> <li> <p>On the left hand side navigation menu, click on the Quick Links and Common Commands section.  Now, if you scroll down on the instructions shown on your screen, you should reach the Commonly Used Commands section of these and in there you should see an <code>oc login ...</code> command to get your terminal associated to this IBM OpenLabs logged into the OpenShift cluster that you will be working with for this quickstart tutorial.  Click on the <code>oc login...</code> command and you should see a <code>Login successful</code> message on the terminal.</p> <p></p> </li> </ol>"},{"location":"use-cases/kafka-streams/lab-3/#one-click-deploy-to-openshift","title":"One Click Deploy to OpenShift","text":"<p>The different components are deployed in the same namespace as the Kafka cluster, and use internal route to access Kafka bootstrap URL.</p> <p>The images for each of the components used are in the quay.io ibmcase repository:</p> <p></p> <p>The Gitops repository includes a makefile to deploy to different environment types. You can use the following command to deploy Event Streams, MQ broker and the apps in the same namespace:</p> <pre><code>make deploy_rt_inventory\n</code></pre>"},{"location":"use-cases/kafka-streams/lab-3/#testing-the-solution","title":"Testing the solution","text":"<p>We have moved the demonstration script to the gitop repository.</p>"},{"location":"use-cases/kafka-streams/lab-3/#understanding-the-kafka-streams-implementation","title":"Understanding the Kafka Streams implementation","text":"<p>The item and store aggregator code are based on the same code structure, reflecting the DDD onion architecture:</p> <pre><code>\u2514\u2500\u2500 ibm\n    \u2514\u2500\u2500 gse\n        \u2514\u2500\u2500 eda\n            \u2514\u2500\u2500 inventory\n                \u251c\u2500\u2500 app\n                \u2502\u00a0\u00a0 \u2514\u2500\u2500 ItemAggregatorApplication.java\n                \u251c\u2500\u2500 domain\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 ItemInventory.java\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 ItemProcessingAgent.java\n                \u2502\u00a0\u00a0 \u2514\u2500\u2500 ItemTransaction.java\n                \u2514\u2500\u2500 infra\n                    \u251c\u2500\u2500 ItemTransactionDeserializer.java\n                    \u251c\u2500\u2500 ItemTransactionStream.java\n                    \u2514\u2500\u2500 api\n                        \u251c\u2500\u2500 InventoryResource.java\n                        \u251c\u2500\u2500 ItemCountQueries.java\n                        \u2514\u2500\u2500 dto\n                            \u251c\u2500\u2500 ItemCountQueryResult.java\n                            \u2514\u2500\u2500 PipelineMetadata.java\n</code></pre> <p>The interesting class that supports the business logic is in ItemProcessingAgent.java. </p> <p>Basically the logic to compute the different stocks are in the <code>processItemTransaction</code> method, which builds a Kafla Stream topology</p> <p>For the stock of items cross store computation the code looks like:</p> <pre><code>@Produces\npublic Topology processItemTransaction(){\n    KStream&lt;String,ItemTransaction&gt; items = inItemsAsStream.getItemStreams();     \n    // process items and aggregate at the store level \n    KTable&lt;String,ItemInventory&gt; itemItemInventory = items\n        // use store name as key, which is what the item event is also using\n        .map((k,transaction) -&gt; {\n            ItemInventory newRecord = new ItemInventory();\n            newRecord.updateStockQuantityFromTransaction(transaction.sku, transaction);\n            return  new KeyValue&lt;String,ItemInventory&gt;(newRecord.itemID,newRecord);\n        })\n        .groupByKey( Grouped.with(Serdes.String(),ItemInventory.itemInventorySerde)).\n        aggregate( () -&gt; new ItemInventory(),\n            (itemID,newValue,currentValue) -&gt; currentValue.updateStockQuantity(itemID,newValue.currentStock),\n            materializeAsStoreInventoryKafkaStore());\n    produceStoreInventoryToOutputStream(itemItemInventory);\n    return inItemsAsStream.run();\n}\n</code></pre> <p>while for the store the code is also in ItemProcessingAgent</p> <pre><code>public Topology processItemTransaction(){\n    KStream&lt;String,ItemTransaction&gt; items = inItemsAsStream.getItemStreams();     \n    // process items and aggregate at the store level \n    KTable&lt;String,StoreInventory&gt; storeItemInventory = items\n        // use store name as key, which is what the item event is also using\n        .groupByKey(ItemStream.buildGroupDefinitionType())\n        // update the current stock for this &lt;store,item&gt; pair\n        // change the value type\n        .aggregate(\n            () -&gt;  new StoreInventory(), // initializer when there was no store in the table\n            (store , newItem, existingStoreInventory) \n                -&gt; existingStoreInventory.updateStockQuantity(store,newItem), \n                materializeAsStoreInventoryKafkaStore());       \n    produceStoreInventoryToInventoryOutputStream(storeItemInventory);\n    return inItemsAsStream.run();\n</code></pre> <p>Each project includes a set of unit tests to validate the logic.</p>"},{"location":"use-cases/kafka-streams/lab-3/#integration-tests","title":"Integration tests","text":"<p>For running the integration test, we propose to copy the e2e folder from the solution repository and follow the readme instructions section end-to-end-testing .</p>"},{"location":"use-cases/kafka-streams/lab-3/#interactive-queries","title":"Interactive queries","text":"<p>Warning</p> <p>With Kafka 3.x release this section needs to be revisited as API changed.</p> <p>We already addressed the interactive queries concept in the kafka stream technology summary article.  Each of the store and item aggregator implements those queries via two classes:</p> <ul> <li>ItemCountQueries</li> <li>StoreInventoryQueries</li> </ul> <p>The principles are the same:</p> <ul> <li>Get the metadata about each \"kafka store\" supporting the stateful KTables which are keeping the aggregate per item or per store.</li> <li>Get the value of the aggregate for the given key, locally or remotely.</li> </ul>"},{"location":"use-cases/monitoring-on-cloud/","title":"Monitoring IBM Event Streams on IBM Cloud","text":"<p> General monitoring practices Monitoring with SYSDIG Event Streams Monitoring </p>"},{"location":"use-cases/monitoring-on-cloud/#general-monitoring-practices","title":"General monitoring practices","text":"<p>We have documented in this note the major tools for monitoring a Kafka cluster. like prometheus and Grafana.</p>"},{"location":"use-cases/monitoring-on-cloud/#monitoring-with-sysdig","title":"Monitoring with SYSDIG","text":"<p>With Event Streams on cloud, the monitoring capability is supported by Sysdig. To enable monitoring approach includes the following steps:</p> <ul> <li>Create a IBM Cloud monitoring with Sysdig service.</li> </ul> <p></p> <p>Enter name, </p> <p>resource group and enable platform monitoring, if you want to combine Event Streams and Cluster monitoring:</p> <p></p> <p>You may want to do the Sysdig getting started tutorial to understand the different features of this product.</p> <ul> <li>Enable Event Streams to be monitored and select the monitoring service.</li> </ul> <p></p> <ul> <li>Link to an existing sysdig instance:</li> </ul> <p></p>"},{"location":"use-cases/monitoring-on-cloud/#event-streams-monitoring","title":"Event Streams Monitoring","text":"<p>Now open the Event Streams dashboard in Sysdig. This can be done from the Event Streams Dashboard -&gt;</p> <p></p> <p>The available metrics are discribed in this product documentation page.</p> <p></p> <p>The following present the network throughput derived by aggregating cross brokers bytes per second.</p> <p></p> <p>For example the enterprise plan is described here, so it is possible to define an alert when the instance byte in and out per second are going over a given threshold (35MB?).</p> <p>Here is an example of such alert:</p> <p></p> <p>Other interesting gauges are related to consumer groups, where it can be interesting to alert on inactive consumer groups, and the number of rebalancing consumer groups (something may be linked to the state of the consumers within the group or the brokers themselves).</p> <p></p> <p>Here are other reported interesting gauges:</p> <p></p> <ul> <li>Authentication failures will help to assess if API keys expired or some security attack.</li> <li>Producer or consumer conversion time are metrics to assess if some codes are using older API version and some time is consumed by doing conversion. This need to be at zero.</li> </ul>"},{"location":"use-cases/monitoring-on-ocp/","title":"Monitoring IBM Event Streams on OpenShift Cloud Platform","text":"<p>This tutorial was developed with and validated against IBM Cloud Pak for Integration Version 2020.2.1 and IBM Event Streams Version 10.0.0. Any deviation from those versions while performing the tasks in this tutorial may produced unexpected results.</p>"},{"location":"use-cases/monitoring-on-ocp/#overview","title":"Overview","text":"<p>Deploying IBM Event Streams on OpenShift Cloud Platform (OCP) as the Apache Kafka-based event backbone is a great first step in your Event-Driven Architecture implementation. However, now you must maintain that Kafka cluster and understand the intricate details of what a \"healthy\" cluster looks like. This tutorial will walk you through some of the initial monitoring scenarios that are available for IBM Event Streams deployed on OCP.</p> <p>The raw monitoring use cases and capabilities are available from the official IBM Event Streams documentation via the links below: - Monitoring deployment health - Monitoring Kafka cluster health - Monitoring topic health - Monitoring Kafka consumer group lag</p> <p>This tutorial will focus on a more guided approach to understanding the foundation of Apache Kafka monitoring capabilities provided by IBM Event Streams and the IBM Cloud Pak for Integration. Upon completion of this tutorial, you can extend your own experience through the Advanced Scenarios section to adapt Kafka monitoring capabilites to your project's needs.</p>"},{"location":"use-cases/monitoring-on-ocp/#scenario-prereqs","title":"Scenario Prereqs","text":"<p>OpenShift Container Platform</p> <ul> <li>This deployment scenario was developed for use on the OpenShift Container Platform, with a minimum version of <code>4.4</code>.</li> </ul> <p>Cloud Pak for Integration</p> <ul> <li>This deployment scenario was developed for use with the 2020.2.x release of the IBM Cloud Pak for Integration, installed on OpenShift 4.4.</li> </ul> <p>IBM Event Streams</p> <ul> <li>This deployment scenario requires a working installation of IBM Event Streams V10.0 or greater, deployed on the Cloud Pak for Integration environment mentioned above.</li> <li>For Cloud Pak installation guidance, you can follow the Cloud Pak Playbook installation instructions.</li> </ul> <p>Git</p> <ul> <li>We will need to clone repositories.</li> </ul> <p>Java</p> <ul> <li>Java Development Kit (JDK) v1.8+ (Java 8+)</li> </ul> <p>Maven</p> <ul> <li>The scenario uses Maven v3.6.x</li> </ul>"},{"location":"use-cases/monitoring-on-ocp/#generate-event-load","title":"Generate Event Load","text":"<p>See now separate note on Starter application</p>"},{"location":"use-cases/monitoring-on-ocp/#explore-the-preconfigured-event-streams-dashboard","title":"Explore the preconfigured Event Streams Dashboard","text":"<p>This section will walk through the default dashboard and user interface available on every IBM Event Streams deployment.</p> <ol> <li> <p>Log into the Event Streams Dashboard.</p> </li> <li> <p>Click the Monitoring tab from the primary navigation menu on the left hand side.</p> </li> <li> <p>From here, you can view information on messages, partitions, and replicas for the past hour, day, week, or month.</p> </li> </ol> <p></p> <ol> <li> <p>Click the Topics tab from the primary navigation menu on the left hand side.</p> </li> <li> <p>Click the name of your topic that you previously created in the Generate Event Load section. This should be in the format of <code>monitoring-lab-topic-[your-initials]</code>.</p> </li> <li> <p>You are presented with a Producers page showing the number of active producers, as well as the average message size produced per second and average number of messages produced per second. You can modify the time window by changing the values in the View producers by time box.    </p> </li> <li>Click the Messages tab to view all the data and metadata for events stored in the topic. You can view messages across partitions or on specific partitions, as well as jump to specific offsets or timestamps.    </li> <li>Click Consumer Groups to be shown the number of consumer groups that have previously registered or are currently registered as consuming from the topic.    </li> <li>You are able to see how many active members a consumer group has, as well as have many unconsumed partitions a topic has inside of a consumer group (also known as consumer group lag)- a key metric for driving parallelism in event-driven microservices!    </li> </ol>"},{"location":"use-cases/monitoring-on-ocp/#import-grafana-dashboards","title":"Import Grafana Dashboards","text":"<p>This section will walk through the Grafana Dashboard capabilities documented in the official IBM Event Streams documentation.</p> <ol> <li>Apply the Grafana Dashboard for overall Kafka Health via a <code>MonitoringDashboard</code> custom resource:</li> </ol> <pre><code>oc apply -f https://raw.githubusercontent.com/ibm-messaging/event-streams-operator-resources/master/grafana-dashboards/ibm-eventstreams-kafka-health-dashboard.yaml\n</code></pre>"},{"location":"use-cases/monitoring-on-ocp/#view-grafana-dashboards","title":"View Grafana Dashboards","text":"<p>To view the newly imported Event Streams Grafana dashboard for overall Kafka Health, follow these steps:</p> <ol> <li>Navigate to the IBM Cloud Platform Common Services console homepage via <code>https://cp-console.[cluster-name]</code>, click the hamburger icon in the top left and click the Monitoring in the expanded menu to open the Grafana homepage.</li> </ol> <p></p> <ol> <li>Click the user icon in the bottom left corner to open the user profile page.</li> </ol> <p></p> <ol> <li>In the Organizations table, find the namespace where you installed the Event Streams <code>monitoringdashboard</code> custom resource (most likely the <code>eventstreams</code>), and switch the user profile to that namespace.</li> </ol> <p></p> <ol> <li> <p>Hover over the Dashboards square on the left and click Manage.</p> </li> <li> <p>Click on IBM Event Streams Kafka dashboard in the Dashboard table to view the newly imported resource.</p> </li> </ol> <p></p> <ol> <li>Using the drop-down selectors at the top, select the following:</li> <li>Namespace which has the running instance of your Event Streams deployment,</li> <li>Cluster Name for the desired Event Streams cluster</li> <li>Topic that matches desired topics for viewing (only topics that have been published to will appear in this list)</li> <li>Broker to select individual or multiple brokers in the cluster.</li> </ol> <p></p> <p>Note: Not all of the metrics that Kafka uses are published to Prometheus by default. The metrics that are published are controlled by a ConfigMap. You can publish metrics by adding them to the ConfigMap.</p>"},{"location":"use-cases/monitoring-on-ocp/#create-an-alert","title":"Create an Alert","text":"<p>A monitoring system is only as good as the alerts it can send out, since you're not going to be watching that Grafana dashboard all day and night! This section will walk through the creation of a quick alert rule which will automatically trigger, as well as how to view and silence that alert in the provided Alertmanager interface.</p> <p>The official Event Streams documentation provides a walkthrough of selecting the desired metrics to monitor, but for our example, we will leverage the <code>kafka_server_replicamanager_partitioncount_value</code> metric as an indicator of topic creation (as the overall partition count will increase when a topic is first created).</p> <ol> <li>On the command line, create this sample rule which will fire whenever the partition count is over 50 (which is the baseline number of partitions the Event Streams system uses for its internal topic partitions). In order to do this, create <code>prom-rule-partitions.yaml</code> file with the following content in it:</li> </ol> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n    labels:\n        component: icp-prometheus\n    name: demo-partition-count\nspec:\n    groups:\n      - name: PartitionCount\n        rules:\n          - alert: PartitionCount\n            expr: kafka_server_replicamanager_partitioncount_value &gt; 50\n            for: 10s\n            labels:\n              severity: critical\n            annotations:\n              identifier: 'Partition count'\n              description: 'There are {{ $value }} partition(s) reported by broker {{ $labels.kafka }}'\n</code></pre> <ol> <li>Create the alert rule via the OpenShift CLI:</li> </ol> <pre><code>oc apply -f prom-rule-partitions.yaml\n</code></pre> <ol> <li>You can view the creation and status of your alert via the OpenShift CLI:</li> </ol> <p><pre><code>oc get PrometheusRule demo-partition-count\noc describe PrometheusRule demo-partition-count\n</code></pre> 1. Access the Prometheus monitoring backend that is provided in the IBM Cloud Pak Common Services via <code>https://cp-console.[cluster-name]/prometheus</code>.</p> <p></p> <ol> <li>Click the Alerts button in the header.</li> <li>You should see your new PartitionCount rule firing and highlighted in red.</li> <li>NOTE: If you do not see your PrometheusRule, you may need to create it in the <code>ibm-common-services</code> namespace depending upon your OpenShift cluster and Cloud Pak operator configuration. This can be done by supplying the <code>-n ibm-common-services</code> flag to the <code>oc apply -f prom-rule-partitions.yaml</code> command.</li> <li>Click on the PartitionCount alert to expand the details and see which components are triggering the alert.</li> </ol> <p></p> <p>Now that we have created alerts from the monitoring system, you will want a way to manage those alerts. The default Alertmanager component provides a way to manage firing alerts, notifications, and silences. Prometheus is capable of integrating with many notification systems - from Slack to PagerDuty to HipChat to common HTTP webhooks. For further information on the extensibility of Prometheus, you can view the Alerting configuration section of the official docs. For configuring the IBM Cloud Pak Common Services deployed instance of Prometheus, you can view the Configuring Alertmanager section of the official docs.</p> <p>In this section of the tutorial, you will walk through the Alertmanager interface and silence the previously created alerts.</p> <ol> <li>Access the default Alertmanager instance via <code>https://cp-console.apps.[cluster-name]/alertmanager/</code>.</li> <li>You should see the newly created <code>PartitionCount</code> alerts listed as firing.</li> </ol> <p></p> <ol> <li>Click on the Info button for the first alert to see the additional context provided by the alert definition (ie there are more than 50 partitions)</li> </ol> <p></p> <ol> <li> <p>As alerts fire and become acknowledged, you can silence them to mark them as known, acknowledged, or resolved. To do this, you create a Silence. Click the Silence button for one of the alerts in the list.</p> </li> <li> <p>You will see a start time, a duration, and an end time by default. This gives you initial control over what you are silencing and for how long.</p> </li> <li> <p>Next, you will see a list of Name and Value pairs that are filled with the information from the alert instance you clicked on.</p> </li> </ol> <p></p> <ol> <li>Delete the elements in the Matchers list until only the following items are left. This will allow for a robust capture of all the PartitionCount alerts for the same Event Streams cluster.</li> <li><code>alertname</code></li> <li><code>app_kubernetes_io_instance</code></li> <li><code>app_kubernetes_io_part_of</code></li> <li> <p><code>kubernetes_namespace</code> </p> </li> <li> <p>Your username should already be filled in for the Creator, so enter a Comment of \"Silencing demo alerts\" and click Preview Alerts.</p> </li> </ol> <p></p> <ol> <li> <p>Once the affected number of alerts matches the same number of PartitionCount alerts that were listed as firing in Prometheus, click Create.</p> </li> <li> <p>Clicking on the Alerts tab in the header, you will now see those alerts are silenced - meaning you acknowledged them.</p> </li> </ol> <p></p> <ol> <li>To make them visible again prior to the expiration of the created Silence, click on the Silences tab from the header. This page lists all the Active, Pending, and Expired silences in the system. You can view, edit, and expire any active Silence to again have the alerts show up in Alertmanager or anywhere else Prometheus is sending notifications.</li> </ol> <p></p>"},{"location":"use-cases/monitoring-on-ocp/#next-steps","title":"Next Steps","text":""},{"location":"use-cases/monitoring-on-ocp/#external-monitoring-tools","title":"External Monitoring Tools","text":"<p>IBM Event Streams supports additional monitoring capabilities with third-party monitoring tools via a connection to the clusters JMX port on the Kafka brokers.</p> <p>You must first configure your IBM Event Streams instance for specific access by these external monitoring tools.</p> <p>You can then follow along with the tutorials defined in the official IBM Event Streams documentation to monitor Event Streams with tools such as Datadog and Splunk.</p>"},{"location":"use-cases/monitoring-on-ocp/#advanced-scenarios","title":"Advanced Scenarios","text":"<p>As shown in this tutorial, IBM Event Streams provides a robust default set of monitoring metrics which are available to use right out of the box. However, you will most likely need to define custom metrics or extend existing metrics for use in custom dashboards or reporting processes. The following links (in order of recommended usage) discuss additional monitoring capabilities, technologies, and endpoints that are supported with IBM Event Streams to extend your custom monitoring solution as needed:</p> <ul> <li> <p>Kafka Exporter - You can use Event Streams to export metrics to Prometheus. These metrics are otherwise only accessible through the Kafka command line tools and allow per-topic metrics, such as consumer group lag, to be colleced.</p> </li> <li> <p>JMX Exporter - You can use Event Streams to collect JMX metrics from Kafka brokers, ZooKeeper nodes, and Kafka Connect nodes, and export them to Prometheus via the Prometheus JMX Exporter.</p> </li> <li> <p>JmxTrans - JmxTrans can be used to push JMX metrics from Kafka brokers to external applications or databases.</p> </li> </ul>"},{"location":"use-cases/monitoring-on-ocp/#additional-reading","title":"Additional Reading","text":"<ul> <li>Monitoring Kafka via official Apache Kafka documentation</li> <li>Monitoring Kafka performance metrics via Datadog</li> <li>How to Monitor Kafka via Server Density</li> <li>OpenShift Day 2 Monitoring via IBM Cloud Paks Playbook</li> <li>Monitoring Kafka cluster health via IBM Event Streams documentation</li> <li>Configuring the monitoring stack via Red Hat OpenShift documentation</li> <li>Examining cluster metrics via Red Hat OpenShift documentation</li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/","title":"Event Streams on IBM Cloud Schema Registry Lab","text":"<p>This documentation aims to be a introductory hands-on lab on IBM Event Streams on IBM Cloud Schema Registry where we will go through the different capabilities of the Schema Registry that is available for IBM Event Streams on IBM Cloud users.</p> <p>We strongly recommend to complete first the IBM Event Streams on IBM Cloud lab you can find here.</p>"},{"location":"use-cases/schema-registry-on-cloud/#index","title":"Index","text":"<p> Requirements IBM Event Streams Service Credentials Kafdrop Python Demo Environment Schema Registry Schemas Python Avro Producer Python Avro Consumer Schemas and Messages Data Evolution Security </p>"},{"location":"use-cases/schema-registry-on-cloud/#requirements","title":"Requirements","text":"<p>This lab requires the following components to work against:</p> <ol> <li>An IBM Cloud account.</li> <li>An IBM Event Streams instance with the Enterprise plan (Schema Registry is only available to these instance for now) - https://cloud.ibm.com/docs/EventStreams?topic=eventstreams-getting_started</li> </ol> <p>On your development workstation you will need:</p> <ol> <li>(optional) IBM Cloud CLI - https://cloud.ibm.com/docs/cli?topic=cloud-cli-getting-started</li> <li>(optional) IBM CLoud CLI Event Streams plugin - https://cloud.ibm.com/docs/EventStreams?topic=eventstreams-cli#step5_es_cli</li> <li>Docker - https://docs.docker.com/get-docker/</li> <li>Kafdrop - https://github.com/obsidiandynamics/kafdrop</li> </ol>"},{"location":"use-cases/schema-registry-on-cloud/#ibm-event-streams-service-credentials","title":"IBM Event Streams Service Credentials","text":"<p>At this point, we want to create the needed service credentials in order to allow other applications, tools, scripts to interact with our IBM Event Streams instance. For doing so, we need to:</p> <ol> <li> <p>In your IBM Event Streams instance service page, click on Service credentials on the left hand side menu:</p> <p></p> </li> <li> <p>Observe, there is no service credentials yet and click on the New credential button on the top right corner:</p> <p></p> </li> <li> <p>Enter a name for your service, choose Manager role for now and click on Add:</p> <p></p> </li> <li> <p>You should now see your new service credential and be able to inspect its details if you click on its dropdown arrow on it left:</p> <p></p> </li> <li> <p>We could have created the service credentials using the CLI as well but we leave that as extra homework for students to try on their own. However, we can explore the service credentials using the CLI with <code>ibmcloud resource service-key &lt;service_credentials_name&gt;</code>:</p> <pre><code>$ ibmcloud resource service-key demo-serv-cred\nRetrieving service key demo-serv-cred in all resource groups under account Kedar Kulkarni's Account as ALMARAZJ@ie.ibm.com...\n\nName:          demo-serv-cred\nID:            crn:v1:bluemix:public:messagehub:eu-de:a/b636d1d83e34d7ae7e904591ac248cfa:b05be932-2a60-4315-951d-a6dd902e687a:resource-key:4ba348d2-5fcf-4c13-a265-360e983d99c5\nCreated At:    Tue May 12 10:53:02 UTC 2020\nState:         active\nCredentials:\n            api_key:                  *****\n            apikey:                   *****\n            iam_apikey_description:   Auto-generated for key 4ba348d2-5fcf-4c13-a265-360e983d99c5\n            iam_apikey_name:          demo-serv-cred\n            iam_role_crn:             crn:v1:bluemix:public:iam::::serviceRole:Manager\n            iam_serviceid_crn:        crn:v1:bluemix:public:iam-identity::a/b636d1d83e34d7ae7e904591ac248cfa::serviceid:ServiceId-380e866c-5914-4e01-85c4-d80bd1b8a899\n            instance_id:              b05be932-2a60-4315-951d-a6dd902e687a\n            kafka_admin_url:          https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud\n            kafka_brokers_sasl:       [kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093 kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093]\n            kafka_http_url:           https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud\n            password:                 *****\n            user:                     token\n</code></pre> </li> </ol> <p> <p>IMPORTANT: Out of number 4 above, we are going to define the following important environment variables that will be used throughout this lab so please make sure you understand and define these properly</p> <p></p> <ol> <li> <p>KAFKA_BROKERS which should take the value of kafka_brokers_sasl comma separated:</p> <pre><code>export KAFKA_BROKERS=kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093\n</code></pre> </li> <li> <p>KAFKA_APIKEY which should take the value of apikey:</p> <pre><code>export KAFKA_APIKEY=*****\n</code></pre> </li> <li> <p>URL which should take the value of kafka_http_url:</p> <pre><code>export URL=https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud\n</code></pre> </li> <li> <p>SCHEMA_REGISTRY_URL which should be a combination of the three before in the form of:</p> <p><code>https://token:&lt;apikey&gt;@&lt;kafka_http_url&gt;/confluent</code></p> <pre><code>export SCHEMA_REGISTRY_URL=https://token:*****@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent\n</code></pre> </li> </ol> <p> <p>INFO: The reason for this \"weird\" schema registry url configuration is because we are using the Confluent SerDes as we will see in the Python Avro Producer and Python Avro Consumer sections later on. Otherwise, the schema registry url would simply be the same as the kafka_http_url in your service credentials.</p> <p></p>"},{"location":"use-cases/schema-registry-on-cloud/#kafdrop","title":"Kafdrop","text":"<p>Kafdrop is a web UI for viewing Kafka topics and browsing consumer groups. The tool displays information such as brokers, topics, partitions, consumers, and lets you view messages: https://github.com/obsidiandynamics/kafdrop</p> <p>Kafdrop runs as a Docker container in your wokstation and in order to run it,</p> <ol> <li> <p>Set the password value for the sasl.jaas.config property in the kafka.properties file you can find in this repo. The value for password is the api_key/password attributes of your service credentials. If you don't remember how to get this, review that section here</p> <pre><code>security.protocol=SASL_SSL\nssl.protocol=TLSv1.2\nssl.enabled.protocols=TLSv1.2\nssl.endpoint.identification.algorithm=HTTPS\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;YOUR_PASSWORD/API_KEY_HERE&gt;\";\n</code></pre> </li> <li> <p>Run the Kafdrop Docker container by executing:</p> <p><pre><code>$ docker run -d --rm -p 9000:9000 \\\n    -e KAFKA_BROKERCONNECT=$KAFKA_BROKERS \\\n    -e KAFKA_PROPERTIES=$(cat kafka.properties | base64) \\\n    -e JVM_OPTS=\"-Xms32M -Xmx64M\" \\\n    -e SERVER_SERVLET_CONTEXTPATH=\"/\" \\\n    obsidiandynamics/kafdrop\n</code></pre> (*) After running the above command, you can use the docker container id returned on the screen to debug any possible issue with <code>docker logs -f &lt;container_id&gt;</code>. This container id will be used to stop the Kafdrop container once we have finished.</p> </li> <li> <p>You can point your browser to http://localhost:9000/ to access the Kafdrop application:</p> <p></p> </li> <li> <p>You can see the topic details by clicking on the name of that topic:</p> <p></p> </li> <li> <p>You can watch the messages on this topic by clicking on View Messages button under the topic name and configuring the partition, offset and number of messages option that you are presented with in the next screen. Finally, click on View Messages button at the right:</p> <p></p> </li> <li> <p>To stop the Kafdrop container once you have finsihed the tutorial, simply list the containers running on your workstation, find the container id for your Kafdrop container and stop it:</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE                                       COMMAND             CREATED             STATUS              PORTS                    NAMES\nba6c8eaf6a3a        ibmcase/python-schema-registry-lab:latest   \"bash\"              31 seconds ago      Up 30 seconds                                keen_neumann\nbab9cae43f15        obsidiandynamics/kafdrop                    \"/kafdrop.sh\"       17 minutes ago      Up 17 minutes       0.0.0.0:9000-&gt;9000/tcp   laughing_dirac\n\n$ docker stop bab9cae43f15\nbab9cae43f15\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-cloud/#python-demo-environment","title":"Python Demo Environment","text":"<p>Given that students' workstations may vary quite a lot, not only on their operating system but also on the tools installed on them and the tools we need for our lab might install differently, we have opted to provide a python demo environment in the form of a Docker container where all the libraries and tools needed are already pre-installed.</p>"},{"location":"use-cases/schema-registry-on-cloud/#clone","title":"Clone","text":"<p>In order to build our python demo environment we first need to clone the github repository where the assets live. This github repository is https://github.com/ibm-cloud-architecture/refarch-eda-tools and the specific assets we refer to can be found under the <code>labs/es-cloud-schema-lab</code> folder:</p> <ol> <li> <p>Clone the github repository on your workstation on the location of your choice:</p> <pre><code>$ git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools.git\nCloning into 'refarch-eda-tools'...\nremote: Enumerating objects: 185, done.\nremote: Counting objects: 100% (185/185), done.\nremote: Compressing objects: 100% (148/148), done.\nremote: Total 185 (delta 23), reused 176 (delta 16), pack-reused 0\nReceiving objects: 100% (185/185), 6.17 MiB | 4.61 MiB/s, done.\nResolving deltas: 100% (23/23), done.\n</code></pre> </li> <li> <p>Change directory into <code>refarch-eda-tools/labs/es-cloud-schema-lab</code> to find the assets we will we working from now on for the python demo environment and few other scripts/applications:</p> <pre><code>$ cd refarch-eda-tools/labs/es-cloud-schema-lab\n\n$ ls -all\ntotal 240\ndrwxr-xr-x   9 user  staff     288 20 May 19:33 .\ndrwxr-xr-x   3 user  staff      96 20 May 19:33 ..\n-rw-r--r--   1 user  staff    1012 20 May 19:33 Dockerfile\n-rw-r--r--   1 user  staff  112578 20 May 19:33 README.md\ndrwxr-xr-x   5 user  staff     160 20 May 19:33 avro_files\ndrwxr-xr-x  27 user  staff     864 20 May 19:33 images\ndrwxr-xr-x   6 user  staff     192 20 May 19:33 kafka\n-rw-r--r--   1 user  staff     286 20 May 19:33 kafka.properties\ndrwxr-xr-x   6 user  staff     192 20 May 19:33 src\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-cloud/#build","title":"Build","text":"<p>This Docker container can be built by using the Dockerfile provided within this github repository.</p> <p>To build your python demo environment Docker container, execute the following on your workstation:</p> <p><pre><code>docker build -t \"ibmcase/python-schema-registry-lab:latest\" .\n</code></pre> <p>WARNING:</p> <ul> <li>Mind the dot at the end of the command.</li> <li>Be consistent throughout the lab with the name you give to the Docker container.</li> </ul> <p></p>"},{"location":"use-cases/schema-registry-on-cloud/#run","title":"Run","text":"<p>In order to run the python demo environment Docker container, execute the following on your workstation:</p> <ol> <li> <p>Make sure you have declared your <code>KAFKA_BROKERS</code>, <code>KAFKA_APIKEY</code> and <code>SCHEMA_REGISTRY_URL</code> environment variables as explain in the IBM Event Streams Service Credentials section.</p> </li> <li> <p>Run the python demo environment container</p> <pre><code>$ docker run -e KAFKA_BROKERS=$KAFKA_BROKERS \\\n             -e KAFKA_APIKEY=$KAFKA_APIKEY \\\n             -e SCHEMA_REGISTRY_URL=$SCHEMA_REGISTRY_URL \\\n             -v ${PWD}:/tmp/lab \\\n             --rm \\\n             -ti ibmcase/python-schema-registry-lab:latest bash\n</code></pre> </li> <li> <p>Go to <code>/tmp/lab</code> to find all the assets you will need to complete this lab.</p> </li> </ol> <p> <p>INFO: we have mounted this working directory into the container so any changes to any of the files apply within the container. This is good as we do not need to restart the python demo environment Docker container if we want to do any changes to the files.</p> <p></p>"},{"location":"use-cases/schema-registry-on-cloud/#exit","title":"Exit","text":"<p>Once you are done with the python demo environment container, just execute <code>exit</code> and you will get out of the container and the container will automatically be removed from your system.</p>"},{"location":"use-cases/schema-registry-on-cloud/#schema-registry","title":"Schema Registry","text":"<p> <p>INFO: The following documentation about the IBM Event Streams on IBM Cloud Schema registry until the end of this lab is based on the Schema Registry status as of mid May 2020 when this tutorial was developed</p> <p></p> <p></p> <p>One of the most common technologies used in the industry these days to define, serialize and deserialize messages flowing through your Kafka topics is Apache Avro (https://avro.apache.org/docs/current/). To learn more about Apache Avro, how to define Apache Avro data schemas and more see our documentation here</p> <p>IBM Event Streams on IBM Cloud development team has developed a Schema Registry to work along your Kafka cluster to provide centralized schema management and compatibility checks as schemas evolve so that the communication between Kafka producers and consumers follow these schemas for consistency or as many like to say, meet the contracts that schemas are.</p>"},{"location":"use-cases/schema-registry-on-cloud/#overview","title":"Overview","text":"<p>The schema registry capability is being developed for the IBM Event Streams managed Kafka service running in IBM Cloud. The purpose of the schema registry is to provide a place to store descriptions of the message formats used by your producers and consumers. The benefit of storing these descriptions is that you are able to validate that your producing and consuming applications will correctly inter-operate.</p> <p>Currently the schema registry is in early access status. This means that a limited function version of the registry is being made available to a small group of users for the purpose of gathering feedback, and rapidly iterating on the design of the registry. Please note that while the schema registry is in early access, there may be occasions when IBM will delete all of the schema data held within the registry.</p>"},{"location":"use-cases/schema-registry-on-cloud/#current-features","title":"Current features","text":"<ul> <li>Support for creating, listing and deleting schemas via a REST interface</li> <li>Support for creating, listing and deleting versions of a schema, via a REST interface</li> <li>Support for using schema information in Kafka producer and consumer applications via the Confluent AVRO SerDes</li> <li>Support for Apache AVRO as the format used to express schemas</li> <li>Support for applying constraints on schema compatibility, either at a global scope, or on a per-schema basis</li> <li>Access to schema registry requires authentication and access is controlled via IAM</li> <li>Access to individual schemas, and compatibility rules can be controlled via a new IAM schema resource type</li> <li>Constraints on maximum schema size (64K), the maximum number of schemas that can be stored in the registry (1000) and the maximum number of versions a schema can have (100)</li> <li>SLA of 99.99% availability, consistent with that of the Event Streams service</li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#current-limitations","title":"Current limitations","text":"<ul> <li>No caching performed for frequently requested schemas</li> <li>Does not publish metrics to Sysdig</li> <li>Does not generate Activity Tracker events</li> <li>There may be other missing functions that you require. Please let IBM know!</li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#enabling-the-schema-registry","title":"Enabling the Schema Registry","text":"<p>Currently the schema registry is not enabled by default and can only be enabled for IBM Event Streams Enterprise plan service instances. To request the enablement of the schema registry, please raise a support ticket here. Ensure that you include the CRN of your Event Streams service instance in the ticket. The CRN of the service instance can be found using the following IBM Cloud CLI command <code>ibmcloud resource service-instance &lt;SERVICENAME&gt;</code> (where <code>SERVICENAME</code> is the name of your Event Streams service instance). For more info on how to get your IBM Event Streams service instance CRN review the Event Streams on Cloud hands on lab.</p> <p>If you have already enabled the schema registry capability for an Event Streams Enterprise plan service instance, it will automatically receive updates as new capabilities become available.</p>"},{"location":"use-cases/schema-registry-on-cloud/#accessing-the-schema-registry","title":"Accessing the Schema Registry","text":"<p>To access the schema registry, you will need the URL of the schema registry as well as a set of credentials that can be used to authenticate with the registry. Both of these pieces of information can be found by inspecting the service credentials for your service. To view these in the UI, click on your service instance, select Service credentials in the left-hand navigation pane, then click on the dropdown arrow next to one of the service credentials name listed in the table. You should see something like this:</p> <p></p> <p>You will need the value of kafka_http_url, which is also the URL of the schema registry, and the value of apikey which you can use as the credential for authenticating with the schema registry.</p> <p>To check we have appropriate permissions to work with the Schema Registry, we can execute the following command that would actually list the schemas stored in the schema registry in our terminal:</p> <p><code>curl -i \u2013u token:&lt;KAFKA_APIKEY&gt; &lt;URL&gt;/artifacts</code></p> <pre><code>curl -i -u token:***** https://mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/artifacts\nHTTP/1.1 200 OK\nDate: Wed, 13 May 2020 11:38:36 GMT\nContent-Type: application/json\nContent-Length: 3\nConnection: keep-alive\n\n[]\n</code></pre> <p>As you can see, no schema is being retrieved which makes sense with the just out of the box schema registry status we are at.</p> <p> <p>INFO: For easiness executing the following schema registry REST API commands, we recommend you set your kafka_http_url and apikey as URL and KAFKA_APIKEY environment variables respectively as explained in the IBM Event Streams Service Credentials section.</p> <p></p>"},{"location":"use-cases/schema-registry-on-cloud/#schemas","title":"Schemas","text":"<p>In this section we will finally get our hands dirty with the IBM Event Steams on IBM Cloud Schema Registry capability by working with Apache Avro schemas and the schema registry.</p>"},{"location":"use-cases/schema-registry-on-cloud/#create-a-schema","title":"Create a schema","text":"<p>This endpoint is used to store a schema in the registry. The schema data is sent as the body of the post request. An ID for the schema can be included using the <code>X-Registry-ArtifactId</code> request header. If this header is not present in the request, then an ID will be generated. The content type header must be set to <code>application/json</code>.</p> <p>Creating a schema requires at least both:</p> <ul> <li>Reader role access to the Event Streams cluster resource type</li> <li> <p>Writer role access to the schema resource that matches the schema being created</p> </li> <li> <p>Create a schema with:</p> <p><code>curl -u token:$KAFKA_APIKEY -H 'Content-Type: application/json' -H 'X- Registry-ArtifactId: &lt;SCHEMA_ID&gt;' $URL/artifacts -d '&lt;AVRO_SCHEMA&gt;'</code></p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: demo-schema' \\\n        $URL/artifacts \\\n        -d '{   \"type\":\"record\",\n                \"name\":\"demoSchema\",\n                \"fields\":[\n                        {\"name\": \"eventKey\",\"type\":\"string\"},\n                        {\"name\": \"message\",\"type\":\"string\"}] }'\n\n{\"id\":\"demo-schema\",\"type\":\"AVRO\",\"version\":1,\"createdBy\":\"\",\"createdOn\":1589371190273,\"modifiedBy\":\"\",\"modifiedOn\":1589371190273,\"globalId\":1}\n</code></pre> </li> <li> <p>Create a few more schemas.</p> </li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#list-schemas","title":"List schemas","text":"<p>Listing schemas requires at least:</p> <ul> <li> <p>Reader role access to the Event Streams cluster resource type</p> </li> <li> <p>List the schemas and see the ones you have created previously with:</p> <p><code>curl -u token:$KAFKA_APIKEY $URL/artifacts</code></p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts\n\n[\"demo-schema\",\"demo-schema-2\"]\n</code></pre> </li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#delete-schemas","title":"Delete schemas","text":"<p>Schemas are deleted from the registry by issuing a DELETE request to the <code>/artifacts/{schema-id}</code> endpoint (where <code>{schema-id}</code> is the ID of the schema). If successful an empty response, and a status code of 204 (no content) is returned.</p> <p>Deleting a schema requires at least both:</p> <ul> <li>Reader role access to the Event Streams cluster resource type</li> <li> <p>Manager role access to the schema resource that matches the schema being deleted</p> </li> <li> <p>Delete one (or all) of the schemas you created previously with:</p> <p><code>curl -u token:$KAFKA_APIKEY -X DELETE $URL/artifacts/my-schema</code></p> <pre><code>$ curl -i -u token:$KAFKA_APIKEY -X DELETE $URL/artifacts/demo-schema\nHTTP/1.1 204 No Content\nDate: Wed, 13 May 2020 12:37:48 GMT\nConnection: keep-alive\n</code></pre> </li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#create-new-schema-version","title":"Create new schema version","text":"<p>To create a new version of a schema, make a POST request to the <code>/artifacts/{schema-id}/versions</code> endpoint, (where <code>{schema-id}</code> is the ID of the schema). The body of the request must contain the new version of the schema.</p> <p>If the request is successful the new schema version is created as the new latest version of the schema, with an appropriate version number, and a response with status code 200 (OK) and a payload containing metadata describing the new version, (including the version number), is returned.</p> <p>Creating a new version of a schema requires at least both:</p> <ul> <li>Reader role access to the Event Streams cluster resource type</li> <li> <p>Writer role access to the schema resource that matches the schema getting a new version</p> </li> <li> <p>Create a new schema:</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: version-demo-schema' \\\n        $URL/artifacts \\\n        -d '{\"type\":\"record\",\n            \"name\":\"versionDemoSchema\",\n            \"fields\":[\n                {\"name\": \"eventKey\",\"type\":\"string\"},\n                {\"name\": \"message\",\"type\":\"string\"}] }'\n\n{\"id\":\"version-demo-schema\",\"type\":\"AVRO\",\"version\":1,\"createdBy\":\"\",\"createdOn\":1589380529049,\"modifiedBy\":\"\",\"modifiedOn\":1589380529049,\"globalId\":9}\n</code></pre> </li> <li> <p>Add a new attribute to the schema and create a new version for it with:</p> <p><code>curl -u token:$KAFKA_APIKEY -H 'Content-Type: application/json' $URL/artifacts/&lt;SCHEMA_ID&gt;/versions -d '&lt;AVRO_SCHEMA'</code></p> <p><pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: version-demo-schema' \\\n        $URL/artifacts/version-demo-schema/versions \\\n        -d '{\"type\":\"record\",\n            \"name\":\"versionDemoSchema\",\n            \"fields\":[\n                {\"name\": \"eventKey\",\"type\":\"string\"},\n                {\"name\": \"message\",\"type\":\"string\"},\n                {\"name\": \"attribute1\",\"type\":\"string\"}]}'\n\n{\"id\":\"version-demo-schema\",\"type\":\"AVRO\",\"version\":2,\"createdBy\":\"\",\"createdOn\":1589380529049,\"modifiedBy\":\"\",\"modifiedOn\":1589380728324,\"globalId\":10}\n</code></pre> (*) See that the returned JSON object includes the version for the schema and that this has increased</p> </li> <li> <p>Create yet another new attribute and version for the schema:</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: version-demo-schema' \\\n        $URL/artifacts/version-demo-schema/versions \\\n        -d '{\"type\":\"record\",\n            \"name\":\"versionDemoSchema\",\n            \"fields\":[\n                {\"name\": \"eventKey\",\"type\":\"string\"},\n                {\"name\": \"message\",\"type\":\"string\"},\n                {\"name\": \"attribute1\",\"type\":\"string\"},\n                {\"name\": \"attribute2\",\"type\":\"string\"}]}'\n\n{\"id\":\"version-demo-schema\",\"type\":\"AVRO\",\"version\":3,\"createdBy\":\"\",\"createdOn\":1589380529049,\"modifiedBy\":\"\",\"modifiedOn\":1589380955324,\"globalId\":11}\n</code></pre> </li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#get-latest-version-of-a-schema","title":"Get latest version of a schema","text":"<p>To retrieve the latest version of a particular schema, make a GET request to the <code>/artifacts/{schema-id}</code> endpoint, (where <code>{schema-id}</code> is the ID of the schema). If successful, the latest version of the schema is returned in the payload of the response.</p> <p>Getting the latest version of a schema requires at least both:</p> <ul> <li>Reader role access to the Event Streams cluster resource type</li> <li> <p>Reader role access to the schema resource that matches the schema being retrieved</p> </li> <li> <p>Get the latest version of the schema with:</p> <p><code>curl -u token:$KAFKA_APIKEY $URL/artifacts/&lt;SCHEMA_ID&gt;</code></p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts/version-demo-schema\n{\n    \"type\":\"record\",\n    \"name\":\"versionDemoSchema\",\n    \"fields\":[\n        {\"name\": \"eventKey\",\"type\":\"string\"},\n        {\"name\": \"message\",\"type\":\"string\"},\n        {\"name\": \"attribute1\",\"type\":\"string\"},\n        {\"name\": \"attribute2\",\"type\":\"string\"}]\n}\n</code></pre> </li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#get-specific-version-of-a-schema","title":"Get specific version of a schema","text":"<p>To retrieve a specific version of a schema, make a GET request to the <code>/artifacts/{schema-id}/versions/{version}</code> endpoint, (where <code>{schema-id}</code> is the ID of the schema, and <code>{version}</code> is the version number of the specific version you need to retrieve). If successful, the specified version of the schema is returned in the payload of the response.</p> <p>Getting the latest version of a schema requires at least both:</p> <ul> <li>Reader role access to the Event Streams cluster resource type</li> <li> <p>Reader role access to the schema resource that matches the schema being retrieved</p> </li> <li> <p>Get a specific version of a schema with:</p> <p><code>curl -u token:$KAFKA_APIKEY $URL/artifacts/&lt;SCHEMA_ID&gt;/versions/&lt;VERSION_ID&gt;</code></p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts/version-demo-schema/versions/2\n{\n    \"type\":\"record\",\n    \"name\":\"versionDemoSchema\",\n    \"fields\":[\n        {\"name\": \"eventKey\",\"type\":\"string\"},\n        {\"name\": \"message\",\"type\":\"string\"},\n        {\"name\": \"attribute1\",\"type\":\"string\"}]\n}\n</code></pre> </li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#listing-all-versions-of-a-schema","title":"Listing all versions of a schema","text":"<p>To list all versions of a schema currently stored in the registry, make a GET request to the <code>/artifacts/{schema-id}/versions</code> endpoint, (where <code>{schema-id}</code> is the ID of the schema). If successful, a list of all current version numbers for the schema is returned in the payload of the response.</p> <p>Getting the list of available versions of a schema requires at least both:</p> <ul> <li>Reader role access to the Event Streams cluster resource type</li> <li> <p>Reader role access to the schema resource that matches the schema being retrieved</p> </li> <li> <p>Get all the versions for a schema with:</p> <p><code>curl -u token:$KAFKA_APIKEY $URL/artifacts/&lt;SCHEMA_ID&gt;/versions</code></p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts/version-demo-schema/versions\n\n[1,2,3]\n</code></pre> </li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#deleting-a-version-of-a-schema","title":"Deleting a version of a schema","text":"<p>Schema versions are deleted from the registry by issuing a DELETE request to the <code>/artifacts/{schema-id}/versions/{version}</code> endpoint (where <code>{schema-id}</code> is the ID of the schema, and {version} is the version number of the schema version). If successful an empty response, and a status code of 204 (no content) is returned. Deleting the only remaining version of a schema will also delete the schema.</p> <p>Deleting a schema version requires at least both:</p> <ul> <li>Reader role access to the Event Streams cluster resource type</li> <li> <p>Manager role access to the schema resource that matches the schema being deleted</p> </li> <li> <p>Delete a version of a schema with:</p> <p><code>curl -u token:$KAFKA_APIKEY -X DELETE $URL/artifacts/&lt;SCHEMA_ID&gt;/versions/&lt;VERSION_ID&gt;</code></p> <pre><code>$ curl -u token:$KAFKA_APIKEY -X DELETE $URL/artifacts/version-demo-schema/versions/2\n</code></pre> </li> <li> <p>Make sure your specific version has been deleted by listing all the version for that schema:</p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts/version-demo-schema/versions\n\n[1,3]\n</code></pre> </li> </ul> <p>If you deleted a schema, it will get deleted along with all its versions.</p>"},{"location":"use-cases/schema-registry-on-cloud/#python-avro-producer","title":"Python Avro Producer","text":"<p>In this section we describe the python scripts we will be using in order to be able to produce avro messages to a Kafka topic.</p>"},{"location":"use-cases/schema-registry-on-cloud/#produce-message","title":"Produce Message","text":"<p>The python script that we will use to send an avro message to a Kafka topic is ProduceAvroMessage.py where we have the following:</p> <ol> <li> <p>A function to parse the arguments:</p> <pre><code>def parseArguments():\n    global TOPIC_NAME\n    print(\"The arguments for this script are: \" , str(sys.argv))\n    if len(sys.argv) == 2:\n        TOPIC_NAME = sys.argv[1]\n    else:\n        print(\"[ERROR] - The produceAvroMessage.py script expects one argument: The Kafka topic to publish the message to\")\n        exit(1)\n</code></pre> </li> <li> <p>A function to create the event to be sent:</p> <pre><code>def createEvent():\n    print('Creating event...')\n\n    event = {\n            \"eventKey\" : \"1\",\n            \"message\" : \"This is a test message\"\n            }\n\n    print(\"DONE\")\n    return json.dumps(event)\n</code></pre> </li> <li> <p>The main where we will:</p> <ol> <li>Parse the arguments</li> <li>Get the Avro schemas for the key and value of the event</li> <li>Create the Event to be sent</li> <li>Print it out for reference</li> <li>Create the Kafka Avro Producer and configure it</li> <li>Send the event</li> </ol> <pre><code>if __name__ == '__main__':\n    # Get the Kafka topic name\n    parseArguments()\n    # Get the avro schemas for the message's key and value\n    event_value_schema = getDefaultEventValueSchema(DATA_SCHEMAS)\n    event_key_schema = getDefaultEventKeySchema(DATA_SCHEMAS)\n    # Create the event\n    message_event = createEvent()\n    # Print out the event to be sent\n    print(\"--- Container event to be published: ---\")\n    print(json.loads(message_event))\n    print(\"----------------------------------------\")\n    # Create the Kafka Avro Producer\n    kafka_producer = KafkaProducer(KAFKA_BROKERS,KAFKA_APIKEY,SCHEMA_REGISTRY_URL)\n    # Prepare the Kafka Avro Producer\n    kafka_producer.prepareProducer(\"ProduceAvroMessagePython\",event_key_schema,event_value_schema)\n    # Publish the event\n    kafka_producer.publishEvent(TOPIC_NAME,message_event,\"eventKey\")\n</code></pre> </li> </ol> <p>As you can see, this python code depends on a Kafka Avro Producer and an Avro Utils for loading the Avro schemas which are explained next.</p>"},{"location":"use-cases/schema-registry-on-cloud/#avro-utils","title":"Avro Utils","text":"<p>This script, called avroEDAUtils.py, contains some very simple utility functions to be able to load Avro schemas from their avsc files in order to be used by the Kafka Avro Producer.</p> <ol> <li> <p>A function to get the key and value Avro schemas for the messages to be sent:</p> <p><pre><code>def getDefaultEventValueSchema(schema_files_location):\n# Get the default event value data schema\nknown_schemas = avro.schema.Names()\ndefault_event_value_schema = LoadAvsc(schema_files_location + \"/default_value.avsc\", known_schemas)\nreturn default_event_value_schema\n\ndef getDefaultEventKeySchema(schema_files_location):\n# Get the default event key data schema\nknown_schemas = avro.schema.Names()\ndefault_event_key_schema = LoadAvsc(schema_files_location + \"/default_key.avsc\", known_schemas)\nreturn default_event_key_schema\n</code></pre> (*) Where <code>known_schemas</code> is an Avro schema dictionary where all Avro schemas read get stored in order to be able to read nested Avro schemas afterwards. See the python script in detail for examples of this.</p> </li> <li> <p>A function to open a file, read its content as an Avro schema and store it in the Avro schema dictionary:</p> <pre><code>def LoadAvsc(file_path, names=None):\n# Load avsc file\n# file_path: path to schema file\n# names(optional): avro.schema.Names object\nfile_text = open(file_path).read()\njson_data = json.loads(file_text)\nschema = avro.schema.SchemaFromJSONData(json_data, names)\nreturn schema\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-cloud/#kafka-avro-producer","title":"Kafka Avro Producer","text":"<p>This script, called KcAvroProducer.py, will actually be the responsible for creating the Kafka Avro Producer, initialize and configure it and provide the publish method:</p> <ol> <li> <p>Initialize and prepare the Kafka Producer</p> <pre><code>class KafkaProducer:\n\n    def __init__(self,kafka_brokers = \"\",kafka_apikey = \"\",schema_registry_url = \"\"):\n        self.kafka_brokers = kafka_brokers\n        self.kafka_apikey = kafka_apikey\n        self.schema_registry_url = schema_registry_url\n\n    def prepareProducer(self,groupID = \"pythonproducers\",key_schema = \"\", value_schema = \"\"):\n        options ={\n                'bootstrap.servers':  self.kafka_brokers,\n                'schema.registry.url': self.schema_registry_url,\n                'group.id': groupID,\n                'security.protocol': 'SASL_SSL',\n                'sasl.mechanisms': 'PLAIN',\n                'sasl.username': 'token',\n                'sasl.password': self.kafka_apikey\n        }\n        # Print out the configuration\n        print(\"--- This is the configuration for the producer: ---\")\n        print(options)\n        print(\"---------------------------------------------------\")\n        # Create the Avro Producer\n        self.producer = AvroProducer(options,default_key_schema=key_schema,default_value_schema=value_schema)\n</code></pre> </li> <li> <p>Publish method</p> <pre><code>def publishEvent(self, topicName, value, key):\n        # Produce the Avro message\n        # Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first\n        self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(value)[key], callback=self.delivery_report)\n        # Flush\n        self.producer.flush()\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-cloud/#run_1","title":"Run","text":"<p>We will see in the following section Schemas and Messages how to send Avro messages according with their schemas to IBM Event Streams.</p>"},{"location":"use-cases/schema-registry-on-cloud/#python-avro-consumer","title":"Python Avro Consumer","text":"<p>In this section we describe the python scripts we will be using in order to be able to consume Avro messages from a Kafka topic.</p>"},{"location":"use-cases/schema-registry-on-cloud/#consume-message","title":"Consume Message","text":"<p>The python script that we will use to consume an Avro message from a Kafka topic is ConsumeAvroMessage.py where we have the following:</p> <ol> <li> <p>A function to parse arguments:</p> <pre><code># Parse arguments to get the container ID to poll for\ndef parseArguments():\n    global TOPIC_NAME\n    print(\"The arguments for the script are: \" , str(sys.argv))\n    if len(sys.argv) != 2:\n        print(\"[ERROR] - The ConsumeAvroMessage.py script expects one arguments: The Kafka topic to events from.\")\n        exit(1)\n    TOPIC_NAME = sys.argv[1]\n</code></pre> </li> <li> <p>The main where we will:</p> <ol> <li>Parse the arguments to get the topic to read from</li> <li>Create the Kafka Consumer and configure it</li> <li>Poll for next avro message</li> <li>Close the Kafka consumer</li> </ol> <pre><code>if __name__ == '__main__':\n    # Parse arguments\n    parseArguments()\n    # Create the Kafka Avro consumer\n    kafka_consumer = KafkaConsumer(KAFKA_BROKERS,KAFKA_APIKEY,TOPIC_NAME,SCHEMA_REGISTRY_URL)\n    # Prepare the consumer\n    kafka_consumer.prepareConsumer()\n    # Consume next Avro event\n    kafka_consumer.pollNextEvent()\n    # Close the Avro consumer\n    kafka_consumer.close()\n</code></pre> </li> </ol> <p>As you can see, this python code depends on a Kafka Consumer which is explained next.</p>"},{"location":"use-cases/schema-registry-on-cloud/#kafka-avro-consumer","title":"Kafka Avro Consumer","text":"<p>This script, called KcAvroConsumer.py, will actually be the responsible for creating the Kafka Avro Consumer, initialize and configure it and provide the poll next event method:</p> <ol> <li> <p>Initialize and prepare the new Kafka consumer:</p> <pre><code>class KafkaConsumer:\n\n    def __init__(self, kafka_brokers = \"\", kafka_apikey = \"\", topic_name = \"\", schema_registry_url = \"\", autocommit = True):\n        self.kafka_brokers = kafka_brokers\n        self.kafka_apikey = kafka_apikey\n        self.topic_name = topic_name\n        self.schema_registry_url = schema_registry_url\n        self.kafka_auto_commit = autocommit\n\n    # See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md\n    def prepareConsumer(self, groupID = \"pythonconsumers\"):\n        options ={\n                'bootstrap.servers':  self.kafka_brokers,\n                'group.id': groupID,\n                'auto.offset.reset': 'earliest',\n                'schema.registry.url': self.schema_registry_url,\n                'enable.auto.commit': self.kafka_auto_commit,\n                'security.protocol': 'SASL_SSL',\n                'sasl.mechanisms': 'PLAIN',\n                'sasl.username': 'token',\n                'sasl.password': self.kafka_apikey\n        }\n        # Print the configuration\n        print(\"--- This is the configuration for the Avro consumer: ---\")\n        print(options)\n        print(\"---------------------------------------------------\")\n        # Create the Avro consumer\n        self.consumer = AvroConsumer(options)\n        # Subscribe to the topic\n        self.consumer.subscribe([self.topic_name])\n</code></pre> </li> <li> <p>Poll next event method:</p> <pre><code># Prints out the message\ndef traceResponse(self, msg):\n    print('@@@ pollNextOrder - {} partition: [{}] at offset {} with key {}:\\n\\tvalue: {}'\n                .format(msg.topic(), msg.partition(), msg.offset(), msg.key(), msg.value() ))\n\n# Polls for next event\ndef pollNextEvent(self):\n    # Poll for messages\n    msg = self.consumer.poll(timeout=10.0)\n    # Validate the returned message\n    if msg is None:\n        print(\"[INFO] - No new messages on the topic\")\n    elif msg.error():\n        if (\"PARTITION_EOF\" in msg.error()):\n            print(\"[INFO] - End of partition\")\n        else:\n            print(\"[ERROR] - Consumer error: {}\".format(msg.error()))\n    else:\n        # Print the message\n        msgStr = self.traceResponse(msg)\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-cloud/#run_2","title":"Run","text":"<p>We will see in the following section Schemas and Messages how to consume Avro messages.</p>"},{"location":"use-cases/schema-registry-on-cloud/#schemas-and-messages","title":"Schemas and Messages","text":"<p>In this section we are going to see how Schema Registry works when you have an application that produces and consumes messages based on Avro data schemas. The application we are going to use for this is the python scripts presented above in the Python Avro Producer and Python Avro Consumer.</p> <p>Once again, we are going to run these scripts in the python demo environment we presented earlier in this lab in this section. Please, review that section in order to understand how to run the environment in your local workstation.</p> <ol> <li> <p>Make sure you have a newly created topic for this exercise (review the IBM Event Streams on IBM Cloud lab if needed):</p> <pre><code>$ ibmcloud es topic-create test\nCreated topic test\nOK\n\n$ ibmcloud es topics\nTopic name\ntest\nOK\n</code></pre> </li> <li> <p>Make sure you dont have any schema registered (preferably for clarity):</p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts\n[]\n</code></pre> </li> <li> <p>Start your python environment with:</p> <pre><code>$ docker run -e KAFKA_BROKERS=$KAFKA_BROKERS \\\n             -e KAFKA_APIKEY=$KAFKA_APIKEY \\\n             -e SCHEMA_REGISTRY_URL=$SCHEMA_REGISTRY_URL \\\n             -v ${PWD}:/tmp/lab \\\n             --rm \\\n             -ti ibmcase/python-schema-registry-lab:latest bash\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-cloud/#create-a-message","title":"Create a message","text":"<p>In order to create a message, we execute the <code>ProduceAvroMessage.py</code> within the <code>/tmp/lab/src</code> folder in our python demo environment. This script, as you could see in the Python Avro Producer section, it is sending the event <code>{'eventKey': '1', 'message': 'This is a test message'}</code> according to the schemas defined in default_key.avsc and default_value.avsc for the key and value of the event respectively.</p> <pre><code>python ProduceAvroMessage.py test\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test']\nCreating event...\nDONE\n--- Container event to be published: ---\n{'eventKey': '1', 'message': 'This is a test message'}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093', 'schema.registry.url': 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****'}\n---------------------------------------------------\nMessage delivered to test [0]\n</code></pre> <p>We should now have a new message in our <code>test</code> kafka topic. We can check that out using Kafdrop:</p> <p></p> <p> <p>INFO: Mind the message now is not in JSON format as Avro does not repeat every field name with every single record which makes Avro more efficient than JSON for high-volume usage. This is thanks to having Avro schemas.</p> <p></p> <p> <p>WARNING: Most of the Avro producer clients, whether it is in Java, Python or many other languages, give users the ability to auto-register a schema automatically with the specified schema registry in its configuration.</p> <p></p> <p>If we look know at the schemas our schema registry has:</p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts\n\n[\"test-key\",\"test-value\"]\n</code></pre> <p>we see two schemas, <code>test-key</code> and <code>test-value</code>, which in fact correspond to the Avro data schema used for the <code>key</code> (default_key.avsc) and the <code>value</code> (default_value.avsc) of events sent to the <code>test</code> topic in the ProduceAvroMessage.py as explained before sending the message.</p> <p>To make sure of what we are saying, we can inspect those schemas:</p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts/test-key\n\n\"string\"\n</code></pre> <pre><code>$ curl -s -u token:$KAFKA_APIKEY $URL/artifacts/test-value | jq .\n{\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"eventKey\",\n      \"doc\": \"We expect any string as the event key\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string message\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema, composed of the key again and any string message\"\n}\n</code></pre> <p>If I now decided that my events should contain another attribute, I would modify the event value schema (default_value.avsc) to reflect that as well as <code>ProduceAvroMessage.py</code> to send that new attribute in the event it sends:</p> <pre><code># python ProduceAvroMessage.py test\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test']\nCreating event...\nDONE\n--- Container event to be published: ---\n{'eventKey': '1', 'message': 'This is a test message', 'anotherAttribute': 'This is another atttribute'}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093', 'schema.registry.url': 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****'}\n---------------------------------------------------\nMessage delivered to test [0]\n</code></pre> <p>I can see that an event with a new attribute has been sent:</p> <p></p> <p>And I can also see that the new shcema has got registered as well as a new version for the already exisiting schema:</p> <p><pre><code>$ curl -s -u token:$KAFKA_APIKEY $URL/artifacts/test-value/versions\n\n[1,2]\n</code></pre> <pre><code>$ curl -s -u token:$KAFKA_APIKEY $URL/artifacts/test-value | jq .\n{\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"eventKey\",\n      \"doc\": \"We expect any string as the event key\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string message\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"anotherAttribute\",\n      \"doc\": \"Another string attribute for demo\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema, composed of the key again and any string message\"\n}\n</code></pre></p> <p> <p>SECURITY: As some of you may have already thought, having your clients (that is your applications), auto-register the Avro data schemas that are in the end kind of the contracts that your components of your overal solution agree on in order to understand each other and collaborate between them is NOT a good idea. Specially in microservices architectures where you might have hundreds of microservices talking and collaborating among themselsves. We will see in the Security section how we can control schema registration and evolution based on roles at the schema level also.</p> <p></p>"},{"location":"use-cases/schema-registry-on-cloud/#create-a-non-compliant-message","title":"Create a non-compliant message","text":"<p>Now, we are trying to send a non-compliant message according to the Avro data schema we have for our events. Im going to try to send the following event:</p> <pre><code>{\n    'eventKey': '1',\n    'message': 'This is a test message',\n    'anotherAttribute': 'This is another atttribute',\n    'yetAnotherAttribute': 'This should fail'\n}\n</code></pre> <p>and this is the output of that attempt:</p> <pre><code># python ProduceAvroMessage.py test\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test']\nCreating event...\nDONE\n--- Container event to be published: ---\n{'eventKey': '1', 'message': 'This is a test message', 'anotherAttribute': 'This is another atttribute', 'yetAnotherAttribute': 'This should fail'}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093', 'schema.registry.url': 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****'}\n---------------------------------------------------\nTraceback (most recent call last):\n  File \"ProduceAvroMessage.py\", line 77, in &lt;module&gt;\n    kp.publishEvent(TOPIC_NAME,message_event,\"eventKey\")\n  File \"/tmp/lab/kafka/KcAvroProducer.py\", line 39, in publishEvent\n    self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(value)[key], callback=self.delivery_report)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\", line 99, in produce\n    value = self._serializer.encode_record_with_schema(topic, value_schema, value)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 118, in encode_record_with_schema\n    return self.encode_record_with_schema_id(schema_id, record, is_key=is_key)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 152, in encode_record_with_schema_id\n    writer(record, outf)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 86, in &lt;lambda&gt;\n    return lambda record, fp: writer.write(record, avro.io.BinaryEncoder(fp))\n  File \"/root/.local/lib/python3.7/site-packages/avro/io.py\", line 771, in write\n    raise AvroTypeException(self.writer_schema, datum)\navro.io.AvroTypeException: The datum {'eventKey': '1', 'message': 'This is a test message', 'anotherAttribute': 'This is another atttribute', 'yetAnotherAttribute': 'This should fail'} is not an example of the schema {\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"eventKey\",\n      \"doc\": \"We expect any string as the event key\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string message\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"anotherAttribute\",\n      \"doc\": \"Another string attribute for demo\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema, composed of the key again and any string message\"\n}\n</code></pre> <p>As we can see, the attempt failed as the Avro producer will check the message against the Avro data schema defined for the topic we want to send the message to and yield that this message does not comply.</p> <p>Therefore, using Avro schemas with IBM Event Streams give us the ability to build our system with robustness protecting downstream data consumers from malformed data, as only valid data will be permitted in the topic.</p>"},{"location":"use-cases/schema-registry-on-cloud/#consume-a-message","title":"Consume a message","text":"<p>In order to consume a message, we execute the <code>ConsumeAvroMessage.py</code> within the <code>/tmp/lab/src</code> folder in our python demo environment:</p> <pre><code># python ConsumeAvroMessage.py test\n @@@ Executing script: ConsumeAvroMessage.py\nThe arguments for the script are:  ['ConsumeAvroMessage.py', 'test']\n--- This is the configuration for the Avro consumer: ---\n{'bootstrap.servers': 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'schema.registry.url': 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****'}\n---------------------------------------------------\n[Message] - Next message consumed from test partition: [0] at offset 0 with key 1:\n    value: {'eventKey': '1', 'message': 'This is a test message'}\n\n# python ConsumeAvroMessage.py test\n @@@ Executing script: ConsumeAvroMessage.py\nThe arguments for the script are:  ['ConsumeAvroMessage.py', 'test']\n--- This is the configuration for the Avro consumer: ---\n{'bootstrap.servers': 'kafka-2.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-1.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093,kafka-0.mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud:9093', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'schema.registry.url': 'https://token:4uk9gZ-n85a2esMoMZ5wtW-yIq_29o3PrHVBEFBj67N0@mh-tcqsppdpzlrkdmkbgmgl-4c201a12d7add7c99d2b22e361c6f175-0000.eu-de.containers.appdomain.cloud/confluent', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': '*****'}\n---------------------------------------------------\n[Message] - Next message consumed from test partition: [0] at offset 1 with key 1:\n    value: {'eventKey': '1', 'message': 'This is a test message', 'anotherAttribute': 'This is another atttribute'}\n</code></pre> <p>As you can see, our script was able to read the Avro messages from the <code>test</code> topic and map that back to their original structure thanks to the Avro schemas:</p> <pre><code>[Message] - Next message consumed from test partition: [0] at offset 0 with key 1:\n    value: {'eventKey': '1', 'message': 'This is a test message'}\n\n[Message] - Next message consumed from - test partition: [0] at offset 1 with key 1:\n    value: {'eventKey': '1', 'message': 'This is a test message', 'anotherAttribute': 'This is another atttribute'}\n</code></pre>"},{"location":"use-cases/schema-registry-on-cloud/#data-evolution","title":"Data Evolution","text":"<p>So far we have more or less seen what Avro is, what an Avro data schema is, what a schema registry is and how this all works together. From creating an Avro data schema for your messages/events to comply with to how the schema registry and Avro data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their Avro data schemas to the rich API the IBM Event Streams on IBM Cloud schema registry provides to interact with.</p> <p>However, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying Event Storming or Domain Driven Design for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data, like your use or business cases, may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases.</p> <p>But it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event backbone) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the event source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to hundreds of years) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the compatibility of old and new data schemas and, in fact, old and new data at the end of the day.</p> <p>The IBM Event Streams on IBM Cloud schema registry supports enforcing compatibility rules when creating a new version of a schema. If a request is made to create a new schema version that does not conform to the required compatibility rule, then the registry will reject the request. The following rules are supported:</p> Compatibility Tested against Descrition NONE N/A No compatibility checking is performed when a new schema version is created BACKWARD Latest version of the schema A new version of the schema can omit fields that are present in the existing version of the schema. A new version of the schema can add optional fields that are not present in the existing version of the schema. BACKWARD_TRANSITIVE All versions of the schema Same as above FORWARD Latest version of the schema A new version of the schema can add fields that are not present in the existing version of the schema. A new version of the schema can omit optional fields that are present in the existing version of the schema. FORWARD_TRANSITIVE All versions of the schema Same as above FULL Latest version of the schema A new version of the schema can add optional fields that are not present in the existing version of the schema. A new version of the schema can omit optional fields that are present in the existing version of the schema. FULL_TRANSITIVE All versions of the schema Same as above <p>These rules can be applied at two scopes:</p> <ol> <li>At a global scope, which is the default that</li> <li>At a per-schema level. If a per-schema level rule is defined, then this overrides the global default for the particular schema.</li> </ol> <p>By default, the registry has a global compatibility rule setting of NONE. Per-schema level rules must be defined otherwise the schema will default to using the global setting.</p>"},{"location":"use-cases/schema-registry-on-cloud/#rules","title":"Rules","text":"<p>In this section we are going to see how to interact with the schema registry in order to alter the compatibility rules both at the global and per-schema level.</p>"},{"location":"use-cases/schema-registry-on-cloud/#get-the-current-value-of-a-global-rule","title":"Get the current value of a global rule","text":"<p>The current value of a global rule is retrieved by issuing a GET request to the <code>/rules/{rule-type}</code> endpoint, (where <code>{rule-type}</code> is the type of global rule to be retrieved - currently the only supported type is COMPATIBILITY). If the request is successful then the current rule configuration is returned in the payload of the response, together with a status code of 200 (OK).</p> <p>Getting global rule configuration requires at least:</p> <ul> <li> <p>Reader role access to the Event Streams cluster resource type</p> </li> <li> <p>Get the current value for the <code>COMPATIBILITY</code> global rule with</p> <p><code>curl -u token:$KAFKA_APIKEY $URL/rules/COMPATIBILITY</code></p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/rules/COMPATIBILITY\n\n{\"type\":\"COMPATIBILITY\",\"config\":\"NONE\"}\n</code></pre> </li> </ul> <p>As already explained, the default out of the box global compatibility rule for IBM Event Streams on IBM Cloud schema registry is <code>NONE</code>.</p>"},{"location":"use-cases/schema-registry-on-cloud/#update-a-global-rule","title":"Update a global rule","text":"<p>Global compatibility rules can be updated by issuing a PUT request to the <code>/rules/{rule-type}</code> endpoint, (where <code>{rule-type}</code> identifies the type of global rule to be updated - currently the only supported type is COMPATIBILITY), with the new rule configuration in the body of the request. If the request is successful then the newly updated rule config is returned in the payload of the response, together with a status code of 200 (OK).</p> <p>Updating a global rule configuration requires at least:</p> <ul> <li> <p>Manager role access to the Event Streams cluster resource type</p> </li> <li> <p>Update the compatibility globar rule from NONE to FORWARD with:</p> <p><code>curl -u token:$KAFKA_APIKEY \u2013X PUT $URL/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"&lt;COMPATIBILITY_MODE&gt;\"}'</code></p> <p><pre><code>$  curl -u token:$KAFKA_APIKEY -X PUT $URL/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"FORWARD\"}'\n\n{\"type\":\"COMPATIBILITY\",\"config\":\"FORWARD\"}\n</code></pre> 1. Verify it has been changed:</p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/rules/COMPATIBILITY\n\n{\"type\":\"COMPATIBILITY\",\"config\":\"FORWARD\"}\n</code></pre> </li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#get-a-per-schema-rule","title":"Get a per-schema rule","text":"<p>To retrieve the current value of a type of rule being applied to a specific schema, a GET request is made to the <code>/artifacts/{schema-id}/rules/{rule-type}</code> endpoint, (where <code>{schema-id}</code> is the ID of the schema, and <code>{rule-type}</code> is the type of global rule to be retrieved - currently the only supported type is COMPATIBILITY). If the request is successful then the current rule value is returned in the payload of the response, together with a status code of 200 (OK).</p> <p>Getting per-schema rules requires at least:</p> <ul> <li>Reader role access to the Event Streams cluster resource type</li> <li> <p>Reader role access to the schema resource to which the rule applies</p> </li> <li> <p>Get the compatibility rule for our <code>test-value</code> schema with:</p> <p><code>curl -u token:$KAFKA_APIKEY $URL/artifacts/&lt;YOUR_SCHEMA_ID&gt;/rules/COMPATIBILITY</code></p> <p><pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts/test-value/rules/COMPATIBILITY\n\n{\"error_code\":404,\"message\":\"no compatibility rule exists for artifact 'test-value'\"}\n</code></pre> which makes sense as we have not yet created a compatibility rule for the <code>test-value</code> schema</p> </li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#create-a-per-schema-rule","title":"Create a per-schema rule","text":"<p>Rules can be applied to a specific schema, overriding any global rules which have been set, by making a POST request to the <code>/artifacts/{schema-id}/rules</code> endpoint, (where <code>{schema-id}</code> is the ID of the schema), with the type and value of the new rule contained in the body of the request, (currently the only supported type is COMPATIBILITY). If successful an empty response, and a status code of 204 (no content) is returned.</p> <p>Creating per-schema rules requires at least:</p> <ul> <li>Reader role access to the Event Streams cluster resource type</li> <li> <p>Manager role access to the schema resource for which the rule will apply</p> </li> <li> <p>Create a compatibility rule for our <code>test-value</code> schema with:</p> <p><code>curl -u token:$KAFKA_APIKEY $URL/artifacts/&lt;YOUR_SCHEMA_ID&gt;/rules -d '{\"type\":\"COMPATIBILITY\",\"config\":\"&lt;COMPATIBILITY_MODE&gt;\"}'</code></p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts/test-value/rules -d '{\"type\":\"COMPATIBILITY\",\"config\":\"FULL\"}'\n\n{\"type\":\"COMPATIBILITY\",\"config\":\"FULL\"}\n</code></pre> </li> <li> <p>Make sure this compatibility rules has been created:</p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts/test-value/rules/COMPATIBILITY\n\n{\"type\":\"COMPATIBILITY\",\"config\":\"FULL\"}\n</code></pre> </li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#update-a-per-schema-rule","title":"Update a per-schema rule","text":"<p>The rules applied to a specific schema are modified by making a PUT request to the <code>/artifacts/{schema-id}/rules/{rule-type}</code> endpoint, (where <code>{schema-id}</code> is the ID of the schema, and <code>{rule-type}</code> is the type of global rule to be retrieved - currently the only supported type is COMPATIBILITY). If the request is successful then the newly updated rule config is returned in the payload of the response, together with a status code of 200 (OK).</p> <p>Updating a per-schema rule requires at least:</p> <ul> <li>Reader role access to the Event Streams cluster resource type</li> <li> <p>Manager role access to the schema resource to which the rule applies</p> </li> <li> <p>Update the compatibility rule we have for our schema <code>test-value</code> with:</p> <p><code>curl -u token:$KAFKA_APIKEY \u2013X PUT $URL/artifacts/&lt;YOUR_SCHEMA_ID&gt;/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"&lt;COMPATIBILITY_MODE&gt;\"}'</code></p> <pre><code>$  curl -u token:$KAFKA_APIKEY -X PUT $URL/artifacts/test-value/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"BACKWARD\"}'\n\n{\"type\":\"COMPATIBILITY\",\"config\":\"BACKWARD\"}\n</code></pre> </li> <li> <p>Make sure the compatibility mode for our <code>test-value</code> schema is as expected:</p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts/test-value/rules/COMPATIBILITY\n\n{\"type\":\"COMPATIBILITY\",\"config\":\"BACKWARD\"}\n</code></pre> </li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#delete-a-per-schema-rule","title":"Delete a per-schema rule","text":"<p>The rules applied to a specific schema are deleted by making a DELETE request to the <code>/artifacts/{schema-id}/rules/{rule-type}</code> endpoint, (where <code>{schema-id}</code> is the ID of the schema, and <code>{rule-type}</code> is the type of global rule to be retrieved - currently the only supported type is COMPATIBILITY). If the request is successful then an empty response is returned, with a status code of 204 (no content).</p> <p>Deleting a per-schema rule requires at least:</p> <ul> <li>Reader role access to the Event Streams cluster resource type</li> <li> <p>Manager role access to the schema resource to which the rule applies</p> </li> <li> <p>Delete the compatibility rule we have for our <code>test-value</code> schema with:</p> <p><code>curl -u token:$KAFKA_APIKEY \u2013X DELETE $URL/artifacts/&lt;YOUR_SCHEMA_ID&gt;/rules/COMPATIBILITY</code></p> <pre><code>$ curl -u token:$KAFKA_APIKEY -X DELETE $URL/artifacts/test-value/rules/COMPATIBILITY\n</code></pre> </li> <li> <p>Make sure there is no compatibility rule for our <code>test-value</code> schema now:</p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts/test-value/rules/COMPATIBILITY\n\n{\"error_code\":404,\"message\":\"no compatibility rule exists for artifact 'test-value'\"}\n</code></pre> </li> </ul>"},{"location":"use-cases/schema-registry-on-cloud/#evolve-your-schemas","title":"Evolve your schemas","text":"<p>In this section, we are going to review the different compatibility modes for our Avro data schemas to evolve (hence our data). For simplicity though, we are going to to review only the following types of schema compatibility and at the global level:</p> <ol> <li>None</li> <li>Backward</li> <li>Forward</li> <li>Full</li> </ol>"},{"location":"use-cases/schema-registry-on-cloud/#none","title":"None","text":"<p>First of all, set the compatibility mode to backwards at the global level (for simplicity):</p> <pre><code>$ curl -u token:$KAFKA_APIKEY -X PUT $URL/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"NONE\"}'\n\n{\"type\":\"COMPATIBILITY\",\"config\":\"NONE\"}\n</code></pre> <p>This compatibility rule does not perform any compatibility check at all when a new schema version is created.</p> <p>That is, if we use the <code>test-value</code> schema we've used through this tutorial, and try to register a completely different schema like:</p> <pre><code>{\n    \"namespace\": \"ibm.eda.default\",\n    \"doc\": \"New Default Message's value Avro data schema, completely different than its predecesors to demonstrate None compatibility\",\n    \"type\":\"record\",\n    \"name\":\"defaultValue\",\n    \"fields\":[\n            {\n                \"name\": \"anAttribute\",\n                \"type\":\"int\",\n                \"doc\": \"an attribute that is an integer\"\n            },\n            {\n                \"name\": \"anotherAttribute\",\n                \"type\":\"long\",\n                \"doc\": \"Just a long number\"\n            }\n     ]\n}\n</code></pre> <p>it should work:</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: test-value' \\\n        $URL/artifacts/test-value/versions \\\n        -d '{   \"namespace\": \"ibm.eda.default\",\n                \"doc\": \"New Default Message value Avro data schema, completely different than its predecesors to demonstrate None compatibility\",\n                \"type\":\"record\",\n                \"name\":\"defaultValue\",\n                \"fields\":[\n                    {\"name\": \"anAttribute\",\"type\":\"int\",\"doc\": \"an attribute that is an integer\"},\n                    {\"name\": \"anotherAttribute\",\"type\":\"long\",\"doc\": \"Just a long number\"}]}'\n\n{\"id\":\"test-value\",\"type\":\"AVRO\",\"version\":3,\"createdBy\":\"\",\"createdOn\":1589479701588,\"modifiedBy\":\"\",\"modifiedOn\":1589551226293,\"globalId\":23}\n</code></pre> <p>As we can see, now the schema for the <code>test</code> topic will enforce that events coming into the topic are formed by an integer and a long number.</p>"},{"location":"use-cases/schema-registry-on-cloud/#new-schema","title":"New Schema","text":"<p>For the following compatibility modes, we are going to use a new schema to describe a person. It will be called <code>demo-schema</code> and will looks like this:</p> <pre><code>{\n    \"namespace\": \"schema.compatibility.test\",\n    \"name\": \"person\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\"\n        },\n        {\n            \"name\" : \"gender\",\n            \"type\" : \"string\"\n        }\n    ]\n }\n</code></pre> <ol> <li> <p>Register the schema:</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: demo-schema' \\\n        $URL/artifacts \\\n        -d '{\n            \"namespace\": \"schema.compatibility.test\",\n            \"name\": \"person\",\n            \"type\": \"record\",\n            \"fields\" : [\n                {\"name\" : \"name\", \"type\" : \"string\"},\n                {\"name\" : \"age\", \"type\" : \"int\"},\n                {\"name\" : \"gender\", \"type\" : \"string\"}]}'\n\n{\"id\":\"demo-schema\",\"type\":\"AVRO\",\"version\":1,\"createdBy\":\"\",\"createdOn\":1589553367867,\"modifiedBy\":\"\",\"modifiedOn\":1589553367867,\"globalId\":24}\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-cloud/#backward","title":"Backward","text":"<p>First of all, set the compatibility mode to backwards at the global level (for simplicity):</p> <pre><code>$ curl -u token:$KAFKA_APIKEY -X PUT $URL/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"BACKWARD\"}'\n\n{\"type\":\"COMPATIBILITY\",\"config\":\"BACKWARD\"}\n</code></pre> <p>Now, What if we decide to change the data schema to add a new attribute such as place of birth? That is, the new schema would look like:</p> <pre><code>{\n    \"namespace\": \"schema.compatibility.test\",\n    \"name\": \"person\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\"\n        },\n        {\n            \"name\" : \"gender\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"place_of_birth\",\n            \"type\" : \"string\"\n        }\n    ]\n}\n</code></pre> <p>If we try to create a new version for our <code>demo-schema</code> to include the <code>place_of_birth</code> attribute with the compatibility mode set to backwards, we will get the following:</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: demo-schema' \\\n        $URL/artifacts/demo-schema/versions \\\n        -d '{\n            \"namespace\": \"schema.compatibility.test\",\n            \"name\": \"person\",\n            \"type\": \"record\",\n            \"fields\" : [\n                {\"name\" : \"name\", \"type\" : \"string\"},\n                {\"name\" : \"age\", \"type\" : \"int\"},\n                {\"name\" : \"gender\", \"type\" : \"string\"},\n                {\"name\" : \"place_of_birth\",\"type\" : \"string\"}]}'\n\n{\"error_code\":409,\"message\":\"Schema failed compatibility check with version ID: demo-schema, schema not backward compatible: reader's 'record' schema named 'schema.compatibility.test.person' contains a field named 'place_of_birth' that does not match any field in the corresponding writer's schema. The reader's schema does not specify a default value for this field\"}\n</code></pre> <p>We see we get an error that says that the new schema version we are trying to register for our <code>demo-schema</code> is not backward compatible because \"reader's 'record' schema named 'schema.compatibility.test.person' contains a field named 'place_of_birth' that does not match any field in the corresponding writer's schema. The reader's schema does not specify a default value for this field\"</p> <p>As the reason above explains, backward compatibility means that consumers using the new schema can read data produced with the last schema. As it stands at the moment, this is not satisfied since the consumer with the newer version expects the attribute <code>place_of_birth</code>.</p> <p>As the error explanation above also suggests, in order to include a new attribute in our schema when we have backward compatibility mode enabled, we need to provide a default value for it so that the consumer uses it when reading messages produced with the older version of the schema that will not include the newer attribute. That is, we need our newer schema version to be like:</p> <pre><code>{\n    \"namespace\": \"schema.compatibility.test\",\n    \"name\": \"person\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\"\n        },\n        {\n            \"name\" : \"gender\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"place_of_birth\",\n            \"type\" : \"string\",\n            \"default\": \"nonDefined\"\n        }\n    ]\n}\n</code></pre> <p>so that the consumer will use <code>nonDefined</code> as the value for <code>place_of_birth</code> whenever it consumes messages produced with the older shcema version that do not include the attribute.</p> <p>Let's check:</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: demo-schema' \\\n        $URL/artifacts/demo-schema/versions \\\n        -d '{\n            \"namespace\": \"schema.compatibility.test\",\n            \"name\": \"person\",\n            \"type\": \"record\",\n            \"fields\" : [\n                {\"name\" : \"name\", \"type\" : \"string\"},\n                {\"name\" : \"age\", \"type\" : \"int\"},\n                {\"name\" : \"gender\", \"type\" : \"string\"},\n                {\"name\" : \"place_of_birth\",\"type\" : \"string\",\"default\": \"nonDefined\"}]}'\n\n{\"id\":\"demo-schema\",\"type\":\"AVRO\",\"version\":2,\"createdBy\":\"\",\"createdOn\":1589553367867,\"modifiedBy\":\"\",\"modifiedOn\":1589556982143,\"globalId\":27}\n</code></pre> <p>Effectively, we have just evolved our Avro data <code>demo-schema</code> schema to include a new attribute called <code>place_of_birth</code>.</p> <p>Now, how about if we wanted to delete an attribute in our schema? Let's try to remove the <code>gender</code> attribute:</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: demo-schema' \\\n        $URL/artifacts/demo-schema/versions \\\n        -d '{\n            \"namespace\": \"schema.compatibility.test\",\n            \"name\": \"person\",\n            \"type\": \"record\",\n            \"fields\" : [\n                {\"name\" : \"name\", \"type\" : \"string\"},\n                {\"name\" : \"age\", \"type\" : \"int\"},\n                {\"name\" : \"place_of_birth\",\"type\" : \"string\",\"default\": \"nonDefined\"}]}'\n\n{\"id\":\"demo-schema\",\"type\":\"AVRO\",\"version\":3,\"createdBy\":\"\",\"createdOn\":1589553367867,\"modifiedBy\":\"\",\"modifiedOn\":1589557152931,\"globalId\":28}\n</code></pre> <p>It worked. The resaon for this is that the consumer reading messages with the newer schema that does not contain the <code>gender</code> attribute, will simply ignore/drop all those attributes in the old person events/messages that are not defined in the newer data schema and just take in those that are defined. In this case, if it reads messages produced with the older data schema that come with the <code>gender</code> attribute, it will simply drop it.</p>"},{"location":"use-cases/schema-registry-on-cloud/#forward","title":"Forward","text":"<p>First of all, set the compatibility mode to forward at the global level (for simplicity):</p> <pre><code>$ curl -u token:$KAFKA_APIKEY -X PUT $URL/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"FORWARD\"}'\n\n{\"type\":\"COMPATIBILITY\",\"config\":\"FORWARD\"}\n</code></pre> <p>Now, how about removing an attribute when the compatibility type configured is set to FORWARD? Let's try to remove the <code>age</code> attribute:</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: demo-schema' \\\n        $URL/artifacts/demo-schema/versions \\\n        -d '{\n            \"namespace\": \"schema.compatibility.test\",\n            \"name\": \"person\",\n            \"type\": \"record\",\n            \"fields\" : [\n                {\"name\" : \"name\", \"type\" : \"string\"},\n                {\"name\" : \"place_of_birth\",\"type\" : \"string\",\"default\": \"nonDefined\"}]}'\n\n{\"error_code\":409,\"message\":\"Schema failed compatibility check with version ID: demo-schema, schema not forward compatible: reader's 'record' schema named 'schema.compatibility.test.person' contains a field named 'age' that does not match any field in the corresponding writer's schema. The reader's schema does not specify a default value for this field\"}\n</code></pre> <p>As the error above explains, the problem we have when we want to remove an attribute from our Avro data schema with the compatibility mode set to <code>forward</code> is that the producer will be sending messages without the removed attribute (<code>age</code> in our case) while the consumer will be reading messages with the older Avro data schema that expects such attribute, which is what the forward compatibility required: forward compatibility means that data produced with a new schema can be read by consumers using the last schema.</p> <p>What can we do to have a schema that allows the producer to send messages that do not contain an attribute that the consumer expects?</p> <p>The trick here is to first register an \"intermediate\" data schema that adds a default value to the attibute we want to remove from the schema (<code>age</code> in our case). This way, the \"intermediate\" data schema will become the older data schema for the consumers so when we register the new schema without the attribute we wanted to remove and produce data according to it (that is, without the <code>age</code> attribute) afterwards, the consumers will not complain since they will have in the schema they use (the \"intermediate\" schema) a default value for that attribute. And they will use that default value when reading messages without the <code>age</code> attribute.</p> <p>That is, our intermediate schema will be:</p> <pre><code>{\n    \"namespace\": \"schema.compatibility.test\",\n    \"name\": \"person\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\",\n            \"default\": 0,\n        },\n        {\n            \"name\" : \"place_of_birth\",\n            \"type\" : \"string\",\n            \"default\": \"nonDefined\"\n        }\n    ]\n}\n</code></pre> <p>If we try to register that new version of our <code>demo-schema</code> schema:</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: demo-schema' \\\n        $URL/artifacts/demo-schema/versions \\\n        -d '{\n            \"namespace\": \"schema.compatibility.test\",\n            \"name\": \"person\",\n            \"type\": \"record\",\n            \"fields\" : [\n                {\"name\" : \"name\", \"type\" : \"string\"},\n                {\"name\" : \"age\", \"type\" : \"int\", \"default\": 0},\n                {\"name\" : \"place_of_birth\",\"type\" : \"string\", \"default\": \"nonDefined\"}]}'\n\n{\"id\":\"demo-schema\",\"type\":\"AVRO\",\"version\":4,\"createdBy\":\"\",\"createdOn\":1589553367867,\"modifiedBy\":\"\",\"modifiedOn\":1589560847136,\"globalId\":30}\n</code></pre> <p>we see we succeed since we are simply adding a default value for the newer schema. This intermediate schema, will become now the older schema at the consumer side but will still be forward compatible with the newer schema at the producer side because it now has a default value for <code>age</code>.  Let's try now to add a new version of our <code>demo-schema</code> schema without the <code>age</code> attribute:</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: demo-schema' \\\n        $URL/artifacts/demo-schema/versions \\\n        -d '{\n            \"namespace\": \"schema.compatibility.test\",\n            \"name\": \"person\",\n            \"type\": \"record\",\n            \"fields\" : [\n                {\"name\" : \"name\", \"type\" : \"string\"},\n                {\"name\" : \"place_of_birth\",\"type\" : \"string\", \"default\": \"nonDefined\"}]}'\n\n{\"id\":\"demo-schema\",\"type\":\"AVRO\",\"version\":5,\"createdBy\":\"\",\"createdOn\":1589553367867,\"modifiedBy\":\"\",\"modifiedOn\":1589561057270,\"globalId\":31}\n</code></pre> <p>tada! we succeeded. As we said, the older schema now at the consumer side has a default value for <code>age</code> so the new messages produced with the newer schema coming without the <code>age</code> attribute can be successfully read by the consumer using the default value.</p> <p>How about adding a new attribute when the compatibility mode is set to forward? Let's try to add the attribute <code>siblings</code> to denote the number of borthers and sisters a person might have:</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: demo-schema' \\\n        $URL/artifacts/demo-schema/versions \\\n        -d '{\n            \"namespace\": \"schema.compatibility.test\",\n            \"name\": \"person\",\n            \"type\": \"record\",\n            \"fields\" : [\n                {\"name\" : \"name\", \"type\" : \"string\"},\n                {\"name\" : \"place_of_birth\",\"type\" : \"string\", \"default\": \"nonDefined\"},\n                {\"name\" : \"siblings\", \"type\" : \"int\"}]}'\n\n{\"id\":\"demo-schema\",\"type\":\"AVRO\",\"version\":6,\"createdBy\":\"\",\"createdOn\":1589553367867,\"modifiedBy\":\"\",\"modifiedOn\":1589561302428,\"globalId\":32}\n</code></pre> <p>Contrary to what happened when addign a new attribute to an Avro data schema when the compatibility mode is set to backward, adding an attribute to an Avro data schema when the compatibility mode is set to forward is not a problem because the new attributes coming with the messages that the consumer does not expect based on the older Avro data schema it uses will simply get dropped/skipped.</p>"},{"location":"use-cases/schema-registry-on-cloud/#full","title":"Full","text":"<p>First of all, set the compatibility mode to full at the global level (for simplicity):</p> <pre><code>$ curl -u token:$KAFKA_APIKEY -X PUT $URL/rules/COMPATIBILITY -d '{\"type\":\"COMPATIBILITY\",\"config\":\"FULL\"}'\n\n{\"type\":\"COMPATIBILITY\",\"config\":\"FULL\"}\n</code></pre> <p>Full compatibility means data schemas are both backward and forward compatible. Data schemas evolve in a fully compatible way: old data can be read with the new data schema, and new data can also be read with the last data schema.</p> <p>In some data formats, such as JSON, there are no full-compatible changes. Every modification is either only forward or only backward compatible. But in other data formats, like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change.</p> <p>So let's see if we can delete the <code>place_of_birth</code> attribute (the only attribute in our data schema that defines a default value):</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: demo-schema' \\\n        $URL/artifacts/demo-schema/versions \\\n        -d '{\n            \"namespace\": \"schema.compatibility.test\",\n            \"name\": \"person\",\n            \"type\": \"record\",\n            \"fields\" : [\n                {\"name\" : \"name\", \"type\" : \"string\"},\n                {\"name\" : \"siblings\", \"type\" : \"int\"}]}'\n\n{\"id\":\"demo-schema\",\"type\":\"AVRO\",\"version\":7,\"createdBy\":\"\",\"createdOn\":1589553367867,\"modifiedBy\":\"\",\"modifiedOn\":1589562453126,\"globalId\":33}\n</code></pre> <p>It seems it works. But how about removing an attribute that does not have a default value? Well, we need to do the trick of the \"intermediate\" schema again where we first add a default for that attribute and register the schema to later register a newer schema where we remove that attibute.</p> <p>Let's see adding a new attribute called <code>height</code> (remember we need to add a default value for it because of the full compatibility mode):</p> <pre><code>$ curl  -u token:$KAFKA_APIKEY \\\n        -H 'Content-Type: application/json' \\\n        -H 'X-Registry-ArtifactId: demo-schema' \\\n        $URL/artifacts/demo-schema/versions \\\n        -d '{\n            \"namespace\": \"schema.compatibility.test\",\n            \"name\": \"person\",\n            \"type\": \"record\",\n            \"fields\" : [\n                {\"name\" : \"name\", \"type\" : \"string\"},\n                {\"name\" : \"siblings\", \"type\" : \"int\"},\n                {\"name\" : \"height\", \"type\" : \"int\", \"default\": 0}]}'\n\n{\"id\":\"demo-schema\",\"type\":\"AVRO\",\"version\":8,\"createdBy\":\"\",\"createdOn\":1589553367867,\"modifiedBy\":\"\",\"modifiedOn\":1589562591316,\"globalId\":34}\n</code></pre> <p>We see it also works. Now we know how a data schema can evolve when full compatibility is required.</p> <p>As a final exercise, check how many times we have evolve our <code>demo-schema</code> schema for the compatibility rules:</p> <pre><code>$ curl -u token:$KAFKA_APIKEY $URL/artifacts/demo-schema/versions\n\n[1,2,3,4,5,6,7,8]\n</code></pre>"},{"location":"use-cases/schema-registry-on-cloud/#security","title":"Security","text":"<p>As we have already mentioned during the this tutorial, we need to pay attention to the permissions we give to users, groups, applications (and thefore the clients they used to interact with IBM Event Streams on IBM Cloud), etc since we dont want everyone and everything to be, for instance, creating or deleting schemas or updating the compatibility modes for schema evolution.</p> <p>As a result, the IBM Event Streams on IBM Cloud schema registry introduces a new IAM resource type: schema. This can be used as part of a CRN to identify a particular schema. For example:</p> <ul> <li> <p>A CRN that names a specific schema (test-schema), for a particular instance of an Event Streams service:</p> <p><code>crn:v1:bluemix:public:messagehub:us-south:a/6db1b0d0b5c54ee5c201552547febcd8:91191e31-5642-4f2d-936f-647332dce3ae:schema:test-schema</code></p> </li> <li> <p>A CRN that describes all of the schemas for a particular instance of the Event Streams service:</p> <p><code>crn:v1:bluemix:public:messagehub:us-south:a/6db1b0d0b5c54ee5c201552547febcd8:91191e31-5642-4f2d-936f-647332dce3ae:schema:</code></p> </li> <li> <p>While not directly related to the addition of the schema resource type, it is also worth noting that it is possible to apply policies to a CRN describing a particular Event Streams instance. For example, policies granted at the scope of this CRN would affect all resources (topics, schemas, etc.) belonging to the cluster:</p> <p><code>crn:v1:bluemix:public:messagehub:us-south:a/6db1b0d0b5c54ee5c201552547febcd8:91191e31-5642-4f2d-936f-647332dce3ae::</code></p> </li> </ul> <p>With the addition of the new schema IAM resource type it is possible to create policies that control access using varying degrees of granularity, for example:</p> <ul> <li>a specific schema</li> <li>a set of schemas selected via a wildcard expression</li> <li>all of the schemas stored by an instance of IBM Event Streams</li> <li>all of the schemas stored by all of the instances of IBM Event Streams in an account</li> </ul> <p>Please, review the security documentation you can find here in order to understand how to create the policies for assigning specific permissions at the differentt levels of a cloud resource (that is, at the level of any Event Streams, a specific Event Streams instance but all topics, etc).</p>"},{"location":"use-cases/schema-registry-on-cloud/#schema-security-role-mapping","title":"Schema security role mapping","text":"Action Event Streams cluster resource Schema resource Create schema Read Write List schemas Read Read Delete schema Read Manager Create schema version Read Write Get latest schema version Read Read Get specific schema version Read Read List all schema versions Read Read Delete a schema version Read Manager"},{"location":"use-cases/schema-registry-on-cloud/#compatibilitty-rules-security-role-mapping","title":"Compatibilitty rules security role mapping","text":"Action Event Streams cluster resource Schema resource Update global rule Manager - Get global rule Read - Create per schema rule Read Manager Get per schema rule Read Read Update per schema rule Read Manager Delete per schema rule Read Manager"},{"location":"use-cases/schema-registry-on-ocp/","title":"IBM Event Streams Schema Registry from IBM CloudPak for Integration","text":"<p>This documentation aims to be a introductory hands-on lab on the IBM Event Streams (v11.0.2) Schema Registry installed throught the IBM Cloud Pak for Integration V2022.2 on an Openshift cluster. It uses Python applications for producer and consumer with schema registry API. For a Quarkus based producer and consumer see the EDA-quickstarts project sub-folders: <code>quarkus-reactive-kafka-producer</code> and <code>quarkus-reactive-kafka-consumer</code> which includes docker compose with Apicur.io and reactive messaging implementation, plus all needed instructions to test schema management.</p>"},{"location":"use-cases/schema-registry-on-ocp/#requirements","title":"Requirements","text":"<p>This lab requires the following components to work against:</p> <ol> <li>An IBM Event Streams V10 instance installed through the IBM CloudPak for Integration V2020.2.X or greater.</li> <li>An IBM Cloud Shell - https://www.ibm.com/cloud/cloud-shell</li> </ol>"},{"location":"use-cases/schema-registry-on-ocp/#ibm-cloud-shell","title":"IBM Cloud Shell","text":"<p>Here we are going to set up our IBM Cloud Shell with all the tools required to carry out this lab.</p> <p>Start your IBM Cloud Shell by pointing your browser to https://cloud.ibm.com/shell</p> <p></p>"},{"location":"use-cases/schema-registry-on-ocp/#ibm-cloud-pak-cli","title":"IBM Cloud Pak CLI","text":"<p>Cloudctl is a command line tool to manage Container Application Software for Enterprises (CASEs). This CLI will allow us to manage Cloud Pak related components as well as software, like IBM Event Streams, installed through any IBM Cloud Pak.</p> <p>In order to install it, execute the following commands in your IBM Cloud Shell:</p> <ol> <li>Download the IBM Cloud Pak CLI - <code>curl -L https://github.com/IBM/cloud-pak-cli/releases/latest/download/cloudctl-linux-amd64.tar.gz -o cloudctl-linux-amd64.tar.gz</code></li> <li>Untar it - <code>tar -xvf cloudctl-linux-amd64.tar.gz</code></li> <li>Rename it for ease of use - <code>mv cloudctl-linux-amd64 cloudctl</code></li> <li>Include it to the PATH environment variable - <code>export PATH=$PATH:$PWD</code></li> <li>Make sure your IBM Cloud Pak CLI is in the path- <code>which cloudctl</code></li> <li>Make sure your IBM Cloud Pak CLI works - <code>cloudctl help</code></li> </ol> <p></p> <p>Note: If you are not using the IBM Cloud Shell to run the lab, be aware that the <code>cloudctl</code> CLI requires the <code>kubectl</code> CLI. To install the <code>kubectl</code> CLI on your personal environment, follow the instructions here</p>"},{"location":"use-cases/schema-registry-on-ocp/#event-streams-plugin-for-ibm-cloud-pak-cli","title":"Event Streams plugin for IBM Cloud Pak CLI","text":"<p>This plugin will allow us to manage IBM Event Streams.</p> <p>In order to install it, execute the following commands in your IBM Cloud Shell:</p> <ol> <li>Download the Event Streams plugin for IBM Cloud Pak CLI - <code>curl -L http://ibm.biz/es-cli-linux -o es-plugin</code></li> <li>Install it - <code>cloudctl plugin install es-plugin</code></li> <li>Make sure it works - <code>cloudctl es help</code></li> </ol> <p></p>"},{"location":"use-cases/schema-registry-on-ocp/#git","title":"Git","text":"<p>IBM Cloud Shell comes with Git already installed out of the box.</p>"},{"location":"use-cases/schema-registry-on-ocp/#vi","title":"Vi","text":"<p>IBM Cloud Shell comes with Vi already installed out of the box.</p>"},{"location":"use-cases/schema-registry-on-ocp/#python-3","title":"Python 3","text":"<p>IBM Cloud Shell comes with Python 3 already installed out of the box. However, we need to install the following modules that will be used later on in this tutorial when we run a Python application to work with Avro, Schemas and messages. These modules are <code>confluent_kafka</code> and <code>avro-python3</code></p> <p>In order to install these modules, execute the following command in your IBM Cloud Shell:</p> <ol> <li>Install the modules - <code>python3 -mpip install avro-python3 confluent_kafka --user</code></li> </ol> <p></p> <p>Congrats! you have now your IBM Cloud Shell ready to start working.</p>"},{"location":"use-cases/schema-registry-on-ocp/#schema-registry","title":"Schema Registry","text":"<p>One of the most common technologies used in the industry these days to define, serialize and deserialize messages flowing through your Kafka topics is Apache Avro (https://avro.apache.org/docs/current/). To learn more about Apache Avro, how to define Apache Avro data schemas and more, we strongly recommend to read through our documentation on Avro and data schemas here</p> <p>IBM Event Streams development team has developed a Schema Registry to work along your Kafka cluster to provide a place to store descriptions of the message formats used by your producers and consumers. The benefit of storing these descriptions is that you are able to validate that your producing and consuming applications will correctly inter-operate. The Schema Registry will also provide the ability for schemas to evolve in time.</p>"},{"location":"use-cases/schema-registry-on-ocp/#accessing-the-schema-registry","title":"Accessing the Schema Registry","text":""},{"location":"use-cases/schema-registry-on-ocp/#ui","title":"UI","text":"<p>To access the schema registry, we first need to log into IBM Event Streams.</p> <ol> <li> <p>Point your browser to your IBM Event Streams instace's user interface url and introduce your credentials</p> <p></p> </li> <li> <p>Once you are logged into your IBM Event Streams instance, you simply need to click on the Schema Registry button on the main left hand vertical menu bar:</p> </li> </ol> <p></p>"},{"location":"use-cases/schema-registry-on-ocp/#cli","title":"CLI","text":"<p>We can also interact with the Schema Registry through the IBM Event Streams CLI. In order to do so, we first need to log in with the IBM Cloud Pak CLI:</p> <ol> <li> <p>Log into your cluster with the IBM CloudPak CLI</p> <p> <p>Make sure to use the appropriate credentials and select the namespace where your IBM Event Streams instance is installed</p> <p></p> <pre><code>cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation\n\nUsername&gt; admin\n\nPassword&gt;\nAuthenticating...\nOK\n\nTargeted account mycluster Account\n\nEnter a namespace &gt; integration\nTargeted namespace integration\n\nConfiguring kubectl ...\nProperty \"clusters.mycluster\" unset.\nProperty \"users.mycluster-user\" unset.\nProperty \"contexts.mycluster-context\" unset.\nCluster \"mycluster\" set.\nUser \"mycluster-user\" set.\nContext \"mycluster-context\" created.\nSwitched to context \"mycluster-context\".\nOK\n\nConfiguring helm: /Users/user/.helm\nOK\n</code></pre> <li> <p>Initialize the Event Streams CLI plugin</p> <pre><code>cloudctl es init\n\nIBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-solutions.gse-ocp.net\nNamespace:                                     integration\nName:                                          es-1\nIBM Cloud Pak for Integration UI address:      No instance of Cloud Pak for Integration has been found. Please check that you have access to it.\nEvent Streams API endpoint:                    https://es-1-ibm-es-admapi-external-integration.apps.eda-solutions.gse-ocp.net\nEvent Streams API status:                      OK\nEvent Streams UI address:                      https://es-1-ibm-es-ui-integration.apps.eda-solutions.gse-ocp.net\nEvent Streams Schema Registry endpoint:        https://es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net\nEvent Streams bootstrap address:               es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443\nOK\n</code></pre> <p>(*)The above information will later be used in the IBM Event Streams Credentials section as these are neeeded by the Python application we will work with.</p> </li> <li> <p>Make sure you can access the IBM Event Streams Schema Registry:</p> <pre><code>cloudctl es schemas\n\nNo schemas were found.\nOK\n</code></pre> </li>"},{"location":"use-cases/schema-registry-on-ocp/#schemas","title":"Schemas","text":"<p>In this section we will finally get our hands dirty with the IBM Event Steams Schema Registry capability by working with Apache Avro schemas and the Schema Registry.</p> <p> <p>We recommend to complete most of the UI steps from your local workstation since these will require you to upload the files your create/modify to IBM Event Streams and that requires having your files available locally on your workstation rather than on the IBM Cloud Shell</p> <p></p>"},{"location":"use-cases/schema-registry-on-ocp/#create-a-schema","title":"Create a schema","text":"<p>Let's see how can we create a schema to start playing with.</p>"},{"location":"use-cases/schema-registry-on-ocp/#ui_1","title":"UI","text":"<p>The IBM EVent Streams user interface allow us to create schemas only from json or Avro schema avsc files.</p> <ol> <li> <p>Create an Avro schema file avsc with your schema:</p> <p> <p>Change USER1</p> <p></p> <pre><code>echo '{\n\"type\":\"record\",\n\"name\":\"demoSchema_UI_USER1\",\n\"namespace\": \"schemas.demo.ui\",\n\"fields\":[\n    {\"name\": \"eventKey\",\"type\":\"string\"},\n    {\"name\": \"message\",\"type\":\"string\"}]\n}' &gt; demoshema-ui.avsc\n</code></pre> <li> <p>On the IBM Event Streams Schema Registry User Interface, Click on Add schema button on the top right corner.</p> </li> <li> <p>Click on Upload definition button on the left hand side and select the <code>demoschema-ui.avsc</code> file we just created.</p> </li> <li> <p>You should now see you Avro schema loaded in the UI with two tabs, definition and preview to make sure your schema looks as desired:</p> <p></p> </li> <li> <p>Click on Add schema button at the top right corner and you should now see that schema listed among your other schemas.</p> </li>"},{"location":"use-cases/schema-registry-on-ocp/#cli_1","title":"CLI","text":"<ol> <li> <p>Create another Avro schema avsc file with a different schema:</p> <p> <p>Change USER1</p> <p></p> <pre><code>echo '{\n\"type\":\"record\",\n\"name\":\"demoSchema_CLI_USER1\",\n\"namespace\": \"schemas.demo.cli\",\n\"fields\":[\n    {\"name\": \"eventKey\",\"type\":\"string\"},\n    {\"name\": \"message\",\"type\":\"string\"}]\n}' &gt; demoshema-cli.avsc\n</code></pre> <li> <p>Create a schema by executing the following command:</p> <pre><code>cloudctl es schema-add --file demoshema-cli.avsc\n\nSchema demoSchema_CLI_USER1 is active.\n\nVersion   Version ID   Schema           State    Updated                         Comment\n1.0.0     1            demoSchema_CLI_USER1   active   Thu, 25 Jun 2020 11:30:42 UTC\n\nAdded version 1.0.0 of schema demoSchema_CLI_USER1 to the registry.\nOK\n</code></pre> </li>"},{"location":"use-cases/schema-registry-on-ocp/#list-schemas","title":"List schemas","text":""},{"location":"use-cases/schema-registry-on-ocp/#ui_2","title":"UI","text":"<p>In order to list the schemas in the UI you just simply need to open up the Schema Registry User Interface and schemas will get listed in there automatically. You also have a search tool bar at the top. You can also see more details about your schema by clicking the drop down arrow on its left:</p> <p></p>"},{"location":"use-cases/schema-registry-on-ocp/#cli_2","title":"CLI","text":"<ol> <li> <p>Execute the following command to list the schemas in your Schema Registry:</p> <pre><code>cloudctl es schemas\n\nSchema                 State    Latest version   Latest version ID   Updated\ndemoSchema_CLI_USER1   active   1.0.0            1                   Fri, 24 Jul 2020 13:55:49 UTC\ndemoSchema_UI_USER1    active   1.0.0            1                   Fri, 24 Jul 2020 13:55:51 UTC\nOK\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-ocp/#delete-schemas","title":"Delete schemas","text":""},{"location":"use-cases/schema-registry-on-ocp/#ui_3","title":"UI","text":"<ol> <li>Click on the schema you want to delete.</li> <li>Click on the Manage schema tab at the top.</li> <li> <p>Click on Remove schema</p> <p></p> </li> </ol>"},{"location":"use-cases/schema-registry-on-ocp/#cli_3","title":"CLI","text":"<p>To remove a schema using the CLI, simply execute the following command and confirm:</p> <p> <p>Change USER1</p> <p></p> <pre><code>cloudctl es schema-remove demoSchema_CLI_USER1\n\nRemove schema demoSchema_CLI_USER1 and all versions? [y/n]&gt; y\nSchema demoSchema_CLI_USER1 and all versions removed.\nOK\n</code></pre>"},{"location":"use-cases/schema-registry-on-ocp/#create-new-schema-version","title":"Create new schema version","text":"<p>To create a new version of a schema,</p> <ol> <li> <p>Let's first create again the previous two schemas:</p> <p> <p>Change USER1</p> <p></p> <p><pre><code>cloudctl es schema-add --file demoshema-ui.avsc\n\nSchema demoSchema_UI_USER1 is active.\n\nVersion   Version ID   Schema                State    Updated                         Comment\n1.0.0     1            demoSchema_UI_USER1   active   Fri, 24 Jul 2020 13:59:55 UTC\n\nAdded version 1.0.0 of schema demoSchema_UI_USER1 to the registry.\nOK\n</code></pre> <pre><code>cloudctl es schema-add --file demoshema-cli.avsc\n\nSchema demoSchema_CLI_USER1 is active.\n\nVersion   Version ID   Schema                 State    Updated                         Comment\n1.0.0     1            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:00:45 UTC\n\nAdded version 1.0.0 of schema demoSchema_CLI_USER1 to the registry.\nOK\n</code></pre></p> <li> <p>Add a new attribute to the schemas by editing their Avro schema avsc files:</p> <p> <p>Change USER1</p> <p></p> <pre><code>cat demoshema-ui.avsc\n\n{\n\"type\":\"record\",\n\"name\":\"demoSchema_UI_USER1\",\n\"namespace\": \"schemas.demo.ui\",\n\"fields\":[\n    {\"name\": \"eventKey\",\"type\":\"string\"},\n    {\"name\": \"message\",\"type\":\"string\"},\n    {\"name\": \"attribute1\",\"type\":\"string\"}]\n}\n</code></pre>"},{"location":"use-cases/schema-registry-on-ocp/#ui_4","title":"UI","text":"<ol> <li>Click on the schema you want to create a new version for.</li> <li>Click on the Add new version button on the left hand side.</li> <li>Click on Upload definition button on the left hand side.</li> <li> <p>Select the Avro schema avsc file and click ok.</p> <p></p> </li> </ol> <p> <p>ERROR: The error we are seeing on the screen is because the IBM Event Streams Schema Registtry enforces full compatibility: https://ibm.github.io/event-streams/schemas/creating/#adding-new-schema-versions</p> <p></p> <p>Full compatibility for data schemas means that messages that have been serialized with an earlier version of a schema can be deserialized with a later version. To be compatible, fields in later versions of a schema cannot be removed, and any new schema field must have a default value. More on data schema compatibility on the section Data Evolution towards the end of this lab.</p> <p>As explained in the error notification above, we need to add a default value for our new attribute in our data schema so that messages serialized with an older version of the data schema which won't contain this new attribute can later be deserialized with the newer version of the data schema that expects such attribute. By providing a default value, we allow deserializers to consume messages that do not contain newer attributes.</p> <ol> <li> <p>Add a default value for the new attribute:</p> <p> <p>Change USER1</p> <p></p> <pre><code>cat demoshema-ui.avsc\n\n{\n\"type\":\"record\",\n\"name\":\"demoSchema_UI_USER1\",\n\"namespace\": \"schemas.demo.ui\",\n\"fields\":[\n    {\"name\": \"eventKey\",\"type\":\"string\"},\n    {\"name\": \"message\",\"type\":\"string\"},\n    {\"name\": \"attribute1\",\"type\":\"string\",\"default\": \"whatever\"}]\n}\n</code></pre> <li> <p>Repeat the steps for adding a new version of a schema above.</p> </li> <li> <p>This time you should see that the schema is valid:</p> <p></p> </li> <li> <p>However, it still does not let us add this new version to the data schema until we actually provide a version for it. Click on the Add + link on the right of the version attribute of the schema and give it <code>2.0.0</code> for example (hit enter for the version to take the value you type in).</p> </li> <li>Click on Add schema.</li> <li> <p>You should now see the two versions for your data schema on the left hand side.</p> <p></p> </li> <li> <p>If you go back to the Schema Registry page where all your schemas are listed, you should now see that the latest version for your data schema is <code>2.0.0</code> now.</p> </li>"},{"location":"use-cases/schema-registry-on-ocp/#cli_4","title":"CLI","text":"<ol> <li> <p>If we try to add the new version of the schema from its <code>demoschema-cli.avsc</code> Avro schema file, we will get the same error as in the previous UI example:</p> <pre><code>cloudctl es schema-add --file demoshema-cli.avsc\n\nFAILED\nEvent Streams API request failed:\nError response from server. Status code: 400. Avro schema is not compatible with latest schema version: Compatibility type 'MUTUAL_READ' does not hold between 1 schema(s) in the chronology because: Schema[0] has incompatibilities: ['READER_FIELD_MISSING_DEFAULT_VALUE: attribute1' at '/fields/2'].\n\nUnable to add version 1.0.0 of schema demoSchema_CLI_USER1 to the registry.\n</code></pre> </li> <li> <p>Add the default value for the new attribute in your Avro schema avsc file and try to add that new version of the schema:</p> <pre><code>cloudctl es schema-add --file demoshema-cli.avsc\n\nFAILED\nEvent Streams API request failed:\nError response from server. Status code: 409. Schema version name already exists\n\nUnable to add version 1.0.0 of schema demoSchema_CLI_USER1 to the registry.\n</code></pre> </li> <li> <p>We see that we still have an error because we have not specified a new version value. Specify a new version value when adding this new version of the schema:</p> <pre><code>cloudctl es schema-add --file demoshema-cli.avsc --version 2.0.0\n\nSchema demoSchema_CLI_USER1 is active.\n\nVersion   Version ID   Schema                 State    Updated                         Comment\n1.0.0     1            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:00:45 UTC\n2.0.0     2            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:09:37 UTC\n\nAdded version 2.0.0 of schema demoSchema_CLI_USER1 to the registry.\nOK\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-ocp/#get-latest-version-of-a-schema","title":"Get latest version of a schema","text":""},{"location":"use-cases/schema-registry-on-ocp/#ui_5","title":"UI","text":"<p>In order to see the latest version of a data schema using the UI, we just need to go to the Schema Registry web user interface and click on the expand arrow buttton that is on the left:</p> <p></p>"},{"location":"use-cases/schema-registry-on-ocp/#cli_5","title":"CLI","text":"<p>In order to see the latest version of a data schema using the CLI, we simply need to run the following command:</p> <pre><code>cloudctl es schema demoSchema_CLI_USER1 --version 2\n\n{\n  \"type\": \"record\",\n  \"name\": \"demoSchema_CLI_USER1\",\n  \"namespace\": \"schemas.demo.cli\",\n  \"fields\": [\n    {\n      \"name\": \"eventKey\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"message\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"attribute1\",\n      \"type\": \"string\",\n      \"default\": \"whatever\"\n    }\n  ]\n}\n</code></pre> <p>(*) The version you specify is actually the version ID (2) rather than the version name we gave to the newer schema version (2.0.0):</p> <pre><code>cloudctl es schema demoSchema_CLI_USER1\n\nSchema demoSchema_CLI_USER1 is active.\n\nVersion   Version ID   Schema                 State    Updated                         Comment\n1.0.0     1            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:00:45 UTC\n2.0.0     2            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:09:37 UTC\nOK\n</code></pre>"},{"location":"use-cases/schema-registry-on-ocp/#get-specific-version-of-a-schema","title":"Get specific version of a schema","text":""},{"location":"use-cases/schema-registry-on-ocp/#ui_6","title":"UI","text":"<p>To see a specific version of a schema, go to the Schema Registry web user interface and click on the schema you want to see the version for. You will now see how many version of the schema you have and you can click on any of these in order to see more details about it.</p> <p></p>"},{"location":"use-cases/schema-registry-on-ocp/#cli_6","title":"CLI","text":"<p>To see a specific version of a schema using the CLI, simply run the following command with the version ID you would like to get retrieved:</p> <pre><code>cloudctl es schema demoSchema_CLI_USER1 --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"demoSchema_CLI_USER1\",\n  \"namespace\": \"schemas.demo.cli\",\n  \"fields\": [\n    {\n      \"name\": \"eventKey\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"message\",\n      \"type\": \"string\"\n    }\n  ]\n}\n</code></pre>"},{"location":"use-cases/schema-registry-on-ocp/#listing-all-versions-of-a-schema","title":"Listing all versions of a schema","text":""},{"location":"use-cases/schema-registry-on-ocp/#ui_7","title":"UI","text":"<p>To list all versions of schema in the Schema Registry user interface, you simply need to click on the data schema you want and a new page will display these:</p> <p></p>"},{"location":"use-cases/schema-registry-on-ocp/#cli_7","title":"CLI","text":"<p>In order to display all versions of a schema, run the following command:</p> <pre><code>cloudctl es schema demoSchema_CLI_USER1\n\nSchema demoSchema_CLI_USER1 is active.\n\nVersion   Version ID   Schema                 State    Updated                         Comment\n1.0.0     1            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:00:45 UTC\n2.0.0     2            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:09:37 UTC\nOK\n</code></pre>"},{"location":"use-cases/schema-registry-on-ocp/#deleting-a-version-of-a-schema","title":"Deleting a version of a schema","text":""},{"location":"use-cases/schema-registry-on-ocp/#ui_8","title":"UI","text":"<p>In order to delete a version of a schema using the Schema Registry user interface,</p> <ol> <li>Click on the data schema you want a version of it deleted for.</li> <li>Select the version you want to delete on the left hand side.</li> <li>Click on Manage version button that is on the top right corner within the main box in the center of the page.</li> <li> <p>Click on Remove version.</p> <p></p> </li> </ol>"},{"location":"use-cases/schema-registry-on-ocp/#cli_8","title":"CLI","text":"<p>In order to delete a version of a schema through the CLI, execute the following command:</p> <pre><code>cloudctl es schema-remove demoSchema_CLI_USER1 --version 1\n\nRemove version with ID 1 of schema demoSchema_CLI_USER1? [y/n]&gt; y\nVersion with ID 1 of schema demoSchema_CLI_USER1 removed.\nOK\n</code></pre> <p>We can see only version 2 now:</p> <pre><code>cloudctl es schema demoSchema_CLI_USER1\n\nSchema demoSchema_CLI_USER1 is active.\n\nVersion   Version ID   Schema                 State    Updated                         Comment\n2.0.0     2            demoSchema_CLI_USER1   active   Fri, 24 Jul 2020 14:09:37 UTC\nOK\n</code></pre>"},{"location":"use-cases/schema-registry-on-ocp/#ibm-event-streams-credentials","title":"IBM Event Streams Credentials","text":"<p>We have seen how to interact with the IBM Event Streams Schema Registry in order to create, delete, update, etc schemas that our applications will theoretically used for data correctness and application robusteness. However, the first thing that we need to set up in our IBM Event Streams instance are these applications' service credentials to be able to interact with IBM Event Streams and its Schema Registry. For doing so, we can either use either the GUI or the CLI.</p>"},{"location":"use-cases/schema-registry-on-ocp/#gui","title":"GUI","text":"<ol> <li> <p>Go to you IBM Event Streams instance console</p> <p></p> </li> <li> <p>Click on Connect to this cluster</p> <p></p> </li> </ol> <p>In this panel, you will find</p> <ol> <li>The Botstrap server to connect your applications to in order to send and receive messages from your IBM Event Streams instance. We can see we have one external listener (whith SCRAM-SHA authentication) and one internal listener (Depending your IBM Event Streams installation you might have different listeners and authentications for these).</li> <li>The Schema Registry url your applications will need to work with Apache Avro data schemas.</li> <li>A Generate SCRAM credentials button to generate the SCRAM credentials your applications will need to authenticate with.</li> <li> <p>A Certificates section to download either the Java PKCS12 or the PEM certificates (or both) that your applications will need in order to be able to establish the communitaction with your IBM Event Streams instance.</p> <p></p> </li> </ol> <p>To generate the SCRAM credentials needed by your application to get authenticated against IBM Event Streams to be able to produce and consume messages as well as to create, delete, etc topics and schemas, we need to create a KafkaUser (this happens behind the scenes) which we will set some permissions and get the corresponding SCRAM usernname and password for to be used in our applications kafka clients configuration:</p> <ol> <li>Click on Generate SCRAM credentials</li> <li> <p>Enter a user name for your credentials and click next (leave the last option selected: Produce messages, consume messages and create topics and schemas so that we give full access to our user for simplicity)</p> <p></p> </li> <li> <p>Select all topics and click next</p> <p></p> </li> <li> <p>Select all consumer groups and click next</p> <p></p> </li> <li> <p>Select all transactional IDs and click next</p> <p></p> </li> </ol> <p>Once you have created your new KafkaUser, you get the SCRAM credentials displayed on the screen:</p> <p></p> <p> <p>You can download the PEM certificate from the UI and then use the IBM Cloud Shell upload file option on the top bar or you can download it from within your IBM Cloud Shell by using the CLI (see below)</p> <p></p>"},{"location":"use-cases/schema-registry-on-ocp/#cli_9","title":"CLI","text":"<p> <p>The following two steps should have been completed already in the previous Accessing the Schema Registry section</p> <p></p> <ol> <li> <p>Log into your cluster with the IBM CloudPak CLI</p> <pre><code>cloudctl login -a https://cp-console.apps.eda-solutions.gse-ocp.net --skip-ssl-validation\n\nUsername&gt; admin\n\nPassword&gt;\nAuthenticating...\nOK\n\nTargeted account mycluster Account\n\nEnter a namespace &gt; integration\nTargeted namespace integration\n\nConfiguring kubectl ...\nProperty \"clusters.mycluster\" unset.\nProperty \"users.mycluster-user\" unset.\nProperty \"contexts.mycluster-context\" unset.\nCluster \"mycluster\" set.\nUser \"mycluster-user\" set.\nContext \"mycluster-context\" created.\nSwitched to context \"mycluster-context\".\nOK\n\nConfiguring helm: /Users/jesusalmaraz/.helm\nOK\n</code></pre> </li> <li> <p>Initialize the Event Streams CLI plugin</p> <pre><code>cloudctl es init\n\nIBM Cloud Platform Common Services endpoint:   https://cp-console.apps.eda-solutions.gse-ocp.net\nNamespace:                                     integration\nName:                                          es-1\nIBM Cloud Pak for Integration UI address:      No instance of Cloud Pak for Integration has been found. Please check that you have access to it.\nEvent Streams API endpoint:                    https://es-1-ibm-es-admapi-external-integration.apps.eda-solutions.gse-ocp.net\nEvent Streams API status:                      OK\nEvent Streams UI address:                      https://es-1-ibm-es-ui-integration.apps.eda-solutions.gse-ocp.net\nEvent Streams Schema Registry endpoint:        https://es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net\nEvent Streams bootstrap address:               es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443\nOK\n</code></pre> </li> </ol> <p>We can see above the Event Streams bootstrap address and Schema Registry url that our applications will need in order to connect to this Event Streams instance</p> <p>To be able to establish communication and authenticate against your IBM Event Streams instance, you will need the PEM certificate and an the SCRAM credentials:</p> <ol> <li> <p>To download your PEM certificate, you can use the following command:</p> <pre><code>cloudctl es certificates --format pem\n\nCertificate successfully written to /home/ALMARAZJ/es-cert.pem.\nOK\n</code></pre> </li> <li> <p>To generate your SCRAM credentials, we first need to create a KafkaUser, you can use the following command:</p> <pre><code>cloudctl es kafka-user-create --name my-user1 --consumer --producer --schema-topic-create --all-topics --all-groups --all-txnids --auth-type scram-sha-512\n\nKafkaUser name   Authentication   Authorization   Username                                                Secret\nmy-user1         scram-sha-512    simple          EntityOperator has not created corresponding username   EntityOperator has not created corresponding secret\n\nResource type     Name        Pattern type   Host   Operation\ntopic             *           literal        *      Read\ntopic             __schema_   prefix         *      Read\ntopic             *           literal        *      Write\ntopic             *           literal        *      Create\ntopic             __schema_   prefix         *      Alter\ngroup             *           literal        *      Read\ntransactionalId   *           literal        *      Write\n\nCreated KafkaUser my-user1.\nOK\n</code></pre> </li> </ol> <p> <p>We recommend to carefully set appropriate roles as well as access to topics, groups, transaction IDs and schemas for the API keys that you generate.</p> <p></p> <p>When a KafkaUser custom resource is created, the Entity Operator within Event Streams will create the principal in ZooKeeper with appropriate ACL entries. It will also create a Kubernetes Secret that contains the Base64-encoded SCRAM password for the scram-sha-512 authentication type, or the Base64-encoded certificates and keys for the tls authentication type.</p> <ol> <li> <p>Retrieve the username and the secret name containing the password of your SCRAM credentials for your KafkaUser:</p> <p><pre><code>oc get kafkauser my-user1 --namespace integration -o jsonpath='{\"username: \"}{.status.username}{\"\\nsecret-name: \"}{.status.secret}{\"\\n\"}'\n\nusername: my-user1\nsecret-name: my-user1\n</code></pre> 1. Retrieve the password of your SCRAM credentials from the secret above:</p> <pre><code>oc get secret my-user1 --namespace integration -o jsonpath='{.data.password}' | base64 --decode\n\n*****\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-ocp/#environment-variables","title":"Environment variables","text":"<p>Now that we have generated the appropriate IBM Event Streams credentials for applications to be able to establish communication and authenticate against our IBM Event Streams instance, we are going to set some environment variables that will be used by our Python application:</p> <ol> <li> <p>KAFKA_BROKERS which should take the value of bootstrap server:</p> <pre><code>export KAFKA_BROKERS=es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443\n</code></pre> </li> <li> <p>SCRAM_USERNAME which should take the value of the SCRAM username you have generated:</p> <pre><code>export SCRAM_USERNAME=my-user1\n</code></pre> </li> <li> <p>SCRAM_PASSWORD which should take the value of the SCRAM password you have generated:</p> <pre><code>export SCRAM_PASSWORD=*****\n</code></pre> </li> <li> <p>PEM_CERT which should take the value of the location where the PEM certificate is within your IBM Cloud Shell:</p> <p> <p>Set the path appropriately to your IBM Cloud Shell</p> <p></p> <pre><code>export PEM_CERT=~/es-cert.pem\n</code></pre> <p>(*) Don't forget to download both the PEM certificate to your IBM Cloud Shell through the CLI or upload it to your IBM Cloud Shell from your laptop if you used the UI to get the certificate. Review previous section if needed.</p> <li> <p>SCHEMA_REGISTRY_URL which should be a combination of the SCRAM username, the SCRAM password and the Schema Registry url in the form of:</p> <p><code>https://&lt;SCRAM_username&gt;:&lt;SCRAM_password&gt;@&lt;Schema_Registry_url&gt;</code></p> <pre><code>export SCHEMA_REGISTRY_URL=https://${SCRAM_USERNAME}:${SCRAM_PASSWORD}@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net\n</code></pre> </li>"},{"location":"use-cases/schema-registry-on-ocp/#python-application","title":"Python Application","text":"<p>The Python application we have built to see how to produce and consume messages (either plain messages or Avro encoded messages based on Avro Data Schemas) to and from an IBM Event Streams instance installed through the IBM Cloud Pak for Integration is public at the following GitHub repo: https://github.com/ibm-cloud-architecture/refarch-eda-tools/tree/master/labs/es-cp4i-schema-lab-v10</p>"},{"location":"use-cases/schema-registry-on-ocp/#clone","title":"Clone","text":"<p>In order to use and work with this Python application, the first thing we need to do is to clone the GitHub repository where it is published.</p> <ol> <li> <p>Clone the github repository on your workstation on the location of your choice:</p> <pre><code>git clone https://github.com/ibm-cloud-architecture/refarch-eda-tools.git\n\nCloning into 'refarch-eda-tools'...\nremote: Enumerating objects: 185, done.\nremote: Counting objects: 100% (185/185), done.\nremote: Compressing objects: 100% (148/148), done.\nremote: Total 185 (delta 23), reused 176 (delta 16), pack-reused 0\nReceiving objects: 100% (185/185), 6.17 MiB | 4.61 MiB/s, done.\nResolving deltas: 100% (23/23), done.\n</code></pre> </li> <li> <p>Change directory into <code>refarch-eda-tools/labs/es-cp4i-schema-lab-v10</code> to find the assets we will we working from now on for the python demo environment and few other scripts/applications:</p> <pre><code>cd refarch-eda-tools/labs/es-cp4i-schema-lab-v10\n\n$ ls -all\ntotal 240\ndrwxr-xr-x   9 user  staff     288 20 May 19:33 .\ndrwxr-xr-x   3 user  staff      96 20 May 19:33 ..\n-rw-r--r--   1 user  staff  112578 20 May 19:33 README.md\ndrwxr-xr-x   5 user  staff     160 20 May 19:33 avro_files\ndrwxr-xr-x   6 user  staff     192 20 May 19:33 kafka\ndrwxr-xr-x   6 user  staff     192 20 May 19:33 src\n</code></pre> </li> </ol> <p>In the next sections, we are going to briefly explain the implementation of this Python application so that you understand what is being done behind the scenes and more importantly, if you are a developer, how to do so.</p>"},{"location":"use-cases/schema-registry-on-ocp/#python-avro-producer","title":"Python Avro Producer","text":"<p>In this section we describe the Python scripts we will be using in order to be able to produce avro messages to a Kafka topic.</p>"},{"location":"use-cases/schema-registry-on-ocp/#produce-message","title":"Produce Message","text":"<p>The python script that we will use to send an avro message to a Kafka topic is ProduceAvroMessage.py where we have the following:</p> <ol> <li> <p>A function to parse the arguments:</p> <pre><code>def parseArguments():\n    global TOPIC_NAME\n    print(\"The arguments for this script are: \" , str(sys.argv))\n    if len(sys.argv) == 2:\n        TOPIC_NAME = sys.argv[1]\n    else:\n        print(\"[ERROR] - The produceAvroMessage.py script expects one argument: The Kafka topic to publish the message to\")\n        exit(1)\n</code></pre> </li> <li> <p>A function to create the event to be sent:</p> <pre><code>def createEvent():\n    print('Creating event...')\n\n    key = {\"key\": 1}\n    value = {\"message\" : \"This is a test message\"}\n\n    print(\"DONE\")\n\n    return json.dumps(value), json.dumps(key)\n</code></pre> </li> <li> <p>The main where we will:</p> <ol> <li>Parse the arguments</li> <li>Get the Avro schemas for the key and value of the event</li> <li>Create the Event to be sent</li> <li>Print it out for reference</li> <li>Create the Kafka Avro Producer and configure it</li> <li>Send the event</li> </ol> <pre><code>if __name__ == '__main__':\n    # Get the Kafka topic name\n    parseArguments()\n    # Get the avro schemas for the message's key and value\n    event_value_schema = getDefaultEventValueSchema(DATA_SCHEMAS)\n    event_key_schema = getDefaultEventKeySchema(DATA_SCHEMAS)\n    # Create the event\n    event_value, event_key = createEvent()\n    # Print out the event to be sent\n    print(\"--- Event to be published: ---\")\n    print(event_key)\n    print(event_value)\n    print(\"----------------------------------------\")\n    # Create the Kafka Avro Producer\n    kafka_producer = KafkaProducer(KAFKA_BROKERS,SCRAM_USERNAME,SCRAM_PASSWORD,SCHEMA_REGISTRY_URL)\n    # Prepare the Kafka Avro Producer\n    kafka_producer.prepareProducer(\"ProduceAvroMessagePython\",event_key_schema,event_value_schema)\n    # Publish the event\n    kafka_producer.publishEvent(TOPIC_NAME,event_value,event_key)\n</code></pre> </li> </ol> <p>As you can see, this python code depends on an Avro Utils for loading the Avro schemas and a Kafka Avro Producer to send the messages. These are explained next.</p>"},{"location":"use-cases/schema-registry-on-ocp/#avro-utils","title":"Avro Utils","text":"<p>This script, called avroEDAUtils.py, contains some very simple utility functions to be able to load Avro schemas from their avsc files in order to be used by the Kafka Avro Producer.</p> <ol> <li> <p>A function to get the key and value Avro schemas for the messages to be sent:</p> <p><pre><code>def getDefaultEventValueSchema(schema_files_location):\n# Get the default event value data schema\nknown_schemas = avro.schema.Names()\ndefault_event_value_schema = LoadAvsc(schema_files_location + \"/default_value.avsc\", known_schemas)\nreturn default_event_value_schema\n\ndef getDefaultEventKeySchema(schema_files_location):\n# Get the default event key data schema\nknown_schemas = avro.schema.Names()\ndefault_event_key_schema = LoadAvsc(schema_files_location + \"/default_key.avsc\", known_schemas)\nreturn default_event_key_schema\n</code></pre> (*) Where <code>known_schemas</code> is an Avro schema dictionary where all Avro schemas read get stored in order to be able to read nested Avro schemas afterwards. See the python script in detail for examples of this.</p> </li> <li> <p>A function to open a file, read its content as an Avro schema and store it in the Avro schema dictionary:</p> <pre><code>def LoadAvsc(file_path, names=None):\n# Load avsc file\n# file_path: path to schema file\n# names(optional): avro.schema.Names object\nfile_text = open(file_path).read()\njson_data = json.loads(file_text)\nschema = avro.schema.SchemaFromJSONData(json_data, names)\nreturn schema\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-ocp/#kafka-avro-producer","title":"Kafka Avro Producer","text":"<p>This script, called KcAvroProducer.py, will actually be the responsible for creating the Kafka Avro Producer, initialize and configure it and provide the publish method:</p> <ol> <li> <p>Initialize and prepare the Kafka Producer</p> <pre><code>class KafkaProducer:\n\n    def __init__(self,kafka_brokers = \"\",scram_username = \"\",scram_password = \"\",schema_registry_url = \"\"):\n        self.kafka_brokers = kafka_brokers\n        self.scram_username = scram_username\n        self.scram_password = scram_password\n        self.schema_registry_url = schema_registry_url\n\n    def prepareProducer(self,groupID = \"pythonproducers\",key_schema = \"\", value_schema = \"\"):\n        options ={\n                'bootstrap.servers':  self.kafka_brokers,\n                'schema.registry.url': self.schema_registry_url,\n                'group.id': groupID,\n                'security.protocol': 'SASL_SSL',\n                'sasl.mechanisms': 'SCRAM-SHA-512',\n                'sasl.username': self.scram_username,\n                'sasl.password': self.scram_password,\n                'ssl.ca.location': os.environ['PEM_CERT'],\n                'schema.registry.ssl.ca.location': os.environ['PEM_CERT']\n        }\n        # Print out the configuration\n        print(\"--- This is the configuration for the avro producer: ---\")\n        print(options)\n        print(\"---------------------------------------------------\")\n        # Create the Avro Producer\n        self.producer = AvroProducer(options,default_key_schema=key_schema,default_value_schema=value_schema)\n</code></pre> </li> <li> <p>Publish method</p> <pre><code>def publishEvent(self, topicName, value, key):\n    # Produce the Avro message\n    # Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first\n    self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(value)[key], callback=self.delivery_report)\n    # Flush\n    self.producer.flush()\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-ocp/#python-avro-consumer","title":"Python Avro Consumer","text":"<p>In this section we describe the python scripts we will be using in order to be able to consume Avro messages from a Kafka topic.</p>"},{"location":"use-cases/schema-registry-on-ocp/#consume-message","title":"Consume Message","text":"<p>The python script that we will use to consume an Avro message from a Kafka topic is ConsumeAvroMessage.py where we have the following:</p> <ol> <li> <p>A function to parse arguments:</p> <pre><code># Parse arguments to get the container ID to poll for\ndef parseArguments():\n    global TOPIC_NAME\n    print(\"The arguments for the script are: \" , str(sys.argv))\n    if len(sys.argv) != 2:\n        print(\"[ERROR] - The ConsumeAvroMessage.py script expects one arguments: The Kafka topic to events from.\")\n        exit(1)\n    TOPIC_NAME = sys.argv[1]\n</code></pre> </li> <li> <p>The main where we will:</p> <ol> <li>Parse the arguments to get the topic to read from</li> <li>Create the Kafka Consumer and configure it</li> <li>Poll for next avro message</li> <li>Close the Kafka consumer</li> </ol> <pre><code>if __name__ == '__main__':\n    # Parse arguments\n    parseArguments()\n    # Create the Kafka Avro consumer\n    kafka_consumer = KafkaConsumer(KAFKA_BROKERS,SCRAM_USERNAME,SCRAM_PASSWORD,TOPIC_NAME,SCHEMA_REGISTRY_URL)\n    # Prepare the consumer\n    kafka_consumer.prepareConsumer()\n    # Consume next Avro event\n    kafka_consumer.pollNextEvent()\n    # Close the Avro consumer\n    kafka_consumer.close()\n</code></pre> </li> </ol> <p>As you can see, this python code depends on a Kafka Avro Consumer to consume messages. This is explained next.</p>"},{"location":"use-cases/schema-registry-on-ocp/#kafka-avro-consumer","title":"Kafka Avro Consumer","text":"<p>This script, called KcAvroConsumer.py, will actually be the responsible for creating the Kafka Avro Consumer, initialize and configure it and provide the poll next event method:</p> <ol> <li> <p>Initialize and prepare the new Kafka consumer:</p> <pre><code>class KafkaConsumer:\n\n    def __init__(self, kafka_brokers = \"\", scram_username = \"\",scram_password = \"\", topic_name = \"\", schema_registry_url = \"\", autocommit = True):\n        self.kafka_brokers = kafka_brokers\n        self.scram_username = scram_username\n        self.scram_password = scram_password\n        self.topic_name = topic_name\n        self.schema_registry_url = schema_registry_url\n        self.kafka_auto_commit = autocommit\n\n    # See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md\n    def prepareConsumer(self, groupID = \"pythonconsumers\"):\n        options ={\n            'bootstrap.servers':  self.kafka_brokers,\n            'group.id': groupID,\n            'auto.offset.reset': 'earliest',\n            'schema.registry.url': self.schema_registry_url,\n            'enable.auto.commit': self.kafka_auto_commit,\n            'security.protocol': 'SASL_SSL',\n            'sasl.mechanisms': 'SCRAM-SHA-512',\n            'sasl.username': self.scram_username,\n            'sasl.password': self.scram_password,\n            'ssl.ca.location': os.environ['PEM_CERT'],\n            'schema.registry.ssl.ca.location': os.environ['PEM_CERT']\n        }\n        # Print the configuration\n        print(\"--- This is the configuration for the Avro consumer: ---\")\n        print(options)\n        print(\"---------------------------------------------------\")\n        # Create the Avro consumer\n        self.consumer = AvroConsumer(options)\n        # Subscribe to the topic\n        self.consumer.subscribe([self.topic_name])\n</code></pre> </li> <li> <p>Poll next event method:</p> <pre><code># Prints out the message\ndef traceResponse(self, msg):\n    print('[Message] - Next message consumed from {} partition: [{}] at offset {} with key {} and value {}'\n                .format(msg.topic(), msg.partition(), msg.offset(), msg.key(), msg.value() ))\n\n# Polls for next event\ndef pollNextEvent(self):\n    # Poll for messages\n    msg = self.consumer.poll(timeout=10.0)\n    # Validate the returned message\n    if msg is None:\n        print(\"[INFO] - No new messages on the topic\")\n    elif msg.error():\n        if (\"PARTITION_EOF\" in msg.error()):\n            print(\"[INFO] - End of partition\")\n        else:\n            print(\"[ERROR] - Consumer error: {}\".format(msg.error()))\n    else:\n        # Print the message\n        msgStr = self.traceResponse(msg)\n</code></pre> </li> </ol>"},{"location":"use-cases/schema-registry-on-ocp/#schemas-and-messages","title":"Schemas and Messages","text":"<p>In this section we are going to see how Schema Registry works when you have an application that produces and consumes messages based on Avro data schemas. The application we are going to use for this is the python application presented above in the Python Avro Producer and Python Avro Consumer sections.</p> <p>IMPORTANT: Before start using our Python application we must set the PYTHONPATH environment variable to point to where we have all the Python scripts that make up our application in order for Python to find these at execution time.</p> <ol> <li>Set the PYTHONPATH variable to the location where you cloned the GitHub repository containing the Python application we are going to be working with</li> </ol> <pre><code>export PYTHONPATH=~/refarch-eda-tools/labs/es-cp4i-schema-lab-v10\n</code></pre>"},{"location":"use-cases/schema-registry-on-ocp/#produce-a-message","title":"Produce a message","text":"<p>In order to produce a message, we execute the <code>ProduceAvroMessage.py</code>. This script, as you could see in the Python Avro Producer section, is sending the event with key <code>{'key': '1'}</code> and value <code>{'message': 'This is a test message'}</code> according to the schemas defined in default_key.avsc and default_value.avsc for the key and value of the event respectively.</p> <p> <p>Make sure you are on the right path where the python scripts live: ~/refarch-eda-tools/labs/es-cp4i-schema-lab-v10/src</p> <p></p> <p> <p>Change user1</p> <p></p> <pre><code>python3 ProduceAvroMessage.py test-schema-user1\n\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema-user1']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": \"This is a test message\"}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\nMessage delivered to test-schema-user1 [0]\n</code></pre> <p>We can see our new message delivered in the <code>test-schema-user1</code> topic by</p> <ol> <li> <p>Go into the topics page in the IBM Event Streams UI</p> <p></p> </li> <li> <p>Click on the topic and then on the Messages tab at the top. Finally, click on a message to see it displayed on the right hand side of the screen</p> <p></p> </li> </ol> <p> <p>INFO: Mind the message now is not in JSON format as Avro does not repeat every field name with every single record which makes Avro more efficient than JSON for high-volume usage. This is thanks to having Avro schemas.</p> <p></p> <p>IMPORTANT: As you can see, we got the <code>test-schema-user1</code> topic auto-created when we produced the message. The reason for this is that</p> <ol> <li>Kafka is set out of the box to let applications to auto-create topics.</li> <li>We created the SCRAM credentials for our application to allow the application to create topics.</li> </ol> <p>On a production-like environment, you don't want developers creating applications that auto-create topics in your IBM Event Streams instance without any control. For that, we would configure Kafka to forbid topic auto-creation (https://kafka.apache.org/documentation/#auto.create.topics.enable) as well as thoroughly created the SCRAM credentials with the most strict but appropriate permissions that our application needs.</p> <p>IMPORTANT: Similar to the auto-creation of topics, we can see below that our application got the Avro data schemas for both the key and value of the message produced auto-registered. This is because many client libraries come with a SerDes property to allow them to auto-register the Avro data schemas (https://docs.confluent.io/current/clients/confluent-kafka-python/#avroserializer). However, on a production-like environment we don't want applications to auto-register schemas without any control but yet we can not leave it to the developers to set the auto-registration property off on their libraries. Instead, we would create the SCRAM credentials with the most strict but appropriate permissions that our application needs.</p> <p>If we look now at the schemas our schema registry has:</p> <pre><code>cloudctl es schemas\n\nSchema                           State    Latest version   Latest version ID   Updated\ndemoSchema_CLI_USER1             active   2.0.0            2                   Fri, 24 Jul 2020 14:09:37 UTC\ndemoSchema_UI_USER1              active   2.0.0            2                   Fri, 24 Jul 2020 14:06:27 UTC\ntest-schema-user1-key-d89uk      active   1                1                   Fri, 24 Jul 2020 15:41:46 UTC\ntest-schema-user1-value-tv5efr   active   1                1                   Fri, 24 Jul 2020 15:41:45 UTC\nOK\n</code></pre> <p>we see two schemas, <code>test-schema-user1-key-d89uk</code> and <code>test-schema-user1-value-tv5efr</code>, which in fact correspond to the Avro data schema used for the <code>key</code> (default_key.avsc) and the <code>value</code> (default_value.avsc) of events sent to the <code>test-schema-user1</code> topic in the ProduceAvroMessage.py as explained before sending the message.</p> <p>To make sure of what we are saying, we can inspect those schemas:</p> <pre><code>cloudctl es schema test-schema-user1-key-d89uk --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"defaultKey\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"int\",\n      \"name\": \"key\",\n      \"doc\": \"We expect any int as the event key\"\n    }\n  ],\n  \"doc\": \"Default Message's key Avro data schema\"\n}\n</code></pre> <pre><code>cloudctl es schema test-schema-user1-value-tv5efr --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n</code></pre> <p>If I now decided that my events should contain another attribute, I would modify the event value schema (default_value.avsc) to reflect that as well as <code>ProduceAvroMessage.py</code> to send that new attribute in the event it sends:</p> <p> <p>Change user1</p> <p></p> <pre><code>python3 ProduceAvroMessage.py test-schema-user1\n\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema-user1']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": \"This is a test message\", \"anotherAttribute\": \"Just another test string\"}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\nMessage delivered to test-schema-user1 [0]\n</code></pre> <p>I can see that an event with a new attribute has been sent:</p> <p></p> <p>And I can also see that the new shcema has got registered as well:</p> <pre><code>cloudctl es schemas\n\nSchema                           State    Latest version   Latest version ID   Updated\ndemoSchema_CLI_USER1             active   2.0.0            2                   Fri, 24 Jul 2020 14:09:37 UTC\ndemoSchema_UI_USER1              active   2.0.0            2                   Fri, 24 Jul 2020 14:06:27 UTC\ntest-schema-user1-key-d89uk      active   1                1                   Fri, 24 Jul 2020 15:41:46 UTC\ntest-schema-user1-value-a5bbaa   active   1                1                   Fri, 24 Jul 2020 15:54:37 UTC\ntest-schema-user1-value-tv5efr   active   1                1                   Fri, 24 Jul 2020 15:41:45 UTC\nOK\n</code></pre> <p>If I inspect that new schema, I see my new attribute in it:</p> <pre><code>cloudctl es schema test-schema-user1-value-a5bbaa --version 1\n\n{\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"anotherAttribute\",\n      \"doc\": \"Any other string\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n</code></pre> <p> <p>The schema evolution above (test-schema-user1-value-a5bbaa) should have got registered as a new version of the already existing schema (test-schema-user1-value-tv5efr). IBM Event Streams allows schemas to auto-register themselves when these are sent along with a message from a producer application. However, the Schema Registry does not pick \"new\" schemas up as a new version of a previous schema and simply creates a new schema. Anyway, when reading messages off the topic, Schema Registry handles well what schema to return back to the receiver application so messages can get properly deserialized. Will see that in the next section.</p> <p></p> <p> <p>SECURITY: As some of you may have already thought, having your clients (that is your applications), auto-register the Avro data schemas that are in the end kind of the contracts that your components of your overal solution agree on in order to understand each other and collaborate between them is NOT a good idea. Specially in microservices architectures where you might have hundreds of microservices talking and collaborating among themselsves. We will see in the Security section how we can control schema registration and evolution based on roles at the schema level also.</p> <p></p>"},{"location":"use-cases/schema-registry-on-ocp/#create-a-non-compliant-message","title":"Create a non-compliant message","text":"<p>Let's see what happens if we send a message that does not comply with its Avro data schema. Let's say that I send the following message:</p> <pre><code>key = {\"key\": 1}\nvalue = {\"message\" : 12345}\n</code></pre> <p>and this is the output of that attempt:</p> <p> <p>Change user1</p> <p></p> <pre><code>python3 ProduceAvroMessage.py test-schema-user1\n\n @@@ Executing script: ProduceAvroMessage.py\nThe arguments for the script are:  ['ProduceAvroMessage.py', 'test-schema-user1']\nCreating event...\nDONE\n--- Event to be published: ---\n{\"key\": 1}\n{\"message\": 12345}\n----------------------------------------\n--- This is the configuration for the avro producer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'group.id': 'ProduceAvroMessagePython', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\nTraceback (most recent call last):\n  File \"ProduceAvroMessage.py\", line 81, in &lt;module&gt;\n    kafka_producer.publishEvent(TOPIC_NAME,event_value,event_key)\n  File \"/tmp/lab/kafka/KcAvroProducer.py\", line 43, in publishEvent\n    self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(key), callback=self.delivery_report)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\", line 99, in produce\n    value = self._serializer.encode_record_with_schema(topic, value_schema, value)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 118, in encode_record_with_schema\n    return self.encode_record_with_schema_id(schema_id, record, is_key=is_key)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 152, in encode_record_with_schema_id\n    writer(record, outf)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 86, in &lt;lambda&gt;\n    return lambda record, fp: writer.write(record, avro.io.BinaryEncoder(fp))\n  File \"/root/.local/lib/python3.7/site-packages/avro/io.py\", line 771, in write\n    raise AvroTypeException(self.writer_schema, datum)\navro.io.AvroTypeException: The datum {'message': 12345} is not an example of the schema {\n  \"type\": \"record\",\n  \"name\": \"defaultValue\",\n  \"namespace\": \"ibm.eda.default\",\n  \"fields\": [\n    {\n      \"type\": \"string\",\n      \"name\": \"message\",\n      \"doc\": \"Any string\"\n    },\n    {\n      \"type\": \"string\",\n      \"name\": \"anotherAttribute\",\n      \"doc\": \"Any other string\"\n    }\n  ],\n  \"doc\": \"Default Message's value Avro data schema\"\n}\n</code></pre> <p>As we can see, the attempt failed as the Avro producer will check the message against the Avro data schema defined for the topic we want to send the message to and yield that this message does not comply (the message value attribute we are sending is an integer rather than a string and we are missing the second attribute).</p> <p>Therefore, using Avro schemas with IBM Event Streams give us the ability to build our system with robustness protecting downstream data consumers from malformed data, as only valid data will be permitted in the topic.</p>"},{"location":"use-cases/schema-registry-on-ocp/#consume-a-message","title":"Consume a message","text":"<p>In order to consume a message, we execute the <code>ConsumeAvroMessage.py</code> within the <code>/tmp/lab/src</code> folder in our python demo environment:</p> <p> <p>Change user1</p> <p></p> <pre><code>python3 ConsumeAvroMessage.py test-schema-user1\n\n @@@ Executing script: ConsumeAvroMessage.py\nThe arguments for this script are:  ['ConsumeAvroMessage.py', 'test-schema-user1']\n--- This is the configuration for the Avro consumer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\n[Message] - Next message consumed from test-schema-user1 partition: [0] at offset 0 with key {'key': 1} and value {'message': 'This is a test message'}\n\n\npython3 ConsumeAvroMessage.py test-schema-user1\n\n @@@ Executing script: ConsumeAvroMessage.py\nThe arguments for this script are:  ['ConsumeAvroMessage.py', 'test-schema-user1']\n--- This is the configuration for the Avro consumer: ---\n{'bootstrap.servers': 'es-1-kafka-bootstrap-integration.apps.eda-solutions.gse-ocp.net:443', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'schema.registry.url': 'https://my-user1:*****@es-1-ibm-es-schema-external-integration.apps.eda-solutions.gse-ocp.net', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'SCRAM-SHA-512', 'sasl.username': 'my-user1', 'sasl.password': '*****', 'ssl.ca.location': '/home/ALMARAZJ/es-cert.pem', 'schema.registry.ssl.ca.location': '/home/ALMARAZJ/es-cert.pem'}\n---------------------------------------------------\n[Message] - Next message consumed from test-schema-user1 partition: [0] at offset 1 with key {'key': 1} and value {'message': 'This is a test message', 'anotherAttribute': 'Just another test string'}\n</code></pre> <p>As you can see, our script was able to read the Avro messages from the <code>test-schema-user1</code> topic and map that back to their original structure thanks to the Avro schemas:</p> <pre><code>[Message] - Next message consumed from test-schema partition: [0] at offset 0 with key {'key': 1} and value {'message': 'This is a test message'}\n\n[Message] - Next message consumed from test-schema partition: [0] at offset 1 with key {'key': 1} and value {'message': 'This is a test message', 'anotherAttribute': 'Just another test string'}\n</code></pre>"},{"location":"use-cases/schema-registry-on-ocp/#data-evolution","title":"Data Evolution","text":"<p>So far we have more or less seen what Avro is, what an Avro data schema is, what a schema registry is and how this all works together. From creating an Avro data schema for your messages/events to comply with to how the schema registry and Avro data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their Avro data schemas to the rich CLI IBM Event Streams provides to interact with.</p> <p>However, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying Event Storming or Domain Driven Design for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data, like your use or business cases, may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases.</p> <p>But it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event backbone) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to hundreds of years) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the compatibility of old and new data schemas and, in fact, old and new data at the end of the day.</p> <p>The IBM Event Streams Schema Registry enforces full compatibility when creating a new version of a schema. Full compatibility means that old data can be read with the new data schema, and new data can also be read with the last data schema.</p> <p>In data formats like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change. Let's see then how this affects when you want your data to evolve in a way that it needs to add a new attribute or delete an existing attribute.</p> <p>But let's see what that means in terms of adding and removing attributes from your data schema.</p>"},{"location":"use-cases/schema-registry-on-ocp/#adding-a-new-attribute","title":"Adding a new attribute","text":"<p>Although we have already seen this in the adding a new version of a schema section, let's try to add a new version of our <code>test-schema-value</code> schema where we have a new attribute. Remember, our <code>default_schema.avsc</code> already contains a new attribute than the original one but that it got registered as a new schema rather than as a new version of the original one. Let's reuse that Avro schema file to register it as a new version (INFO: you might need to copy/download that file to your local workstation in order to be able to then upload it to the IBM Event Streams through its UI)</p> <p>When doing so from the UI, we see the following error:</p> <p></p> <p>The reason, as alread explained in the add a new version of a schema section, is because full compatibility dictates that you can only add new attributes to a schema if these have a default value. Reason being that a receiver should be able to deserialize messages produced with an older schema using the newer schema. Because old messages were written with an older schema that did not contain our new attribute, those messages won't have that attribute so we need to provide a default value for it in our never version of the schema so that the receiver is able to deserialize those older messages with the newer schema.</p> <p>If we add the default value for the new attribute, we see that our newer version is now compatible:</p> <p></p> <p>and that it gets registered fine:</p> <p></p>"},{"location":"use-cases/schema-registry-on-ocp/#removing-an-existing-attribute","title":"Removing an existing attribute","text":"<p>What if we now wanted to remove the original <code>message</code> attribute from our schema. Let's remove it from the <code>default_value.avsc</code> file and try to register that new version:</p> <p></p> <p>We, again, get the same error. And the reason is because receivers must be able to read and deserialize messages produced with the newer schema (that is, without the <code>message</code> attribute) but with the older schema (that is, with the schema version that enforces the existence of the <code>message</code> attribute).</p> <p>In order to work this around, what we need to do is to register first an intermediate schema that defines a default value for the <code>message</code> attribute:</p> <p></p> <p>Once we have a default value for the <code>message</code> attribute, we can register a new version of the schema that finally removes that attribute:</p> <p></p>"},{"location":"use-cases/schema-registry-on-ocp/#security","title":"Security","text":"<p>As we have already mentioned during the this tutorial, we need to pay attention to the permissions we give to users, groups, applications (and thefore the clients they used to interact with IBM Event Streams), etc since we don't want everyone and everything to be, for instance, creating or deleting topics, schemas, etc.</p> <p>You can secure your IBM Event Streams resources in a fine-grained manner by managing the access each user and application has to each resource. Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in Access Control List (ACL) rules:</p> <ul> <li>Topics (topic): you can control the ability of users and applications to create, delete, read, and write to a topic.</li> <li>Consumer groups (group): you can control an application\u2019s ability to join a consumer group.</li> <li>Transactional IDs (transactionalId): you can control the ability to use the transaction capability in Kafka.</li> </ul> <p>Note: Schemas in the Event Streams Schema Registry are a special case and are secured using the resource type of topic combined with a prefix of __schema_. You can control the ability of users and applications to create, delete, read, and update schemas.</p> <p>You can find more information about how to secure your IBM Event Streams resources in the official documentation at: https://ibm.github.io/event-streams/security/managing-access/</p>"}]}